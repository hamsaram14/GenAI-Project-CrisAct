{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1ebebf55e67486a9ec4b8ca41b03dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c012d6ab9f84429f8878c53d0e372fd3",
              "IPY_MODEL_e54df062433d47f88736bd5022ad289c",
              "IPY_MODEL_25ce15978d794864ad145b54ee78c6a3"
            ],
            "layout": "IPY_MODEL_6ecae96315e84ab783a40c94a6a25f4b"
          }
        },
        "c012d6ab9f84429f8878c53d0e372fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b006e5790fd74ca88b34c12d1193424a",
            "placeholder": "​",
            "style": "IPY_MODEL_70666b207d1a46489365f079d22f41c8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e54df062433d47f88736bd5022ad289c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e1e701db75e425ca5669e7398ac40b4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b17dceaa924839a3e8685646661436",
            "value": 2
          }
        },
        "25ce15978d794864ad145b54ee78c6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64d36fe81e1146d99fe6847d046fc01b",
            "placeholder": "​",
            "style": "IPY_MODEL_acd6df2db53b4210976cefe81094e5d4",
            "value": " 2/2 [00:03&lt;00:00,  1.75s/it]"
          }
        },
        "6ecae96315e84ab783a40c94a6a25f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b006e5790fd74ca88b34c12d1193424a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70666b207d1a46489365f079d22f41c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e1e701db75e425ca5669e7398ac40b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b17dceaa924839a3e8685646661436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64d36fe81e1146d99fe6847d046fc01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acd6df2db53b4210976cefe81094e5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef05f224aa64fb68ce4f51f50335742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bac2ab631021451fb52a08f4768fb890",
              "IPY_MODEL_f5e74949343e4f349b123159bafe192d",
              "IPY_MODEL_2c44435fcdc04f12a9b1940f080b4392"
            ],
            "layout": "IPY_MODEL_e721f090a31c4aababe3ab1bb5a17cd6"
          }
        },
        "bac2ab631021451fb52a08f4768fb890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcb9991902bc45469d5cd3e19ca6384e",
            "placeholder": "​",
            "style": "IPY_MODEL_e0caa17743bf4b5f9b48f95f9eda1c25",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f5e74949343e4f349b123159bafe192d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab71615002ff47ce80b0390168128a5a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ce9ee6c7dcf475bb200d4a829aae284",
            "value": 2
          }
        },
        "2c44435fcdc04f12a9b1940f080b4392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d18491b1e87743bfa2eec05fdff99fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_50a2b82d6ac14d84ac1ac18b835f0f0a",
            "value": " 2/2 [00:16&lt;00:00,  7.31s/it]"
          }
        },
        "e721f090a31c4aababe3ab1bb5a17cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcb9991902bc45469d5cd3e19ca6384e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0caa17743bf4b5f9b48f95f9eda1c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab71615002ff47ce80b0390168128a5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ce9ee6c7dcf475bb200d4a829aae284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d18491b1e87743bfa2eec05fdff99fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a2b82d6ac14d84ac1ac18b835f0f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c02e05ae64fe48ff9cf399e1d835e3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d42c32aa5384920a5b9804068736d72",
              "IPY_MODEL_f4d1382dbd1d482abf4f0017da0cb88a",
              "IPY_MODEL_41d71cb85e1f4c80b6218ad7a996609e"
            ],
            "layout": "IPY_MODEL_1ded3436ebc74681ad00d21a887ddcec"
          }
        },
        "6d42c32aa5384920a5b9804068736d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ea2b54ee53419bbb34e68ec1fd2398",
            "placeholder": "​",
            "style": "IPY_MODEL_e1f6ab9fee6146d6b8269c5de87c458f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f4d1382dbd1d482abf4f0017da0cb88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c73333580ee41b8a933de484130a3ff",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df02b75cc435416a92598076f953047a",
            "value": 2
          }
        },
        "41d71cb85e1f4c80b6218ad7a996609e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_787777a990054dee804ff6bac37b38c4",
            "placeholder": "​",
            "style": "IPY_MODEL_a22a00f41f7f4ceea9788885f1b55c2a",
            "value": " 2/2 [00:16&lt;00:00,  7.43s/it]"
          }
        },
        "1ded3436ebc74681ad00d21a887ddcec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ea2b54ee53419bbb34e68ec1fd2398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1f6ab9fee6146d6b8269c5de87c458f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c73333580ee41b8a933de484130a3ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df02b75cc435416a92598076f953047a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "787777a990054dee804ff6bac37b38c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a22a00f41f7f4ceea9788885f1b55c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c256213a80a34285bcfd5f4f69a11ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa9311b279094058b7cc6d589ad37341",
              "IPY_MODEL_89180d53f4c04a268df6faf96220c914",
              "IPY_MODEL_28755c3b913349399099d57100b19fc8"
            ],
            "layout": "IPY_MODEL_f2ff0925cb624001ade2951352a0409b"
          }
        },
        "aa9311b279094058b7cc6d589ad37341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c97aaa893d14159829c15656cb384f1",
            "placeholder": "​",
            "style": "IPY_MODEL_e5927c1abea0414180e3294868c1e4c9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "89180d53f4c04a268df6faf96220c914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed14425688b4ce7ad64dafef79d8e24",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75d4d68c9a38454aa04e0afc8228cf91",
            "value": 2
          }
        },
        "28755c3b913349399099d57100b19fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa900aa073cd48ec905928d07a2c26fe",
            "placeholder": "​",
            "style": "IPY_MODEL_48d432be093e4dd18963acd6e4790489",
            "value": " 2/2 [00:04&lt;00:00,  2.09s/it]"
          }
        },
        "f2ff0925cb624001ade2951352a0409b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c97aaa893d14159829c15656cb384f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5927c1abea0414180e3294868c1e4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ed14425688b4ce7ad64dafef79d8e24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d4d68c9a38454aa04e0afc8228cf91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa900aa073cd48ec905928d07a2c26fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d432be093e4dd18963acd6e4790489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6095ab49dc5441f99c248aadfb246f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5f1636b3d5a435892c182ddebdd626a",
              "IPY_MODEL_b12ac360c7e344518edd5f8e82ca7ed6",
              "IPY_MODEL_52709daf75d3498c9ed52887e4df3d13"
            ],
            "layout": "IPY_MODEL_026cf883e22c4e44bc697a688d5bcbf9"
          }
        },
        "c5f1636b3d5a435892c182ddebdd626a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e99926854744734b028e1399d0d253d",
            "placeholder": "​",
            "style": "IPY_MODEL_74b2c21e65844720b1bc0f3ae93a8ac4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b12ac360c7e344518edd5f8e82ca7ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe69e70a8aa4a72a46b7e8aadd1b3ac",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c1b0869b0ba480f9d7358ff87e3010c",
            "value": 2
          }
        },
        "52709daf75d3498c9ed52887e4df3d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3f4dab2036448659eea3ba86e6c9078",
            "placeholder": "​",
            "style": "IPY_MODEL_f580781227ab4cbf886a510fd3272da2",
            "value": " 2/2 [00:04&lt;00:00,  2.10s/it]"
          }
        },
        "026cf883e22c4e44bc697a688d5bcbf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e99926854744734b028e1399d0d253d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b2c21e65844720b1bc0f3ae93a8ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efe69e70a8aa4a72a46b7e8aadd1b3ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1b0869b0ba480f9d7358ff87e3010c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3f4dab2036448659eea3ba86e6c9078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f580781227ab4cbf886a510fd3272da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd012cecca024051ab433bb46813c0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b71591e9ab1b4f78a0f2f1153af47f71",
              "IPY_MODEL_9ef52d0c6b6743fbb52ecfc559920f44",
              "IPY_MODEL_d79aa50ccc91439697fa5294cae2721b"
            ],
            "layout": "IPY_MODEL_03d1206b681f49e29bdf0a0d67ab3850"
          }
        },
        "b71591e9ab1b4f78a0f2f1153af47f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63b1c1c5ed5549a59fd4ff66d5625281",
            "placeholder": "​",
            "style": "IPY_MODEL_2b530f0f299747fb8017a5b8b8a10035",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9ef52d0c6b6743fbb52ecfc559920f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb8a4274ebb54d87a0ffa1d0be38b0f8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3167ce4b91194703aabe148d63659159",
            "value": 2
          }
        },
        "d79aa50ccc91439697fa5294cae2721b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef68e5f7c6764f98972c5048713f5a09",
            "placeholder": "​",
            "style": "IPY_MODEL_808fee9b53d84a34889772e42e754253",
            "value": " 2/2 [00:04&lt;00:00,  2.13s/it]"
          }
        },
        "03d1206b681f49e29bdf0a0d67ab3850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63b1c1c5ed5549a59fd4ff66d5625281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b530f0f299747fb8017a5b8b8a10035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb8a4274ebb54d87a0ffa1d0be38b0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3167ce4b91194703aabe148d63659159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef68e5f7c6764f98972c5048713f5a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808fee9b53d84a34889772e42e754253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "543b129f9c044154a8c57c32e185d4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29ef8ebb3f004f498bd2fa89466c59ad",
              "IPY_MODEL_ef191ffa6afc49b88e935b6166530f34",
              "IPY_MODEL_82c93e581cc942d5915ea45d903f5b6e"
            ],
            "layout": "IPY_MODEL_7552314c8fa44982b1de8b4761bcef30"
          }
        },
        "29ef8ebb3f004f498bd2fa89466c59ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76046b56505146a5af6799b80de65b18",
            "placeholder": "​",
            "style": "IPY_MODEL_69dc75a03f5e4486930519a5f8c3d6c3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ef191ffa6afc49b88e935b6166530f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d0d5972a95473a955d10207ef5ef2b",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f391ab0688004443b39558825dd35c6c",
            "value": 2324
          }
        },
        "82c93e581cc942d5915ea45d903f5b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad11e1d152d49a2911d3efae9bf6a2d",
            "placeholder": "​",
            "style": "IPY_MODEL_e623601cdd674d72a02edf668ca47461",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 275kB/s]"
          }
        },
        "7552314c8fa44982b1de8b4761bcef30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76046b56505146a5af6799b80de65b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69dc75a03f5e4486930519a5f8c3d6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d0d5972a95473a955d10207ef5ef2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f391ab0688004443b39558825dd35c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cad11e1d152d49a2911d3efae9bf6a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e623601cdd674d72a02edf668ca47461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d0cb5708262471db0144b6e44698f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72856c771bce4113bef8a3cd038bc452",
              "IPY_MODEL_49a8b098dc884e6ba7f2b2b631cbc57a",
              "IPY_MODEL_10cf2e431d5841c1a5b58ac1698f7a55"
            ],
            "layout": "IPY_MODEL_d0330849069644efa6807a675bfb5366"
          }
        },
        "72856c771bce4113bef8a3cd038bc452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76758ac031d449ef805f54ac625afbcd",
            "placeholder": "​",
            "style": "IPY_MODEL_4d558daf2bb744d08661cf4222f44557",
            "value": "spiece.model: 100%"
          }
        },
        "49a8b098dc884e6ba7f2b2b631cbc57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d51d220bcfd49a9b5e54899b91afce5",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e126a335640e4599ab364f81046e6fd0",
            "value": 791656
          }
        },
        "10cf2e431d5841c1a5b58ac1698f7a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6a75eb9e1294037837765ebe374167e",
            "placeholder": "​",
            "style": "IPY_MODEL_c643e959a14743b8b9bd6b6b20696c1a",
            "value": " 792k/792k [00:00&lt;00:00, 3.64MB/s]"
          }
        },
        "d0330849069644efa6807a675bfb5366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76758ac031d449ef805f54ac625afbcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d558daf2bb744d08661cf4222f44557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d51d220bcfd49a9b5e54899b91afce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e126a335640e4599ab364f81046e6fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6a75eb9e1294037837765ebe374167e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c643e959a14743b8b9bd6b6b20696c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0c4fad873a94b5580473fe1d784af92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc9ed71d00534708a2982d768ab553d6",
              "IPY_MODEL_aece68696c4d43618aeff04a8b6b78f9",
              "IPY_MODEL_cc95346d3bf246869041e73439002215"
            ],
            "layout": "IPY_MODEL_7056c046c77a4fc3a23d2a76bd682311"
          }
        },
        "cc9ed71d00534708a2982d768ab553d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46c6b4cfbad243869ddd8b11e01deba8",
            "placeholder": "​",
            "style": "IPY_MODEL_b1348a6ee01a4d1287ce4790cdc6b2c5",
            "value": "tokenizer.json: 100%"
          }
        },
        "aece68696c4d43618aeff04a8b6b78f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eba44ba12a95469b889dc9f75229d522",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc9d75725d194c5a8db4d969354a3c02",
            "value": 1389353
          }
        },
        "cc95346d3bf246869041e73439002215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2f700c3c9e45fc81d0b6c96aba0f04",
            "placeholder": "​",
            "style": "IPY_MODEL_505da97374634700a688fa1acf845d55",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 3.19MB/s]"
          }
        },
        "7056c046c77a4fc3a23d2a76bd682311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c6b4cfbad243869ddd8b11e01deba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1348a6ee01a4d1287ce4790cdc6b2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eba44ba12a95469b889dc9f75229d522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc9d75725d194c5a8db4d969354a3c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc2f700c3c9e45fc81d0b6c96aba0f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505da97374634700a688fa1acf845d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36391478761f4fd5b44571b032f6c5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_545430afd07b402cb8d3e83f40dbab7d",
              "IPY_MODEL_5eb890a3f5914ad69a1b3e607c843f6e",
              "IPY_MODEL_15facfc92b774a3abd6360c160d509a8"
            ],
            "layout": "IPY_MODEL_fb6d6f6dd0384da692b01cd9839ccd5d"
          }
        },
        "545430afd07b402cb8d3e83f40dbab7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeca2760cebd4c4bb60f134080dc9e65",
            "placeholder": "​",
            "style": "IPY_MODEL_e284793fa72545faa726777637219a14",
            "value": "config.json: 100%"
          }
        },
        "5eb890a3f5914ad69a1b3e607c843f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9e4a3e34c247d3891fbf7af8fa35d1",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28ef3552a73a45b2acb0477c8e9e7f82",
            "value": 1206
          }
        },
        "15facfc92b774a3abd6360c160d509a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48aeb49ac47747588562f133f01fd410",
            "placeholder": "​",
            "style": "IPY_MODEL_af40d99f3b0f4d00a6c8fda30855c617",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 141kB/s]"
          }
        },
        "fb6d6f6dd0384da692b01cd9839ccd5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeca2760cebd4c4bb60f134080dc9e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e284793fa72545faa726777637219a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e9e4a3e34c247d3891fbf7af8fa35d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ef3552a73a45b2acb0477c8e9e7f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48aeb49ac47747588562f133f01fd410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af40d99f3b0f4d00a6c8fda30855c617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcafc064c9db4789bd1017bf55c3411c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_422ae5b225cc445e8b2ff4c205a0fd43",
              "IPY_MODEL_7601bf32f5b44e25afbbe615ec4667ad",
              "IPY_MODEL_a83c21532b1f4c51b4255bb7ca89dcf5"
            ],
            "layout": "IPY_MODEL_52eb7efec91a4cdca1e2c0317d417211"
          }
        },
        "422ae5b225cc445e8b2ff4c205a0fd43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_146bb0bb682a49a6a33cebd06039f35c",
            "placeholder": "​",
            "style": "IPY_MODEL_f337d5d54e2f405ca67a72ce46446df7",
            "value": "model.safetensors: 100%"
          }
        },
        "7601bf32f5b44e25afbbe615ec4667ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2bfe5a4fe15450182c94f1b0861a935",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1671f1d229da42b698ecc5ef2ad587f2",
            "value": 242043056
          }
        },
        "a83c21532b1f4c51b4255bb7ca89dcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8e9e7d52db43b797ef540b16315a7e",
            "placeholder": "​",
            "style": "IPY_MODEL_fdbbbf521ffe456c927e46b9c0877525",
            "value": " 242M/242M [00:00&lt;00:00, 500MB/s]"
          }
        },
        "52eb7efec91a4cdca1e2c0317d417211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "146bb0bb682a49a6a33cebd06039f35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f337d5d54e2f405ca67a72ce46446df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2bfe5a4fe15450182c94f1b0861a935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1671f1d229da42b698ecc5ef2ad587f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa8e9e7d52db43b797ef540b16315a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdbbbf521ffe456c927e46b9c0877525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a4b89c5b1c44741adcc7cefbb40e8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26308484177b420ab007cb3ea8da0f9c",
              "IPY_MODEL_9ae24fc6cc954c1888d5d2e7c4deb86b",
              "IPY_MODEL_fb3ac4dbc0404bb0a334c8c69b11411a"
            ],
            "layout": "IPY_MODEL_0ad3a32d9ff543a2ac1790382c64306a"
          }
        },
        "26308484177b420ab007cb3ea8da0f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4da0a5afeef1412183f94c851abc72c5",
            "placeholder": "​",
            "style": "IPY_MODEL_fabe4dbbc5a34f5782e775d29eb6860b",
            "value": "generation_config.json: 100%"
          }
        },
        "9ae24fc6cc954c1888d5d2e7c4deb86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51918c4a22904c6391fe8ccbab2f2a0a",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05282e63009d451db8f9f7e2b21dd352",
            "value": 147
          }
        },
        "fb3ac4dbc0404bb0a334c8c69b11411a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c74d88051748a2afacf2392cbd80bb",
            "placeholder": "​",
            "style": "IPY_MODEL_11eb373da6e04312bb6823e44081b300",
            "value": " 147/147 [00:00&lt;00:00, 16.8kB/s]"
          }
        },
        "0ad3a32d9ff543a2ac1790382c64306a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4da0a5afeef1412183f94c851abc72c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabe4dbbc5a34f5782e775d29eb6860b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51918c4a22904c6391fe8ccbab2f2a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05282e63009d451db8f9f7e2b21dd352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58c74d88051748a2afacf2392cbd80bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11eb373da6e04312bb6823e44081b300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4396fa43c10a490fad244f6b50794fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e10ec6185d8446b9ad256cb06dbc2050",
              "IPY_MODEL_d11849f1f86b41d2802a757395e140e4",
              "IPY_MODEL_574a7aba61b24ebb8b574f021fff97ba"
            ],
            "layout": "IPY_MODEL_6a7188ef52784a06bd0e06b33d04d9f6"
          }
        },
        "e10ec6185d8446b9ad256cb06dbc2050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98629ddb8053494abfe0691dd9e47429",
            "placeholder": "​",
            "style": "IPY_MODEL_14f07dcbecaa46678ae989a5cfc7df34",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "d11849f1f86b41d2802a757395e140e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1a0416682dd40eda9ae278fdf5f9099",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2287d8ca8275461496737394762fd004",
            "value": 26788
          }
        },
        "574a7aba61b24ebb8b574f021fff97ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0a1d5d55a74c95a08fbe41b649c130",
            "placeholder": "​",
            "style": "IPY_MODEL_5becf270e2cf49069bd91b4dcd7e220a",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 2.58MB/s]"
          }
        },
        "6a7188ef52784a06bd0e06b33d04d9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98629ddb8053494abfe0691dd9e47429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f07dcbecaa46678ae989a5cfc7df34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1a0416682dd40eda9ae278fdf5f9099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2287d8ca8275461496737394762fd004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d0a1d5d55a74c95a08fbe41b649c130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5becf270e2cf49069bd91b4dcd7e220a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f6f2f2d5e0c4d1f91690db68f5f5a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aad55de7bc91419ba6319c305cd21902",
              "IPY_MODEL_6f2eed4b4e75480c9a10a240bf684e8e",
              "IPY_MODEL_012886d678dc414babb3811613e3564f"
            ],
            "layout": "IPY_MODEL_4724b5a5b709487998eb1a181410958e"
          }
        },
        "aad55de7bc91419ba6319c305cd21902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_165f145650bf4600bec86b6d22423763",
            "placeholder": "​",
            "style": "IPY_MODEL_246c3b399ad445409a0ffbfa1fc54872",
            "value": "Fetching 2 files: 100%"
          }
        },
        "6f2eed4b4e75480c9a10a240bf684e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_651c8a08c0784004aa198fe7dc1b7ea4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb4a570cfef147a0ab4fb5f7a52c8e91",
            "value": 2
          }
        },
        "012886d678dc414babb3811613e3564f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7fdfdb0522f43fc8dd18e50327be23d",
            "placeholder": "​",
            "style": "IPY_MODEL_fb452319855149ac9a92e58f6bc5073a",
            "value": " 2/2 [01:29&lt;00:00, 38.11s/it]"
          }
        },
        "4724b5a5b709487998eb1a181410958e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165f145650bf4600bec86b6d22423763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246c3b399ad445409a0ffbfa1fc54872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "651c8a08c0784004aa198fe7dc1b7ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4a570cfef147a0ab4fb5f7a52c8e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7fdfdb0522f43fc8dd18e50327be23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb452319855149ac9a92e58f6bc5073a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dd63d299ffd48c5a8bcb8aa0e11d58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e27503f510454d7da9b6110aa9246b9c",
              "IPY_MODEL_2f0f03acbc2f4c3084b88dcdb1b58bec",
              "IPY_MODEL_40e9462a951f4c88bb11595f3385afab"
            ],
            "layout": "IPY_MODEL_ea920350f6ce460185ae72c133741d0a"
          }
        },
        "e27503f510454d7da9b6110aa9246b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ae133d61d6d42fe8a945f59ea138612",
            "placeholder": "​",
            "style": "IPY_MODEL_7a6da5d5010147d58feb273ec3fb89e2",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "2f0f03acbc2f4c3084b88dcdb1b58bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f874f2a4cff14c288322c7d0a858f289",
            "max": 9976578928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70269740d5e64fd493b0ed6d98512796",
            "value": 9976578928
          }
        },
        "40e9462a951f4c88bb11595f3385afab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4430caaac83b47c382a37f87acbe551b",
            "placeholder": "​",
            "style": "IPY_MODEL_f95e3b608798450abdab1ea16b4571ed",
            "value": " 9.98G/9.98G [01:22&lt;00:00, 131MB/s]"
          }
        },
        "ea920350f6ce460185ae72c133741d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ae133d61d6d42fe8a945f59ea138612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6da5d5010147d58feb273ec3fb89e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f874f2a4cff14c288322c7d0a858f289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70269740d5e64fd493b0ed6d98512796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4430caaac83b47c382a37f87acbe551b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f95e3b608798450abdab1ea16b4571ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3b176a4470c4863ad320f7af8136584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fabcca4884374cfa9d860385a31e3575",
              "IPY_MODEL_d51d26576dae4c37ad11c7068e55c4db",
              "IPY_MODEL_37221acb31ca407e811dd5b124bd281f"
            ],
            "layout": "IPY_MODEL_43683853897842858c93f27bf81ce0b7"
          }
        },
        "fabcca4884374cfa9d860385a31e3575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c70ae2f803fb4a48aeb1c4b486643f17",
            "placeholder": "​",
            "style": "IPY_MODEL_9f0d0ffe5af445348a71052718c63e2c",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "d51d26576dae4c37ad11c7068e55c4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585c6097a37b4b19b4c94f94c147f06e",
            "max": 3500297344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a31647075a1410ba23387185c1701b9",
            "value": 3500297344
          }
        },
        "37221acb31ca407e811dd5b124bd281f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a95eb37ca4654aeea182b94265d8ccb7",
            "placeholder": "​",
            "style": "IPY_MODEL_f165029f557d4de18db3c9e49d9ad5bb",
            "value": " 3.50G/3.50G [01:29&lt;00:00, 42.1MB/s]"
          }
        },
        "43683853897842858c93f27bf81ce0b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c70ae2f803fb4a48aeb1c4b486643f17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0d0ffe5af445348a71052718c63e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "585c6097a37b4b19b4c94f94c147f06e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a31647075a1410ba23387185c1701b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a95eb37ca4654aeea182b94265d8ccb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f165029f557d4de18db3c9e49d9ad5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "164f042cddec4736ad68af7527edba83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe6708e0a20d438e98309961bff04e0c",
              "IPY_MODEL_7de1509d65a14fc58f2fbeaf9f3ec03f",
              "IPY_MODEL_1ecd9f77afe0446bb4debd5b6bf15ecc"
            ],
            "layout": "IPY_MODEL_1a50de3ae514415ab7c6d4620eca8f80"
          }
        },
        "fe6708e0a20d438e98309961bff04e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637decbe390545e4aaa4aaa8a38a9220",
            "placeholder": "​",
            "style": "IPY_MODEL_608734eb0f47490db08bd2a7bce83153",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7de1509d65a14fc58f2fbeaf9f3ec03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b58c704a2e44d92becff8620bf907ba",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f608d8962694cca88530d14717bc400",
            "value": 2
          }
        },
        "1ecd9f77afe0446bb4debd5b6bf15ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b4d91fbae794338b65ba48a1f086c5c",
            "placeholder": "​",
            "style": "IPY_MODEL_a1db1a0a7d624077b7adf064ea80e5cc",
            "value": " 2/2 [00:03&lt;00:00,  1.77s/it]"
          }
        },
        "1a50de3ae514415ab7c6d4620eca8f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637decbe390545e4aaa4aaa8a38a9220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608734eb0f47490db08bd2a7bce83153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b58c704a2e44d92becff8620bf907ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f608d8962694cca88530d14717bc400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b4d91fbae794338b65ba48a1f086c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1db1a0a7d624077b7adf064ea80e5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62244a53b0794e33ae638263186de76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c692652f048400dbc39deb5cffd891b",
              "IPY_MODEL_227014cf382a4e179ad5735bf029476b",
              "IPY_MODEL_63e9269d0d1c4407a5644320de51d3b3"
            ],
            "layout": "IPY_MODEL_93be8b0cf82847039790cc6cc6aa2441"
          }
        },
        "2c692652f048400dbc39deb5cffd891b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5adf6838954e4bbaf6b10789e6ebed",
            "placeholder": "​",
            "style": "IPY_MODEL_ad3c8e5761224de79852721bd4e8015b",
            "value": "generation_config.json: 100%"
          }
        },
        "227014cf382a4e179ad5735bf029476b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd6f7a8cb38d45818765763a7841df74",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8045fbd05414b6794f0922c49b9f7a0",
            "value": 188
          }
        },
        "63e9269d0d1c4407a5644320de51d3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_760b58ab23eb48759c85df7dc58cc7bd",
            "placeholder": "​",
            "style": "IPY_MODEL_b50b514ee703411f82edec018d7e0356",
            "value": " 188/188 [00:00&lt;00:00, 24.1kB/s]"
          }
        },
        "93be8b0cf82847039790cc6cc6aa2441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5adf6838954e4bbaf6b10789e6ebed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3c8e5761224de79852721bd4e8015b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd6f7a8cb38d45818765763a7841df74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8045fbd05414b6794f0922c49b9f7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "760b58ab23eb48759c85df7dc58cc7bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b50b514ee703411f82edec018d7e0356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcaedee12fae4933822b5615a8a15193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f01f2b5047cb4042890901c390e4361b",
              "IPY_MODEL_0f1827983ebf41a6b0aee1f91594e73a",
              "IPY_MODEL_e8cd7349be0742b29ba7bc66bae460f7"
            ],
            "layout": "IPY_MODEL_34affcd31d17444cb5aad2324c289b52"
          }
        },
        "f01f2b5047cb4042890901c390e4361b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf3210ef94249f284c820ce960531a5",
            "placeholder": "​",
            "style": "IPY_MODEL_3a78d3f02a2b4163bd6bfb5f2a315eac",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "0f1827983ebf41a6b0aee1f91594e73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cca6775aa82949a9aec4a92da7ca84e8",
            "max": 776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa537f5a8a29485abd0f35f0662dc8e4",
            "value": 776
          }
        },
        "e8cd7349be0742b29ba7bc66bae460f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d66adc435b4aed822e913bce61a301",
            "placeholder": "​",
            "style": "IPY_MODEL_cf28132f929f4f3c921a11a7aa7fe69c",
            "value": " 776/776 [00:00&lt;00:00, 111kB/s]"
          }
        },
        "34affcd31d17444cb5aad2324c289b52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf3210ef94249f284c820ce960531a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a78d3f02a2b4163bd6bfb5f2a315eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cca6775aa82949a9aec4a92da7ca84e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa537f5a8a29485abd0f35f0662dc8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24d66adc435b4aed822e913bce61a301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf28132f929f4f3c921a11a7aa7fe69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e396bbc7ce0464bb0e0f12c447d92a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae01a33bd3f7440b896d6d0542c50c21",
              "IPY_MODEL_ad3a5ad5f06443ef87e0d18cea0c5142",
              "IPY_MODEL_e6d842f5a9f94ae3a96fb151226e7375"
            ],
            "layout": "IPY_MODEL_418ed1fc468849008c69eb92cb6d7007"
          }
        },
        "ae01a33bd3f7440b896d6d0542c50c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44e698c55558476786c6fe730784ad83",
            "placeholder": "​",
            "style": "IPY_MODEL_78b076f98e8a4cbaa69c84b247b58ff1",
            "value": "tokenizer.model: 100%"
          }
        },
        "ad3a5ad5f06443ef87e0d18cea0c5142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7723db8c14cd484eb9e9f1e3a7a5ee63",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84e1b22df86141e285f499f6ea2fe93d",
            "value": 499723
          }
        },
        "e6d842f5a9f94ae3a96fb151226e7375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cebc86f3121345f2bbc4279a3291e6e6",
            "placeholder": "​",
            "style": "IPY_MODEL_71a917dd3b8047e88c7f1d262f5bdcc4",
            "value": " 500k/500k [00:00&lt;00:00, 7.68MB/s]"
          }
        },
        "418ed1fc468849008c69eb92cb6d7007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e698c55558476786c6fe730784ad83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b076f98e8a4cbaa69c84b247b58ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7723db8c14cd484eb9e9f1e3a7a5ee63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e1b22df86141e285f499f6ea2fe93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cebc86f3121345f2bbc4279a3291e6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a917dd3b8047e88c7f1d262f5bdcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c17f1ea5064482f8a68b0d91411522b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c70b97ec8de4681bcdb665f56dac379",
              "IPY_MODEL_7d4a65b3e74147ff9983030e75f98b8f",
              "IPY_MODEL_db544e75a98e4fdc9e4189c8dedc00f1"
            ],
            "layout": "IPY_MODEL_e302a3ed8b35458ea7cc9e9af982b03a"
          }
        },
        "2c70b97ec8de4681bcdb665f56dac379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df838bb368648a39eb34fcfd531e175",
            "placeholder": "​",
            "style": "IPY_MODEL_a219eaf6fc5044a2b6f4760caed17a19",
            "value": "tokenizer.json: 100%"
          }
        },
        "7d4a65b3e74147ff9983030e75f98b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d12172f56c3c4e5d9566c738832d7ee7",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a1fa1f5d7f149b78db591878cc2c67d",
            "value": 1842767
          }
        },
        "db544e75a98e4fdc9e4189c8dedc00f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d21279112a464e97455813d48e7098",
            "placeholder": "​",
            "style": "IPY_MODEL_a1d6dbc523414d1689a63f4570376e95",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 28.6MB/s]"
          }
        },
        "e302a3ed8b35458ea7cc9e9af982b03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9df838bb368648a39eb34fcfd531e175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a219eaf6fc5044a2b6f4760caed17a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12172f56c3c4e5d9566c738832d7ee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a1fa1f5d7f149b78db591878cc2c67d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55d21279112a464e97455813d48e7098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d6dbc523414d1689a63f4570376e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0787ffd8ca29458eb0675b04a5900dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a7a8fcb94134deeac5752bf76a6b5e9",
              "IPY_MODEL_d3e93071cbcf4e41ac9d46eca35d1b8d",
              "IPY_MODEL_74bff334786c4a85a2f3192f6d83b685"
            ],
            "layout": "IPY_MODEL_feb0ac615f544701a745fb2e71b298e1"
          }
        },
        "0a7a8fcb94134deeac5752bf76a6b5e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53d27358e4cc4711be34bf32952bf5e4",
            "placeholder": "​",
            "style": "IPY_MODEL_e7272cd9fa604405b3f5d507659437e9",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d3e93071cbcf4e41ac9d46eca35d1b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17a6f411b9345d49c6d18d88202b2ae",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e7cc2543a44c81afd779053cd8cbc4",
            "value": 414
          }
        },
        "74bff334786c4a85a2f3192f6d83b685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79f538758a347fc9b8f83a1c09bb247",
            "placeholder": "​",
            "style": "IPY_MODEL_2d02a5953bde4738bde09f15d56cef0c",
            "value": " 414/414 [00:00&lt;00:00, 56.8kB/s]"
          }
        },
        "feb0ac615f544701a745fb2e71b298e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d27358e4cc4711be34bf32952bf5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7272cd9fa604405b3f5d507659437e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d17a6f411b9345d49c6d18d88202b2ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e7cc2543a44c81afd779053cd8cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f79f538758a347fc9b8f83a1c09bb247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d02a5953bde4738bde09f15d56cef0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b535606670e47a693b8493b80868b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7026461057ba404088516da40cf91918",
              "IPY_MODEL_ef96cbc306804d53b55f79f6085e9c8a",
              "IPY_MODEL_6e1af3e900184bb7bd765f43d2af2368"
            ],
            "layout": "IPY_MODEL_5b5da60fe402443a895589d3f947ecbf"
          }
        },
        "7026461057ba404088516da40cf91918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_747731b4eeef4adf9f8c8a8954e30ad9",
            "placeholder": "​",
            "style": "IPY_MODEL_558ed476e3324fa1a7c9172f5d33aa33",
            "value": "Map: 100%"
          }
        },
        "ef96cbc306804d53b55f79f6085e9c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c01a20b0179c4047b3d32f4d45c5334c",
            "max": 14425,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a9480466040428aa696a921bf083ced",
            "value": 14425
          }
        },
        "6e1af3e900184bb7bd765f43d2af2368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c199c1307a454d0e8ab5d3aa396363c4",
            "placeholder": "​",
            "style": "IPY_MODEL_1304176e091d469ba33eb13be81e9b2b",
            "value": " 14425/14425 [00:02&lt;00:00, 5072.25 examples/s]"
          }
        },
        "5b5da60fe402443a895589d3f947ecbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "747731b4eeef4adf9f8c8a8954e30ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558ed476e3324fa1a7c9172f5d33aa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c01a20b0179c4047b3d32f4d45c5334c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a9480466040428aa696a921bf083ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c199c1307a454d0e8ab5d3aa396363c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1304176e091d469ba33eb13be81e9b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b938145684c46d3ae90b2765aa02512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f9f2b8d1d20458aa62ed7a41e81139c",
              "IPY_MODEL_946c7f91f000492dba43a5ce087aa551",
              "IPY_MODEL_d8e454f3385d4686a6d54d89b8f448f0"
            ],
            "layout": "IPY_MODEL_746a88cfd3f04e7c8776478cdef19ce0"
          }
        },
        "0f9f2b8d1d20458aa62ed7a41e81139c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5254edc543144f2a90215033acbe4be6",
            "placeholder": "​",
            "style": "IPY_MODEL_7c37becf799b4ee89c4735fd0aa60b01",
            "value": "Map: 100%"
          }
        },
        "946c7f91f000492dba43a5ce087aa551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b8b80404e047d7803acbfb0d6f01a9",
            "max": 1803,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4f6a61f89a946c8aa249de1c104df79",
            "value": 1803
          }
        },
        "d8e454f3385d4686a6d54d89b8f448f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce5de9d896754473aa9b06abb2bd4c24",
            "placeholder": "​",
            "style": "IPY_MODEL_8b9031e6fefd4f3e839398a086d9cc2e",
            "value": " 1803/1803 [00:00&lt;00:00, 4672.68 examples/s]"
          }
        },
        "746a88cfd3f04e7c8776478cdef19ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5254edc543144f2a90215033acbe4be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c37becf799b4ee89c4735fd0aa60b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32b8b80404e047d7803acbfb0d6f01a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4f6a61f89a946c8aa249de1c104df79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce5de9d896754473aa9b06abb2bd4c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b9031e6fefd4f3e839398a086d9cc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "480bbab3517142efa483ea9c6faf13ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4531a0ac1d8477891d592791f3c30e5",
              "IPY_MODEL_2c5c5ca66f734f0fb2a864ecb4851eb4",
              "IPY_MODEL_fbfa417afa324928aa936044512bb0e2"
            ],
            "layout": "IPY_MODEL_2765a88eb82e4d89a61f0b3b21588095"
          }
        },
        "b4531a0ac1d8477891d592791f3c30e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfe85813b0804300aa0eda7c49ac1472",
            "placeholder": "​",
            "style": "IPY_MODEL_3746fe474be34c9d8fb774e0e5b07de4",
            "value": "Map: 100%"
          }
        },
        "2c5c5ca66f734f0fb2a864ecb4851eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac703191715348ad87abe45c48f12f7b",
            "max": 1803,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_783c67e023d041a5b19d866dbe955024",
            "value": 1803
          }
        },
        "fbfa417afa324928aa936044512bb0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c8687851e7a42efa34280cc4f39e280",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c06bf0c03243499f85bd8d16198b34",
            "value": " 1803/1803 [00:00&lt;00:00, 4649.72 examples/s]"
          }
        },
        "2765a88eb82e4d89a61f0b3b21588095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfe85813b0804300aa0eda7c49ac1472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3746fe474be34c9d8fb774e0e5b07de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac703191715348ad87abe45c48f12f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "783c67e023d041a5b19d866dbe955024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c8687851e7a42efa34280cc4f39e280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c06bf0c03243499f85bd8d16198b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "601cf8fa62824fb5a88cb87d5a179b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b1272275b344eb195d68b2bb034c600",
              "IPY_MODEL_df660fe15c184f4a8fc75026bd4a02d8",
              "IPY_MODEL_90fcce6bb7504d9aaece3853c4454b64"
            ],
            "layout": "IPY_MODEL_aa2c54aba8204cb0aac691daea0bf1a5"
          }
        },
        "7b1272275b344eb195d68b2bb034c600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98892c3f7ef460da46a81fbda4d172c",
            "placeholder": "​",
            "style": "IPY_MODEL_7e4e0ca9328f487eb37514f99b4ce69e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "df660fe15c184f4a8fc75026bd4a02d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_247f9646b8df4382ba605731e7cc5c55",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09b4adb85bc547cc94d844eb11cbdfe7",
            "value": 2
          }
        },
        "90fcce6bb7504d9aaece3853c4454b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35e8c6f5d6ec4265a554733831983d33",
            "placeholder": "​",
            "style": "IPY_MODEL_f0a83a7fb5f94a91aaeb2ccb9387a986",
            "value": " 2/2 [00:04&lt;00:00,  1.84s/it]"
          }
        },
        "aa2c54aba8204cb0aac691daea0bf1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98892c3f7ef460da46a81fbda4d172c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4e0ca9328f487eb37514f99b4ce69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "247f9646b8df4382ba605731e7cc5c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b4adb85bc547cc94d844eb11cbdfe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35e8c6f5d6ec4265a554733831983d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a83a7fb5f94a91aaeb2ccb9387a986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "083cd610783645a9b7998e450833932a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44e047da85b04a81879f0e9417e8be58",
              "IPY_MODEL_d6066f54a70e4522862de406ff3e190d",
              "IPY_MODEL_3b076cb15fb74122ae9325a2aa2bc575"
            ],
            "layout": "IPY_MODEL_0208adeb492c4ac4adfb414df590d427"
          }
        },
        "44e047da85b04a81879f0e9417e8be58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3464664f8994d7390aa6bce8770905c",
            "placeholder": "​",
            "style": "IPY_MODEL_7f2b0d7c6c71472397345480b1baa9d0",
            "value": "Map: 100%"
          }
        },
        "d6066f54a70e4522862de406ff3e190d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22e9bc91d8ac4622b501ac5ad067ab32",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_317adb5428a34d04a3e9e03e6ff23a74",
            "value": 500
          }
        },
        "3b076cb15fb74122ae9325a2aa2bc575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239e05efcc004e50ad1bca956170bda9",
            "placeholder": "​",
            "style": "IPY_MODEL_41b4b8bb9c0b4e22befd2ce0c595581e",
            "value": " 500/500 [00:00&lt;00:00, 3106.02 examples/s]"
          }
        },
        "0208adeb492c4ac4adfb414df590d427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3464664f8994d7390aa6bce8770905c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f2b0d7c6c71472397345480b1baa9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22e9bc91d8ac4622b501ac5ad067ab32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "317adb5428a34d04a3e9e03e6ff23a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "239e05efcc004e50ad1bca956170bda9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b4b8bb9c0b4e22befd2ce0c595581e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6afff58d763343ffb4e06197d2398417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd92083587bf45a1bae5a9b128c765a2",
              "IPY_MODEL_48de575f93d341b6961b7c2ba582674a",
              "IPY_MODEL_0a01d094933f4c848c7c648cddea407f"
            ],
            "layout": "IPY_MODEL_6859cae5237749168caf8c72d5822960"
          }
        },
        "fd92083587bf45a1bae5a9b128c765a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e631c3a11940bda33d193d8d55a91a",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e7e3f5e864494495022ccd24b1fb62",
            "value": "Map: 100%"
          }
        },
        "48de575f93d341b6961b7c2ba582674a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24d5b1c0501f48138bd35f9681bc797d",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_898321c9a11c40b9b893f57a4561afcd",
            "value": 414
          }
        },
        "0a01d094933f4c848c7c648cddea407f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80a2afd4b5bc41c48ae9953846dab6e9",
            "placeholder": "​",
            "style": "IPY_MODEL_fdf8265a56d64735bc33a690c887064f",
            "value": " 414/414 [00:00&lt;00:00, 3840.59 examples/s]"
          }
        },
        "6859cae5237749168caf8c72d5822960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e631c3a11940bda33d193d8d55a91a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e7e3f5e864494495022ccd24b1fb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24d5b1c0501f48138bd35f9681bc797d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "898321c9a11c40b9b893f57a4561afcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80a2afd4b5bc41c48ae9953846dab6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf8265a56d64735bc33a690c887064f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b77835cb376c4e6e88e80f78bfd90d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a80e3fe44dee4f69a61ecba991b1b095",
              "IPY_MODEL_653ce373c1604db58be15bbeb24d61f0",
              "IPY_MODEL_6bb80f34ee7e460b91cbca5bdf2504aa"
            ],
            "layout": "IPY_MODEL_09e52f26d0fe4f29bc33d190c958c570"
          }
        },
        "a80e3fe44dee4f69a61ecba991b1b095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6462c37341254c9dbffba655bdbaa630",
            "placeholder": "​",
            "style": "IPY_MODEL_616a00eb0655405889f4b7f34246d146",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "653ce373c1604db58be15bbeb24d61f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_792d8c5b6c3c4dfc948e4f4300f5807c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5e4cb14655445a3b8459f022b216ab6",
            "value": 2
          }
        },
        "6bb80f34ee7e460b91cbca5bdf2504aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7382e6a0e7f47d9b4e88487e4801fe5",
            "placeholder": "​",
            "style": "IPY_MODEL_1fc514eba36f41e9be1fbf5fe23447b1",
            "value": " 2/2 [00:03&lt;00:00,  1.82s/it]"
          }
        },
        "09e52f26d0fe4f29bc33d190c958c570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6462c37341254c9dbffba655bdbaa630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "616a00eb0655405889f4b7f34246d146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "792d8c5b6c3c4dfc948e4f4300f5807c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5e4cb14655445a3b8459f022b216ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7382e6a0e7f47d9b4e88487e4801fe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc514eba36f41e9be1fbf5fe23447b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4643bf871554643a4275f5f47d299b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5fe3792867745d58d0ad38b8c1ade68",
              "IPY_MODEL_463a0c88eef443228715fedac6473fcb",
              "IPY_MODEL_a80dba283f964fd6a12c52c9b9f9a171"
            ],
            "layout": "IPY_MODEL_41575976371c4061afe24878cf7404fb"
          }
        },
        "c5fe3792867745d58d0ad38b8c1ade68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658453ecf2024545ada278ae17a2b995",
            "placeholder": "​",
            "style": "IPY_MODEL_f9efbe1bec76434ebf9b3844c5362e07",
            "value": "Map: 100%"
          }
        },
        "463a0c88eef443228715fedac6473fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239a3472de594dcc91870c96f2d002af",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_391a959d88de4f8b9b9bbaae520d3599",
            "value": 500
          }
        },
        "a80dba283f964fd6a12c52c9b9f9a171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca781216344b4c0bbda5f3ec4b8bc0e0",
            "placeholder": "​",
            "style": "IPY_MODEL_0888b52e3c0540d38ca5c932b8709425",
            "value": " 500/500 [00:00&lt;00:00, 4002.00 examples/s]"
          }
        },
        "41575976371c4061afe24878cf7404fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "658453ecf2024545ada278ae17a2b995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9efbe1bec76434ebf9b3844c5362e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "239a3472de594dcc91870c96f2d002af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391a959d88de4f8b9b9bbaae520d3599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca781216344b4c0bbda5f3ec4b8bc0e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0888b52e3c0540d38ca5c932b8709425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fdc4bf37be340c28b75c20d002357c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_251e3bcc10754548b1186963481f1509",
              "IPY_MODEL_047a10ebb20048dc932d047ae61b616d",
              "IPY_MODEL_6316a82d393047a89f6416f05d916fae"
            ],
            "layout": "IPY_MODEL_c8a0b48d42ef4241b5cda34f1a5d4744"
          }
        },
        "251e3bcc10754548b1186963481f1509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5d12aca3fb40378260d93299a6059b",
            "placeholder": "​",
            "style": "IPY_MODEL_e3f3628ded8a474e899af4e742832b26",
            "value": "Map: 100%"
          }
        },
        "047a10ebb20048dc932d047ae61b616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b431613cf3cd46219b451ded3fc9909b",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_339e7a3e01bd46a38ea5f5b6b31ce836",
            "value": 414
          }
        },
        "6316a82d393047a89f6416f05d916fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb9e43db329a458b8fb0f890b7f118d9",
            "placeholder": "​",
            "style": "IPY_MODEL_c80032adb153415792d27d4aa57605a6",
            "value": " 414/414 [00:00&lt;00:00, 4752.51 examples/s]"
          }
        },
        "c8a0b48d42ef4241b5cda34f1a5d4744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5d12aca3fb40378260d93299a6059b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f3628ded8a474e899af4e742832b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b431613cf3cd46219b451ded3fc9909b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "339e7a3e01bd46a38ea5f5b6b31ce836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb9e43db329a458b8fb0f890b7f118d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80032adb153415792d27d4aa57605a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2104ac0474514f95a42b7387d42e9366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f653cb3630440a6805bc562bcb2034e",
              "IPY_MODEL_388d6106279b4189bb432e836709caf3",
              "IPY_MODEL_a6c3e43533394cfe9cfe9752c527afaf"
            ],
            "layout": "IPY_MODEL_c62cf984240d446ab1c8a71f11b5df66"
          }
        },
        "4f653cb3630440a6805bc562bcb2034e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9ccc2dc211f477eae075da8fcd66c25",
            "placeholder": "​",
            "style": "IPY_MODEL_7f0f2eccec6842c1ae30ba24f5831d80",
            "value": "config.json: 100%"
          }
        },
        "388d6106279b4189bb432e836709caf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83cdf57335024de8b50cea9e47b8471a",
            "max": 609,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9024f713e3ae4920b5e3916a77840f93",
            "value": 609
          }
        },
        "a6c3e43533394cfe9cfe9752c527afaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5842521be4044011ba9da4ae4267f0a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5330e3adec2445fea7acbe8c4f78c1e9",
            "value": " 609/609 [00:00&lt;00:00, 63.9kB/s]"
          }
        },
        "c62cf984240d446ab1c8a71f11b5df66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9ccc2dc211f477eae075da8fcd66c25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0f2eccec6842c1ae30ba24f5831d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83cdf57335024de8b50cea9e47b8471a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9024f713e3ae4920b5e3916a77840f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5842521be4044011ba9da4ae4267f0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5330e3adec2445fea7acbe8c4f78c1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc58278bc1c943139288394f19e641f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61da9ab943b04409a4ea8af77751e7dc",
              "IPY_MODEL_46afde21174c423eb085dad74fc3c1d8",
              "IPY_MODEL_d5cd70f7bde44c11af47a0cbdfa9a3ed"
            ],
            "layout": "IPY_MODEL_eac198c2b76d4ed4938415886c27f9d9"
          }
        },
        "61da9ab943b04409a4ea8af77751e7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1679285db54f4281a833477df5d125e1",
            "placeholder": "​",
            "style": "IPY_MODEL_fff7b6e991cb4bfe997a8b6e2ad9371f",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "46afde21174c423eb085dad74fc3c1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68815aeed48443a78105b0a902ed282b",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba30a19e85f3485fa86f267d56368c7f",
            "value": 26788
          }
        },
        "d5cd70f7bde44c11af47a0cbdfa9a3ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ff68682e8de4db5951dcd3988214262",
            "placeholder": "​",
            "style": "IPY_MODEL_92850207383344f7902d04af5550baec",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 3.06MB/s]"
          }
        },
        "eac198c2b76d4ed4938415886c27f9d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1679285db54f4281a833477df5d125e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fff7b6e991cb4bfe997a8b6e2ad9371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68815aeed48443a78105b0a902ed282b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba30a19e85f3485fa86f267d56368c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ff68682e8de4db5951dcd3988214262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92850207383344f7902d04af5550baec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "370bf3d5f1cc4bd8942ab21cf521309e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba79a579296e4b9a8447e51a94fbf24f",
              "IPY_MODEL_70541e196fe6424196f1cbe7b827af9b",
              "IPY_MODEL_072c15aba3a84bb1baa129079a0db61b"
            ],
            "layout": "IPY_MODEL_d6ef0af66c7e4a7ea5ee169fa4e45cd0"
          }
        },
        "ba79a579296e4b9a8447e51a94fbf24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc6eece1056742a7a341fc0a1e9026e7",
            "placeholder": "​",
            "style": "IPY_MODEL_d125aa763c8c4ee9ae37eb692d0c86c7",
            "value": "Fetching 2 files: 100%"
          }
        },
        "70541e196fe6424196f1cbe7b827af9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44befd36156b4e06927668e7922134a2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c00a78b174584d14a350609df563b530",
            "value": 2
          }
        },
        "072c15aba3a84bb1baa129079a0db61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79d57bba3be840048baf6e2e149bd6c6",
            "placeholder": "​",
            "style": "IPY_MODEL_f4530ecfa6634b06ad1e87b406049a7c",
            "value": " 2/2 [01:03&lt;00:00, 63.89s/it]"
          }
        },
        "d6ef0af66c7e4a7ea5ee169fa4e45cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6eece1056742a7a341fc0a1e9026e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d125aa763c8c4ee9ae37eb692d0c86c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44befd36156b4e06927668e7922134a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c00a78b174584d14a350609df563b530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79d57bba3be840048baf6e2e149bd6c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4530ecfa6634b06ad1e87b406049a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60fff09ee5ff40b284dfe5413cc88c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bda87ab57cbd4467a684381d5c0a3fb0",
              "IPY_MODEL_24410c244b5542818084333ade45b123",
              "IPY_MODEL_73b2deb0bf5e4073be2fa97c14ef2672"
            ],
            "layout": "IPY_MODEL_07c56fce14f048878ade9446514f0043"
          }
        },
        "bda87ab57cbd4467a684381d5c0a3fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85cac8c8c27542798593d913eb9fcb4d",
            "placeholder": "​",
            "style": "IPY_MODEL_0629e9ea1c77429094756ad394aa7214",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "24410c244b5542818084333ade45b123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e973fd15380408c930d5077fcaf6306",
            "max": 9976578928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0952c6c442b4440da196ca5a9ef25f19",
            "value": 9976578928
          }
        },
        "73b2deb0bf5e4073be2fa97c14ef2672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7bf7c7d91644330aec5e4fbfd480ea5",
            "placeholder": "​",
            "style": "IPY_MODEL_cd416835d7634737be322b41a6c44e04",
            "value": " 9.98G/9.98G [01:03&lt;00:00, 161MB/s]"
          }
        },
        "07c56fce14f048878ade9446514f0043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85cac8c8c27542798593d913eb9fcb4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0629e9ea1c77429094756ad394aa7214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e973fd15380408c930d5077fcaf6306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0952c6c442b4440da196ca5a9ef25f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7bf7c7d91644330aec5e4fbfd480ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd416835d7634737be322b41a6c44e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0889bab6506c461b8d2dea77c8c0c96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edbf4c1558e04f169fe610d82803c95a",
              "IPY_MODEL_e445b20fdfde461a8318a0a12e74fa7d",
              "IPY_MODEL_5c6002933a2344f6b9a1abff524aa020"
            ],
            "layout": "IPY_MODEL_962fbba7f27c4a26acb7043be97faf18"
          }
        },
        "edbf4c1558e04f169fe610d82803c95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c62d11ec474485d87a24e67834ae793",
            "placeholder": "​",
            "style": "IPY_MODEL_86bba8f813324658b983caaef96dc0b7",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "e445b20fdfde461a8318a0a12e74fa7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39a97b64431046798ffc5185bbef594b",
            "max": 3500297344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b0d287fafee4ebcbf873bc08b4f1977",
            "value": 3500297344
          }
        },
        "5c6002933a2344f6b9a1abff524aa020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef61e6d333cf42e7bbcfb24b84c0941c",
            "placeholder": "​",
            "style": "IPY_MODEL_e2d4cc535b2843a1afab2fb85a1646e1",
            "value": " 3.50G/3.50G [00:24&lt;00:00, 149MB/s]"
          }
        },
        "962fbba7f27c4a26acb7043be97faf18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c62d11ec474485d87a24e67834ae793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86bba8f813324658b983caaef96dc0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39a97b64431046798ffc5185bbef594b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0d287fafee4ebcbf873bc08b4f1977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef61e6d333cf42e7bbcfb24b84c0941c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d4cc535b2843a1afab2fb85a1646e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd07acf8e0394f888b426177d43c43d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f423b93fef741dbb33fc558bf6c6f35",
              "IPY_MODEL_3aa8766830c744e5a49a3fc3552a5903",
              "IPY_MODEL_933f26ac67e34443b6cb861d16bef8a7"
            ],
            "layout": "IPY_MODEL_4ee79de79aeb491fb4b6c81e7f06cb9d"
          }
        },
        "8f423b93fef741dbb33fc558bf6c6f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69b4b6d020174059919211c9f607143c",
            "placeholder": "​",
            "style": "IPY_MODEL_665f8a3187274ddba3fdbaf205aaa2d1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3aa8766830c744e5a49a3fc3552a5903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1156e3d625ae4bbdb1f1d7b433c86016",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f73758bd5194beb9c796efe75e81894",
            "value": 2
          }
        },
        "933f26ac67e34443b6cb861d16bef8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f95a368225f49a38af69aef77ec9b12",
            "placeholder": "​",
            "style": "IPY_MODEL_d3088cd9082347e1943bcb63bc80f3ad",
            "value": " 2/2 [00:03&lt;00:00,  1.77s/it]"
          }
        },
        "4ee79de79aeb491fb4b6c81e7f06cb9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b4b6d020174059919211c9f607143c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "665f8a3187274ddba3fdbaf205aaa2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1156e3d625ae4bbdb1f1d7b433c86016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f73758bd5194beb9c796efe75e81894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f95a368225f49a38af69aef77ec9b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3088cd9082347e1943bcb63bc80f3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55dfc7822c074e80a6f6017a09fe6536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15d6e28c34c44902be5e716694686eec",
              "IPY_MODEL_ae4fa3f1211a4245904dcf3f3748fd73",
              "IPY_MODEL_571a97bab30c471f9ad741dec110db61"
            ],
            "layout": "IPY_MODEL_ff6e1f733efb47238db9a6b71d9d8a06"
          }
        },
        "15d6e28c34c44902be5e716694686eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162c6cfd08ac47e592e472c4aad9f4b0",
            "placeholder": "​",
            "style": "IPY_MODEL_ecfa65f5d6f74dde845d2d08ede7255d",
            "value": "generation_config.json: 100%"
          }
        },
        "ae4fa3f1211a4245904dcf3f3748fd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50c22ffbc5e4736883f05383aea33f7",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_007f55f1c7004a09894cebe60b12d068",
            "value": 188
          }
        },
        "571a97bab30c471f9ad741dec110db61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0a975298f6041bab6555e6508183197",
            "placeholder": "​",
            "style": "IPY_MODEL_5cde6d237df045c7b430080db545db16",
            "value": " 188/188 [00:00&lt;00:00, 24.0kB/s]"
          }
        },
        "ff6e1f733efb47238db9a6b71d9d8a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162c6cfd08ac47e592e472c4aad9f4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecfa65f5d6f74dde845d2d08ede7255d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f50c22ffbc5e4736883f05383aea33f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007f55f1c7004a09894cebe60b12d068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0a975298f6041bab6555e6508183197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cde6d237df045c7b430080db545db16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5da136472eda4752ba80fb1819a823d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04639b53ee9445d4acd22aa6d2e8e2b3",
              "IPY_MODEL_89d273e95dd54c05aee1bd8aeb8c5678",
              "IPY_MODEL_527bef38aac94be388d22e8373fa2f66"
            ],
            "layout": "IPY_MODEL_f9c215aadbaf4a9fa177a8d7d1a50e46"
          }
        },
        "04639b53ee9445d4acd22aa6d2e8e2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b221e5ad08dd4e3e92cee6765f600142",
            "placeholder": "​",
            "style": "IPY_MODEL_6b922e30dbfb4651aa473526e56093d7",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "89d273e95dd54c05aee1bd8aeb8c5678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c373caabe22047678f2fe5e874110d10",
            "max": 776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38d9034531b24c068367482f7790ff87",
            "value": 776
          }
        },
        "527bef38aac94be388d22e8373fa2f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86cbbc1750e6477287eed7b1bcdb74f0",
            "placeholder": "​",
            "style": "IPY_MODEL_50506f0027c1407486216c994ea53a6f",
            "value": " 776/776 [00:00&lt;00:00, 100kB/s]"
          }
        },
        "f9c215aadbaf4a9fa177a8d7d1a50e46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b221e5ad08dd4e3e92cee6765f600142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b922e30dbfb4651aa473526e56093d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c373caabe22047678f2fe5e874110d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d9034531b24c068367482f7790ff87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86cbbc1750e6477287eed7b1bcdb74f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50506f0027c1407486216c994ea53a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44c92c499e024b55bd84977fe6991c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38bbd1b7c4624b27ba183aac63a55f5d",
              "IPY_MODEL_2e88255d47e845e58e27aac3fdc526f9",
              "IPY_MODEL_422d8adbfa19476783fcb7519de66ee4"
            ],
            "layout": "IPY_MODEL_77fc5c6896b04b79b75b9d0ab7a30621"
          }
        },
        "38bbd1b7c4624b27ba183aac63a55f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df015e66efd845cf973e14563da7d155",
            "placeholder": "​",
            "style": "IPY_MODEL_bbe0b3f463c04ae1ae6c48c4948409f8",
            "value": "tokenizer.model: 100%"
          }
        },
        "2e88255d47e845e58e27aac3fdc526f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5110abbaf154a378b8fa5bf6dd1e332",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc7ae0582bef4eb3a37c2d1fb040fe8f",
            "value": 499723
          }
        },
        "422d8adbfa19476783fcb7519de66ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5bd5bd62a3348b0ae8eaa7f8282621a",
            "placeholder": "​",
            "style": "IPY_MODEL_25cceca2b2b34e55859f0de5c3363056",
            "value": " 500k/500k [00:00&lt;00:00, 5.57MB/s]"
          }
        },
        "77fc5c6896b04b79b75b9d0ab7a30621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df015e66efd845cf973e14563da7d155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe0b3f463c04ae1ae6c48c4948409f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5110abbaf154a378b8fa5bf6dd1e332": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc7ae0582bef4eb3a37c2d1fb040fe8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5bd5bd62a3348b0ae8eaa7f8282621a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25cceca2b2b34e55859f0de5c3363056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd8fc903ef264506bad7517da90fe56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3b5d675806d4e37b090dcb019eb5a35",
              "IPY_MODEL_c14797faacb346a1983a8aa3d1bd76a6",
              "IPY_MODEL_31565735535a4ec6957be6cdc36a58b0"
            ],
            "layout": "IPY_MODEL_52af6ece88b246dcaa0b0aaeda6cd58d"
          }
        },
        "a3b5d675806d4e37b090dcb019eb5a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49ed7df865ef45a2806bb99928a22ed5",
            "placeholder": "​",
            "style": "IPY_MODEL_7c877386ef654584a6756b0f28035e4a",
            "value": "tokenizer.json: 100%"
          }
        },
        "c14797faacb346a1983a8aa3d1bd76a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d32640162774e959d1522e50334f7fe",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78b7b608070b4b05be1b48b2e773587f",
            "value": 1842767
          }
        },
        "31565735535a4ec6957be6cdc36a58b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6368df6c6a5c45f48f1310ab5740df45",
            "placeholder": "​",
            "style": "IPY_MODEL_946532b3bb204b6a85ea7b31c43a150d",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 16.6MB/s]"
          }
        },
        "52af6ece88b246dcaa0b0aaeda6cd58d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ed7df865ef45a2806bb99928a22ed5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c877386ef654584a6756b0f28035e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d32640162774e959d1522e50334f7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78b7b608070b4b05be1b48b2e773587f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6368df6c6a5c45f48f1310ab5740df45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946532b3bb204b6a85ea7b31c43a150d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09dc82f9536a4828a400c133c24f77ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb3b2b92a9474ba78cc4383a11b2b4c5",
              "IPY_MODEL_5031a61f8d674bd19ba80b07c86bcd53",
              "IPY_MODEL_95f73134eefa47e89bf31062efc4670c"
            ],
            "layout": "IPY_MODEL_58464dfedbd9444186643b29095e683d"
          }
        },
        "cb3b2b92a9474ba78cc4383a11b2b4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8927d36418f74325bbe65119a3ffbb0b",
            "placeholder": "​",
            "style": "IPY_MODEL_089347c771b34665bcdb32e6758c465d",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5031a61f8d674bd19ba80b07c86bcd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ba8ec54343e4feab58fdbf5296e3f63",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ba28963e2d145c8b678d5da77df0726",
            "value": 414
          }
        },
        "95f73134eefa47e89bf31062efc4670c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7844a64be95f4b58b9e5393d4e3b6dfe",
            "placeholder": "​",
            "style": "IPY_MODEL_8dd61266d0e442eebd4517669840ae77",
            "value": " 414/414 [00:00&lt;00:00, 50.8kB/s]"
          }
        },
        "58464dfedbd9444186643b29095e683d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8927d36418f74325bbe65119a3ffbb0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "089347c771b34665bcdb32e6758c465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ba8ec54343e4feab58fdbf5296e3f63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba28963e2d145c8b678d5da77df0726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7844a64be95f4b58b9e5393d4e3b6dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd61266d0e442eebd4517669840ae77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b43067aa7a648e38fc6aaca3c44839f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4afd3a09468443a3a1b36f2edb0a58a9",
              "IPY_MODEL_d4caf020189b47d49e25c998faea33df",
              "IPY_MODEL_e8ff3934f0c94c52abb17bd4d2c3da37"
            ],
            "layout": "IPY_MODEL_b9101474adec4a2aad94a5bdfc967c8a"
          }
        },
        "4afd3a09468443a3a1b36f2edb0a58a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e1d592842724d5daa1d8dc4b8281e0a",
            "placeholder": "​",
            "style": "IPY_MODEL_440b1f8a443144ee8e56d2553775b97a",
            "value": "Map: 100%"
          }
        },
        "d4caf020189b47d49e25c998faea33df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93fc697c0a04ab6aef5aefcdbea804a",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86f0130b65614220b9d90a28df715963",
            "value": 500
          }
        },
        "e8ff3934f0c94c52abb17bd4d2c3da37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa836abb18942e2b3d50c10f71a3e2e",
            "placeholder": "​",
            "style": "IPY_MODEL_ac5341e163cb40548b9e26da7c888483",
            "value": " 500/500 [00:00&lt;00:00, 3752.60 examples/s]"
          }
        },
        "b9101474adec4a2aad94a5bdfc967c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e1d592842724d5daa1d8dc4b8281e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "440b1f8a443144ee8e56d2553775b97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e93fc697c0a04ab6aef5aefcdbea804a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f0130b65614220b9d90a28df715963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3aa836abb18942e2b3d50c10f71a3e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5341e163cb40548b9e26da7c888483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6ebe78c5c8a4b78829332707706357a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_676019d50cd44071a1006d3198e513ea",
              "IPY_MODEL_ed9d9ba6f8294d7b88ebf2100268026b",
              "IPY_MODEL_102f59a8cf494434a5848c764fc1c27d"
            ],
            "layout": "IPY_MODEL_0cbe783d57184c25a3189bc5208ab059"
          }
        },
        "676019d50cd44071a1006d3198e513ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c7facb195d46598bd882c84cc6018e",
            "placeholder": "​",
            "style": "IPY_MODEL_52c775a3c8c345d6b62b8557c1f33023",
            "value": "Map: 100%"
          }
        },
        "ed9d9ba6f8294d7b88ebf2100268026b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ad13c2ec85e46808745ebf23439663c",
            "max": 358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1517e4e366b14306b3f39841df80dbab",
            "value": 358
          }
        },
        "102f59a8cf494434a5848c764fc1c27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f44aedc47dd044898afe33742b66a31c",
            "placeholder": "​",
            "style": "IPY_MODEL_245e81eb63c743b3a51253ae64055596",
            "value": " 358/358 [00:00&lt;00:00, 4787.12 examples/s]"
          }
        },
        "0cbe783d57184c25a3189bc5208ab059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c7facb195d46598bd882c84cc6018e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c775a3c8c345d6b62b8557c1f33023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad13c2ec85e46808745ebf23439663c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1517e4e366b14306b3f39841df80dbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f44aedc47dd044898afe33742b66a31c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "245e81eb63c743b3a51253ae64055596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b717c096d2764fe7bd9fcb3dfdde925b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68eb60a75b324ef988745be44e9e64e1",
              "IPY_MODEL_e0e5873a8f0940f7a82c74bf0e7deeb4",
              "IPY_MODEL_b7543757a74c418c9474a325d4b9dac8"
            ],
            "layout": "IPY_MODEL_ba0293db13f54ad08ba6c2b06b361c5a"
          }
        },
        "68eb60a75b324ef988745be44e9e64e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b56c5452c8c4026aa3163ba8546c1f1",
            "placeholder": "​",
            "style": "IPY_MODEL_5326f92decb24ce2b4832c240b7f7f18",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e0e5873a8f0940f7a82c74bf0e7deeb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ce81a4d00f9413389dcbc04e2693aa5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b11e1c58790642169cc46d126ce45e33",
            "value": 2
          }
        },
        "b7543757a74c418c9474a325d4b9dac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a2b2edb75c84c6aac1cb546c1aeb42e",
            "placeholder": "​",
            "style": "IPY_MODEL_5f43a36a64704156afda4a1c52b0fae1",
            "value": " 2/2 [00:03&lt;00:00,  1.81s/it]"
          }
        },
        "ba0293db13f54ad08ba6c2b06b361c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b56c5452c8c4026aa3163ba8546c1f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5326f92decb24ce2b4832c240b7f7f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ce81a4d00f9413389dcbc04e2693aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b11e1c58790642169cc46d126ce45e33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a2b2edb75c84c6aac1cb546c1aeb42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f43a36a64704156afda4a1c52b0fae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b60d3563f3484feea283704067a7f20c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3cf8c5e115c462fa54f86fae7fb78eb",
              "IPY_MODEL_8285f0f3cf4e48e1a1892b9ba4d878ea",
              "IPY_MODEL_1384bdd53d4141d2a9e3a92da325e2c8"
            ],
            "layout": "IPY_MODEL_3271ebfdb41d455a8f6cc8cebee6650d"
          }
        },
        "b3cf8c5e115c462fa54f86fae7fb78eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc0d127e4c7445c9eaa8091d51aaab8",
            "placeholder": "​",
            "style": "IPY_MODEL_963d3ad6af01422cbfd393bcdf5d62d1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8285f0f3cf4e48e1a1892b9ba4d878ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5ffa43ba15242b689cb8a8e12c84137",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7f97f958f2d4054b2bc9721381f02d0",
            "value": 2
          }
        },
        "1384bdd53d4141d2a9e3a92da325e2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eed5404f9a14483fa26117e75dfbfd73",
            "placeholder": "​",
            "style": "IPY_MODEL_68833a5e06f44a8f850f2bf8d4ff76bc",
            "value": " 2/2 [00:04&lt;00:00,  1.84s/it]"
          }
        },
        "3271ebfdb41d455a8f6cc8cebee6650d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc0d127e4c7445c9eaa8091d51aaab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963d3ad6af01422cbfd393bcdf5d62d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5ffa43ba15242b689cb8a8e12c84137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f97f958f2d4054b2bc9721381f02d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eed5404f9a14483fa26117e75dfbfd73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68833a5e06f44a8f850f2bf8d4ff76bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92db660e55c94e2b842d89d0eb59cf7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80d53f8137434acba6f61bf6ed400d6a",
              "IPY_MODEL_ff86fcdc8f5f48be8a97c2cbf7e36869",
              "IPY_MODEL_cd03e14434f04d3bbad60f278058ad05"
            ],
            "layout": "IPY_MODEL_4917e72f0c9d4dffa13fbf38fe80183d"
          }
        },
        "80d53f8137434acba6f61bf6ed400d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372b2cc079494a5893f4c4045c507705",
            "placeholder": "​",
            "style": "IPY_MODEL_5b2856b8a7d94e4da7c253b32a6aa731",
            "value": "Map: 100%"
          }
        },
        "ff86fcdc8f5f48be8a97c2cbf7e36869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a714bcffb525422ab6c35a45121f653c",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6944e19f24fa4443a6b3551f9f65203f",
            "value": 500
          }
        },
        "cd03e14434f04d3bbad60f278058ad05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_792387d9fb3d4279b19991f9f7ded3b2",
            "placeholder": "​",
            "style": "IPY_MODEL_47654b19fa684161a57f65d274cd041a",
            "value": " 500/500 [00:00&lt;00:00, 3468.87 examples/s]"
          }
        },
        "4917e72f0c9d4dffa13fbf38fe80183d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372b2cc079494a5893f4c4045c507705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2856b8a7d94e4da7c253b32a6aa731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a714bcffb525422ab6c35a45121f653c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6944e19f24fa4443a6b3551f9f65203f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "792387d9fb3d4279b19991f9f7ded3b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47654b19fa684161a57f65d274cd041a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc1f6bbc05164ac4865d3a3453b50792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c358ce9bfd3540ed88d92760cc465aa5",
              "IPY_MODEL_4ea87abcf3a54cf785240fc408e559d6",
              "IPY_MODEL_8482ce3578a343878ca4f356a92ce055"
            ],
            "layout": "IPY_MODEL_8551dc2e992e417fbb6446c051350db8"
          }
        },
        "c358ce9bfd3540ed88d92760cc465aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0f5c88d619742ba89868aabd0f5216c",
            "placeholder": "​",
            "style": "IPY_MODEL_cdcd6061cd1d49288fc65010a1e4ba77",
            "value": "Map: 100%"
          }
        },
        "4ea87abcf3a54cf785240fc408e559d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d137e90386427c97dad82d91c4a84e",
            "max": 358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a3f2b09625b45a4babed4f8200082ab",
            "value": 358
          }
        },
        "8482ce3578a343878ca4f356a92ce055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41325219baa5470dab2210932e67802f",
            "placeholder": "​",
            "style": "IPY_MODEL_b61565af409b400baed61e7a127fab8a",
            "value": " 358/358 [00:00&lt;00:00, 3408.80 examples/s]"
          }
        },
        "8551dc2e992e417fbb6446c051350db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f5c88d619742ba89868aabd0f5216c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdcd6061cd1d49288fc65010a1e4ba77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6d137e90386427c97dad82d91c4a84e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a3f2b09625b45a4babed4f8200082ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41325219baa5470dab2210932e67802f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b61565af409b400baed61e7a127fab8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE1fbw8paW6l",
        "outputId": "ac5355cd-6dc6-4d2f-837f-c192e370721b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset.csv'"
      ],
      "metadata": {
        "id": "1KWQpEFsayvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Replace with the actual path to your friend's CSV file ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset.csv'\n",
        "\n",
        "try:\n",
        "    # Load the CSV file into a pandas DataFrame named 'ground_truth_df'\n",
        "    ground_truth_df = pd.read_csv(csv_path)\n",
        "    print(\"CSV file loaded successfully.\")\n",
        "    print(ground_truth_df.head()) # Display the first few rows\n",
        "    print(ground_truth_df.columns) # Display the column names\n",
        "\n",
        "    # --- Create the 'Wildfire' label based on the 'state' column ---\n",
        "    ground_truth_df['Wildfire'] = 'No'\n",
        "    ground_truth_df.loc[ground_truth_df['state'] == 'California', 'Wildfire'] = 'Yes'\n",
        "\n",
        "    print(\"\\n'Wildfire' label created.\")\n",
        "    print(ground_truth_df[['state', 'Wildfire']].head()) # Display first few rows with 'Wildfire' label\n",
        "    print(\"\\nDistribution of 'Wildfire' label:\")\n",
        "    print(ground_truth_df['Wildfire'].value_counts()) # Show counts of 'Yes' and 'No'\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5LFjDkcayy8",
        "outputId": "b889e763-ef8a-483d-c312-b437d73d4ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file loaded successfully.\n",
            "       tweet_id                  image_id  \\\n",
            "0  9.177910e+17  917791044158185473_0.jpg   \n",
            "1  9.177911e+17  917791130590183424_0.jpg   \n",
            "2  9.177913e+17  917791291823591425_0.jpg   \n",
            "3  9.177913e+17  917791291823591425_1.jpg   \n",
            "4  9.177921e+17  917792092100988929_0.jpg   \n",
            "\n",
            "                                      raw_tweet_text  \\\n",
            "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
            "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
            "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "3  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "4  RT @TIME: California's raging wildfires as you...   \n",
            "\n",
            "                                          tweet_text tweet_hashtags  \\\n",
            "0  wildfires raging through northern california a...            NaN   \n",
            "1         photos deadly wildfires rage in california            NaN   \n",
            "2  pls share were capturing wildfire response rec...            NaN   \n",
            "3  pls share were capturing wildfire response rec...            NaN   \n",
            "4  californias raging wildfires as youve never se...            NaN   \n",
            "\n",
            "                                       image_caption  distress take_action  \\\n",
            "0  a fire is seen burning through the trees in th...         0         NaN   \n",
            "1  two people are standing in a burned area with ...         0         NaN   \n",
            "2         a fire burns in a burned area near a house         0         NaN   \n",
            "3  several people standing in front of a screen w...         0         NaN   \n",
            "4  a night sky with a mountain and a milky in the...         0         NaN   \n",
            "\n",
            "        state sub_location  \n",
            "0  California     northern  \n",
            "1  California          NaN  \n",
            "2         NaN          NaN  \n",
            "3         NaN          NaN  \n",
            "4  California          NaN  \n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location'],\n",
            "      dtype='object')\n",
            "\n",
            "'Wildfire' label created.\n",
            "        state Wildfire\n",
            "0  California      Yes\n",
            "1  California      Yes\n",
            "2         NaN       No\n",
            "3         NaN       No\n",
            "4  California      Yes\n",
            "\n",
            "Distribution of 'Wildfire' label:\n",
            "Wildfire\n",
            "No     16730\n",
            "Yes     1352\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a new CSV file\n",
        "output_csv_path = '/content/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"\\nDataFrame with 'Wildfire' label saved to: {output_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLcLSzZvay2L",
        "outputId": "8617d066-b136-43e0-9433-64e4d3ff5ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame with 'Wildfire' label saved to: /content/ground_truth_dataset_with_wildfire.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with the state as the primary location. Since your dataset is focused on California wildfires, we know the state is relevant.\n",
        "\n",
        "If sub_location is available (not NaN), append it to the state with a separator (e.g., \", \"). This will provide more specific location details when they exist.\n",
        "\n",
        "If sub_location is missing (NaN), just use \"California\" as the location."
      ],
      "metadata": {
        "id": "lJKvDtmvbOVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new 'Location' column\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',  # Handle cases where state might be missing (though unlikely here)\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n'Location' column created.\")\n",
        "print(ground_truth_df[['state', 'sub_location', 'Location']].head(10)) # Display first 10 rows\n",
        "print(\"\\nValue counts for 'Location':\")\n",
        "print(ground_truth_df['Location'].value_counts().head(20)) # Show top 20 locations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrrHOAtybK2F",
        "outputId": "f05f7c7f-fb50-4f9f-8c2e-cfe6f67e5dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Location' column created.\n",
            "        state sub_location               Location\n",
            "0  California     northern   California, northern\n",
            "1  California          NaN             California\n",
            "2         NaN          NaN  No location mentioned\n",
            "3         NaN          NaN  No location mentioned\n",
            "4  California          NaN             California\n",
            "5  California          NaN             California\n",
            "6  California    wildfires  California, wildfires\n",
            "7  California          NaN             California\n",
            "8  California          NaN             California\n",
            "9  California          NaN             California\n",
            "\n",
            "Value counts for 'Location':\n",
            "Location\n",
            "No location mentioned                            16730\n",
            "California                                         966\n",
            "California, northern                                88\n",
            "California, southern                                15\n",
            "California, santa rosa                               9\n",
            "California, napa                                     8\n",
            "California, wildfires                                8\n",
            "California, sonoma northern                          5\n",
            "California, reuters                                  5\n",
            "California, mexico                                   5\n",
            "California, oak grove                                4\n",
            "California, sonoma                                   4\n",
            "California, disneyland                               4\n",
            "California, northern spain portugal                  4\n",
            "California, santa cruz cowlitz clark counties        4\n",
            "California, obamanation s forests                    4\n",
            "California, oak grove oakgrove                       4\n",
            "California, the redwood forest                       3\n",
            "California, puerto rico                              3\n",
            "California, yosemite                                 3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "    # Add more keywords and responders as you analyze your 'take_action' data\n",
        "}"
      ],
      "metadata": {
        "id": "JysLjjLEbK56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()  # Convert to string and lowercase for matching\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "\n",
        "print(\"\\n'Responders (Suggested)' column created.\")\n",
        "print(ground_truth_df[['distress', 'take_action', 'Responders (Suggested)']].head(20))\n",
        "print(\"\\nValue counts for 'Responders (Suggested)':\")\n",
        "print(ground_truth_df['Responders (Suggested)'].value_counts().head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0q64_iwbK-0",
        "outputId": "b6a27d14-36f5-4cba-f56a-c71a5c60cea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Responders (Suggested)' column created.\n",
            "    distress                          take_action  \\\n",
            "0          0                                  NaN   \n",
            "1          0                                  NaN   \n",
            "2          0                                  NaN   \n",
            "3          0                                  NaN   \n",
            "4          0                                  NaN   \n",
            "5          0                                  NaN   \n",
            "6          1  send evacuation and shelter support   \n",
            "7          0                                  NaN   \n",
            "8          0                                  NaN   \n",
            "9          0                                  NaN   \n",
            "10         0                                  NaN   \n",
            "11         0                                  NaN   \n",
            "12         0                                  NaN   \n",
            "13         0                                  NaN   \n",
            "14         1          start missing person search   \n",
            "15         0                                  NaN   \n",
            "16         1          start missing person search   \n",
            "17         0                                  NaN   \n",
            "18         0                                  NaN   \n",
            "19         0                                  NaN   \n",
            "\n",
            "                      Responders (Suggested)  \n",
            "0                             Not applicable  \n",
            "1                             Not applicable  \n",
            "2                             Not applicable  \n",
            "3                             Not applicable  \n",
            "4                             Not applicable  \n",
            "5                             Not applicable  \n",
            "6            Red Cross, Emergency Management  \n",
            "7                             Not applicable  \n",
            "8                             Not applicable  \n",
            "9                             Not applicable  \n",
            "10                            Not applicable  \n",
            "11                            Not applicable  \n",
            "12                            Not applicable  \n",
            "13                            Not applicable  \n",
            "14  Search and Rescue Teams, Law Enforcement  \n",
            "15                            Not applicable  \n",
            "16  Search and Rescue Teams, Law Enforcement  \n",
            "17                            Not applicable  \n",
            "18                            Not applicable  \n",
            "19                            Not applicable  \n",
            "\n",
            "Value counts for 'Responders (Suggested)':\n",
            "Responders (Suggested)\n",
            "Not applicable                              16100\n",
            "Local Authorities, Emergency Services        1548\n",
            "Search and Rescue Teams, Law Enforcement      149\n",
            "Red Cross, Emergency Management               128\n",
            "Search and Rescue Teams, Fire Department       56\n",
            "Responders unclear                             53\n",
            "Emergency Medical Services                     31\n",
            "General Emergency Services                     17\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inspection_sample = ground_truth_df.sample(frac=0.15, random_state=42) # Adjust fraction as needed\n",
        "print(f\"Generated a sample of {len(inspection_sample)} rows for manual inspection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-vDetPbLDe",
        "outputId": "1f53b498-2376-4058-d06f-f93009c4ca7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated a sample of 2712 rows for manual inspection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inspection_subset = inspection_sample[['raw_tweet_text', 'state', 'sub_location', 'Wildfire', 'distress', 'Location', 'take_action', 'Responders (Suggested)']]\n",
        "print(inspection_subset.head(20)) # Display the first 20 rows of the sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gb0e7XhbLIi",
        "outputId": "c7a2ebbc-f142-4d0e-95ac-9c3a09975d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          raw_tweet_text       state  \\\n",
            "11482  . #Maria is now a weak and ragged looking Cat-...         NaN   \n",
            "13112  Puerto Rico governor: I answered Trump... http...         NaN   \n",
            "2501   RT @MPrendergastTX: Buffalo Bayou in Houston. ...         NaN   \n",
            "322    Company Helps Coordinate Air Attack On Califor...  California   \n",
            "8422   4th hr's back &amp; louder than Irma #whatifIr...         NaN   \n",
            "15453  Turkish Red Crescent cooperates with Iraqi Red...         NaN   \n",
            "11923  #PuertoRico has suffered immense devastation f...         NaN   \n",
            "7695   BuzzFeed : This Florida county used an interpr...     Florida   \n",
            "15472  President Dr. Kerem Kinik in Darbendixan distr...         NaN   \n",
            "7945   .@SecretarySonny Perdue, @marcorubio and @TomR...     Florida   \n",
            "290    Fire chief: We got outrun by the fires https:/...         NaN   \n",
            "12303  Hurricane Maria Not Getting Same Amount of Cov...         NaN   \n",
            "11326  This morning's update on #hurricanemaria - rem...         NaN   \n",
            "15929  Rescue efforts underway after second deadly Me...         NaN   \n",
            "9736   Adecco is in our office NOW! Stop by and find ...         NaN   \n",
            "1162   Baptism by fire for California's potÂ farmers ...  California   \n",
            "4706   Salvation Army &amp; Ingram Micro employees un...         NaN   \n",
            "4304   Out-of-towners helping with Harvey relief prob...       Texas   \n",
            "2003   RT @NavyTimes: Drones, disaster teams helping ...         NaN   \n",
            "17582  Retweeted Kate Mora (@jedikat71): Can't have e...         NaN   \n",
            "\n",
            "              sub_location Wildfire  distress               Location  \\\n",
            "11482                  NaN       No         0  No location mentioned   \n",
            "13112          puerto rico       No         0  No location mentioned   \n",
            "2501                   NaN       No         0  No location mentioned   \n",
            "322                    NaN      Yes         1             California   \n",
            "8422                   NaN       No         0  No location mentioned   \n",
            "15453                 iraq       No         0  No location mentioned   \n",
            "11923           puertorico       No         1  No location mentioned   \n",
            "7695   this florida county       No         0  No location mentioned   \n",
            "15472                  NaN       No         0  No location mentioned   \n",
            "7945                   NaN       No         0  No location mentioned   \n",
            "290                 usnews       No         0  No location mentioned   \n",
            "12303                  NaN       No         0  No location mentioned   \n",
            "11326       hurricanemaria       No         0  No location mentioned   \n",
            "15929               mexico       No         0  No location mentioned   \n",
            "9736                   NaN       No         1  No location mentioned   \n",
            "1162                   NaN      Yes         0             California   \n",
            "4706                   NaN       No         0  No location mentioned   \n",
            "4304                 texas       No         1  No location mentioned   \n",
            "2003                   NaN       No         1  No location mentioned   \n",
            "17582                  NaN       No         0  No location mentioned   \n",
            "\n",
            "             take_action                 Responders (Suggested)  \n",
            "11482                NaN                         Not applicable  \n",
            "13112                NaN                         Not applicable  \n",
            "2501                 NaN                         Not applicable  \n",
            "322    monitor situation  Local Authorities, Emergency Services  \n",
            "8422                 NaN                         Not applicable  \n",
            "15453                NaN                         Not applicable  \n",
            "11923  monitor situation  Local Authorities, Emergency Services  \n",
            "7695                 NaN                         Not applicable  \n",
            "15472                NaN                         Not applicable  \n",
            "7945                 NaN                         Not applicable  \n",
            "290                  NaN                         Not applicable  \n",
            "12303                NaN                         Not applicable  \n",
            "11326                NaN                         Not applicable  \n",
            "15929                NaN                         Not applicable  \n",
            "9736   monitor situation  Local Authorities, Emergency Services  \n",
            "1162                 NaN                         Not applicable  \n",
            "4706                 NaN                         Not applicable  \n",
            "4304   monitor situation  Local Authorities, Emergency Services  \n",
            "2003   monitor situation  Local Authorities, Emergency Services  \n",
            "17582                NaN                         Not applicable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset split"
      ],
      "metadata": {
        "id": "WijkbTu7bgBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'  # Or the path where you saved it\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "print(\"Ground truth dataset loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoGoVNwbbLM3",
        "outputId": "bcd123bf-46b0-4f8b-9b39-2eb523c07296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth dataset loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load the CSV file from the root directory ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "\n",
        "try:\n",
        "    ground_truth_df = pd.read_csv(csv_path)\n",
        "    print(\"Ground truth dataset loaded.\")\n",
        "\n",
        "    # --- Determine the counts of each combination of 'Wildfire' and 'distress' ---\n",
        "    label_counts = ground_truth_df.groupby(['Wildfire', 'distress']).size().reset_index(name='counts')\n",
        "    print(\"Counts of each label combination:\")\n",
        "    print(label_counts)\n",
        "\n",
        "    # --- Aim for roughly equal samples (up to 25) from each combination for the 100-sample split ---\n",
        "    sample_size_per_group = 25\n",
        "    sample_split = pd.DataFrame()\n",
        "    sampled_indices = []\n",
        "\n",
        "    for index, row in label_counts.iterrows():\n",
        "        wildfire_label = row['Wildfire']\n",
        "        distress_label = row['distress']\n",
        "        count = row['counts']\n",
        "\n",
        "        n_samples = min(count, sample_size_per_group)  # Take up to 25, or fewer if the group is smaller\n",
        "        group_sample = ground_truth_df[\n",
        "            (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "        ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "        sample_split = pd.concat([sample_split, group_sample])\n",
        "        sampled_indices.extend(group_sample.index)\n",
        "\n",
        "    print(f\"\\nSize of the Sample Split: {len(sample_split)}\")\n",
        "    print(\"\\nDistribution of labels in the Sample Split:\")\n",
        "    print(sample_split.groupby(['Wildfire', 'distress']).size())\n",
        "\n",
        "    # --- Create the remaining DataFrame by removing the sampled rows ---\n",
        "    remaining_df = ground_truth_df.drop(sampled_indices)\n",
        "    print(f\"\\nSize of the Remaining DataFrame: {len(remaining_df)}\")\n",
        "\n",
        "    # Now 'sample_split' contains your 100-sample (or close to it, balanced) held-out set\n",
        "    # and 'remaining_df' contains the data for the 80/10/10 split.\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at /content/ground-truth_dataset_with_wildfire.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrK4WaP2bLQh",
        "outputId": "f1dcdedf-5c27-4806-c3e5-061865781afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth dataset loaded.\n",
            "Counts of each label combination:\n",
            "  Wildfire  distress  counts\n",
            "0       No         0   14896\n",
            "1       No         1    1834\n",
            "2      Yes         0    1204\n",
            "3      Yes         1     148\n",
            "\n",
            "Size of the Sample Split: 100\n",
            "\n",
            "Distribution of labels in the Sample Split:\n",
            "Wildfire  distress\n",
            "No        0           25\n",
            "          1           25\n",
            "Yes       0           25\n",
            "          1           25\n",
            "dtype: int64\n",
            "\n",
            "Size of the Remaining DataFrame: 17982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Split remaining_df into training (80%) and a temporary set (20%) ---\n",
        "train_df, temp_df = train_test_split(\n",
        "    remaining_df,\n",
        "    test_size=0.2,\n",
        "    stratify=remaining_df[['Wildfire', 'distress']],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- Split the temporary set (20%) into validation (10%) and testing (10%) ---\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,  # 50% of the temp_df is 10% of the original\n",
        "    stratify=temp_df[['Wildfire', 'distress']],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Size of Training Set: {len(train_df)}\")\n",
        "print(f\"Size of Validation Set: {len(val_df)}\")\n",
        "print(f\"Size of Testing Set: {len(test_df)}\")\n",
        "\n",
        "print(\"\\nDistribution of labels in Training Set:\")\n",
        "print(train_df.groupby(['Wildfire', 'distress']).size() / len(train_df))\n",
        "\n",
        "print(\"\\nDistribution of labels in Validation Set:\")\n",
        "print(val_df.groupby(['Wildfire', 'distress']).size() / len(val_df))\n",
        "\n",
        "print(\"\\nDistribution of labels in Testing Set:\")\n",
        "print(test_df.groupby(['Wildfire', 'distress']).size() / len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Z4_WBYbLTq",
        "outputId": "aa7a170f-cc95-49d9-fa89-19256d5befb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Training Set: 14385\n",
            "Size of Validation Set: 1798\n",
            "Size of Testing Set: 1799\n",
            "\n",
            "Distribution of labels in Training Set:\n",
            "Wildfire  distress\n",
            "No        0           0.826973\n",
            "          1           0.100591\n",
            "Yes       0           0.065554\n",
            "          1           0.006882\n",
            "dtype: float64\n",
            "\n",
            "Distribution of labels in Validation Set:\n",
            "Wildfire  distress\n",
            "No        0           0.827030\n",
            "          1           0.100667\n",
            "Yes       0           0.065628\n",
            "          1           0.006674\n",
            "dtype: float64\n",
            "\n",
            "Distribution of labels in Testing Set:\n",
            "Wildfire  distress\n",
            "No        0           0.827126\n",
            "          1           0.100611\n",
            "Yes       0           0.065592\n",
            "          1           0.006670\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "t5-small"
      ],
      "metadata": {
        "id": "BTYLI731bzDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# --- Load base T5 small model and tokenizer ---\n",
        "model_name_base = \"t5-small\"\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(model_name_base)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name_base).to(device)\n",
        "\n",
        "print(f\"Base model and tokenizer for {model_name_base} loaded on {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OctLOPWGbxhT",
        "outputId": "ec4618f6-8e26-4d4e-f8b2-3afa0e12c687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model and tokenizer for t5-small loaded on cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare prompts for the sample split ---\n",
        "prompts = []\n",
        "sample_tweets = sample_split['tweet_text'].tolist()\n",
        "\n",
        "for tweet in sample_tweets:\n",
        "    prompts.append(f\"Is this tweet about a California wildfire? Tweet: {tweet}\")\n",
        "    prompts.append(f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What location is mentioned in this tweet? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What action and responders are needed based on this tweet? Tweet: {tweet}\")\n",
        "\n",
        "# --- Tokenize the prompts ---\n",
        "inputs = tokenizer_base.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model_base.device)\n",
        "\n",
        "# --- Generate predictions ---\n",
        "with torch.no_grad():\n",
        "    outputs = model_base.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# --- Decode the predictions ---\n",
        "predictions = tokenizer_base.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# --- Display the prompts and predictions for the first few examples ---\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    tweet_index = i // 4\n",
        "    question_index = i % 4\n",
        "    question = [\"Wildfire?\", \"Distress?\", \"Location?\", \"Action/Responders?\"][question_index]\n",
        "    print(f\"Tweet: {sample_tweets[tweet_index][:50]}...\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71k4pxgIb3Sg",
        "outputId": "462f0d1e-56d8-4e7c-df8e-0651e86d5eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Wildfire?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Distress?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery? Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Location?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Action/Responders?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: hurricane maria moves north lee still far from lan...\n",
            "Question: Wildfire?\n",
            "Prediction: Tweet: hurricane maria moves north lee still far from land.\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare prompts and generate predictions for a small sample of tweets ---\n",
        "num_tweets_to_examine = 3  # You can change this number\n",
        "sample_tweets = sample_split['tweet_text'].tolist()[:num_tweets_to_examine] # Take the first N tweets\n",
        "\n",
        "all_predictions = []\n",
        "all_prompts = []\n",
        "original_tweets = []\n",
        "\n",
        "for tweet in sample_tweets:\n",
        "    original_tweets.append(tweet)\n",
        "    prompts = [\n",
        "        f\"Is this tweet about a California wildfire? Tweet: {tweet}\",\n",
        "        f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\",\n",
        "        f\"What location is mentioned in this tweet? Tweet: {tweet}\",\n",
        "        f\"What action and responders are needed based on this tweet? Tweet: {tweet}\"\n",
        "    ]\n",
        "    all_prompts.extend(prompts)\n",
        "\n",
        "    inputs = tokenizer_base.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model_base.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_base.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "    predictions = tokenizer_base.batch_decode(outputs, skip_special_tokens=True)\n",
        "    all_predictions.extend(predictions)\n",
        "\n",
        "# --- Display the prompts and predictions ---\n",
        "for i in range(len(original_tweets)):\n",
        "    tweet = original_tweets[i]\n",
        "    print(f\"Tweet: {tweet[:50]}...\")\n",
        "    for j in range(4):\n",
        "        question = [\"Wildfire?\", \"Distress?\", \"Location?\", \"Action/Responders?\"][j]\n",
        "        prediction = all_predictions[i * 4 + j]\n",
        "        print(f\"  Question: {question}\")\n",
        "        print(f\"  Prediction: {prediction}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz6ZgUM8b3Wj",
        "outputId": "4b71e0b5-c5e6-49f9-86fe-1b0193783f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "  Question: Wildfire?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "  Question: Distress?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery? Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "  Question: Location?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "  Question: Action/Responders?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: hurricane maria moves north lee still far from lan...\n",
            "  Question: Wildfire?\n",
            "  Prediction: Tweet: hurricane maria moves north lee still far from land.\n",
            "  Question: Distress?\n",
            "  Prediction: Tweet:\n",
            "  Question: Location?\n",
            "  Prediction: Hurricane maria moves north lee still far from land\n",
            "  Question: Action/Responders?\n",
            "  Prediction: Tweet: hurricane maria moves north lee still far from land.\n",
            "------------------------------\n",
            "Tweet: this hurricane is targeting all the rich folks wit...\n",
            "  Question: Wildfire?\n",
            "  Prediction: Tweet: this hurricane is targeting all the rich folks witchcraft things #irmahurricane #irmahurricane #irmahurricane #irmahurricane\n",
            "  Question: Distress?\n",
            "  Prediction: Tweet: This hurricane is targeting all the rich folks witchcraft things #irmahurricane #irma\n",
            "  Question: Location?\n",
            "  Prediction: Tweet: this hurricane is targeting all the rich folks witchcraft things #irmahurricane #irmahurricane #irmahurricane #irmahurricane\n",
            "  Question: Action/Responders?\n",
            "  Prediction: ? Tweet: this hurricane is targeting all the rich folks witchcraft things #irmahurricane #irmahurricane #irmahurricane #irmahurric\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new 'Location' column\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',  # Handle cases where state might be missing (though unlikely here)\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n'Location' column created.\")\n",
        "print(ground_truth_df[['state', 'sub_location', 'Location']].head(10))\n",
        "print(\"\\nValue counts for 'Location':\")\n",
        "print(ground_truth_df['Location'].value_counts().head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJfJtXMZb3bL",
        "outputId": "0758dc74-8399-411b-afb8-6fd12a555fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Location' column created.\n",
            "        state sub_location               Location\n",
            "0  California     northern   California, northern\n",
            "1  California          NaN             California\n",
            "2         NaN          NaN  No location mentioned\n",
            "3         NaN          NaN  No location mentioned\n",
            "4  California          NaN             California\n",
            "5  California          NaN             California\n",
            "6  California    wildfires  California, wildfires\n",
            "7  California          NaN             California\n",
            "8  California          NaN             California\n",
            "9  California          NaN             California\n",
            "\n",
            "Value counts for 'Location':\n",
            "Location\n",
            "No location mentioned                            16730\n",
            "California                                         966\n",
            "California, northern                                88\n",
            "California, southern                                15\n",
            "California, santa rosa                               9\n",
            "California, napa                                     8\n",
            "California, wildfires                                8\n",
            "California, sonoma northern                          5\n",
            "California, reuters                                  5\n",
            "California, mexico                                   5\n",
            "California, oak grove                                4\n",
            "California, sonoma                                   4\n",
            "California, disneyland                               4\n",
            "California, northern spain portugal                  4\n",
            "California, santa cruz cowlitz clark counties        4\n",
            "California, obamanation s forests                    4\n",
            "California, oak grove oakgrove                       4\n",
            "California, the redwood forest                       3\n",
            "California, puerto rico                              3\n",
            "California, yosemite                                 3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in ground_truth_df after creating 'Location':\")\n",
        "print(ground_truth_df.columns)\n",
        "print(\"\\nColumns in sample_split:\")\n",
        "print(sample_split.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUeYg__kbsUp",
        "outputId": "4ee2a404-ca70-4d5a-a70a-45865d51cc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in ground_truth_df after creating 'Location':\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire', 'Location'],\n",
            "      dtype='object')\n",
            "\n",
            "Columns in sample_split:\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Aim for roughly equal samples (up to 25) from each combination for the 100-sample split ---\n",
        "sample_size_per_group = 25\n",
        "sample_split = pd.DataFrame()\n",
        "sampled_indices = []\n",
        "\n",
        "for index, row in label_counts.iterrows():\n",
        "    wildfire_label = row['Wildfire']\n",
        "    distress_label = row['distress']\n",
        "    count = row['counts']\n",
        "\n",
        "    n_samples = min(count, sample_size_per_group)  # Take up to 25, or fewer if the group is smaller\n",
        "    group_sample = ground_truth_df[\n",
        "        (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "    ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "    sample_split = pd.concat([sample_split, group_sample])\n",
        "    sampled_indices.extend(group_sample.index)\n",
        "\n",
        "print(f\"\\nSize of the Sample Split: {len(sample_split)}\")\n",
        "print(\"\\nDistribution of labels in the Sample Split:\")\n",
        "print(sample_split.groupby(['Wildfire', 'distress']).size())\n",
        "\n",
        "# --- Create the remaining DataFrame by removing the sampled rows ---\n",
        "remaining_df = ground_truth_df.drop(sampled_indices)\n",
        "print(f\"\\nSize of the Remaining DataFrame: {len(remaining_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhQYEuYkbsXb",
        "outputId": "d6329548-b889-4975-ec30-aa0fa4718b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Size of the Sample Split: 100\n",
            "\n",
            "Distribution of labels in the Sample Split:\n",
            "Wildfire  distress\n",
            "No        0           25\n",
            "          1           25\n",
            "Yes       0           25\n",
            "          1           25\n",
            "dtype: int64\n",
            "\n",
            "Size of the Remaining DataFrame: 17982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "    # Add more keywords and responders as you analyze your 'take_action' data\n",
        "}\n",
        "\n",
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()  # Convert to string and lowercase for matching\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "\n",
        "print(\"\\n'Responders (Suggested)' column created.\")\n",
        "print(ground_truth_df[['distress', 'take_action', 'Responders (Suggested)']].head(20))\n",
        "print(\"\\nValue counts for 'Responders (Suggested)':\")\n",
        "print(ground_truth_df['Responders (Suggested)'].value_counts().head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDUwSCYRbsbO",
        "outputId": "5e5c1439-e7e0-45d8-ff36-f4770739cf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Responders (Suggested)' column created.\n",
            "    distress                          take_action  \\\n",
            "0          0                                  NaN   \n",
            "1          0                                  NaN   \n",
            "2          0                                  NaN   \n",
            "3          0                                  NaN   \n",
            "4          0                                  NaN   \n",
            "5          0                                  NaN   \n",
            "6          1  send evacuation and shelter support   \n",
            "7          0                                  NaN   \n",
            "8          0                                  NaN   \n",
            "9          0                                  NaN   \n",
            "10         0                                  NaN   \n",
            "11         0                                  NaN   \n",
            "12         0                                  NaN   \n",
            "13         0                                  NaN   \n",
            "14         1          start missing person search   \n",
            "15         0                                  NaN   \n",
            "16         1          start missing person search   \n",
            "17         0                                  NaN   \n",
            "18         0                                  NaN   \n",
            "19         0                                  NaN   \n",
            "\n",
            "                      Responders (Suggested)  \n",
            "0                             Not applicable  \n",
            "1                             Not applicable  \n",
            "2                             Not applicable  \n",
            "3                             Not applicable  \n",
            "4                             Not applicable  \n",
            "5                             Not applicable  \n",
            "6            Red Cross, Emergency Management  \n",
            "7                             Not applicable  \n",
            "8                             Not applicable  \n",
            "9                             Not applicable  \n",
            "10                            Not applicable  \n",
            "11                            Not applicable  \n",
            "12                            Not applicable  \n",
            "13                            Not applicable  \n",
            "14  Search and Rescue Teams, Law Enforcement  \n",
            "15                            Not applicable  \n",
            "16  Search and Rescue Teams, Law Enforcement  \n",
            "17                            Not applicable  \n",
            "18                            Not applicable  \n",
            "19                            Not applicable  \n",
            "\n",
            "Value counts for 'Responders (Suggested)':\n",
            "Responders (Suggested)\n",
            "Not applicable                              16100\n",
            "Local Authorities, Emergency Services        1548\n",
            "Search and Rescue Teams, Law Enforcement      149\n",
            "Red Cross, Emergency Management               128\n",
            "Search and Rescue Teams, Fire Department       56\n",
            "Responders unclear                             53\n",
            "Emergency Medical Services                     31\n",
            "General Emergency Services                     17\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Step 1: Load Data ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "# --- Step 2: Create 'Wildfire' Column ---\n",
        "ground_truth_df['Wildfire'] = 'No'\n",
        "ground_truth_df.loc[ground_truth_df['state'] == 'California', 'Wildfire'] = 'Yes'\n",
        "print(\"'Wildfire' column created.\")\n",
        "\n",
        "# --- Step 3: Create 'Location' Column ---\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',\n",
        "    axis=1\n",
        ")\n",
        "print(\"'Location' column created.\")\n",
        "\n",
        "# --- Step 4: Create 'Responders (Suggested)' Column ---\n",
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "}\n",
        "\n",
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "print(\"'Responders (Suggested)' column created.\")\n",
        "\n",
        "# --- Step 5: Create sample_split ---\n",
        "label_counts = ground_truth_df.groupby(['Wildfire', 'distress']).size().reset_index(name='counts')\n",
        "\n",
        "sample_size_per_group = 25\n",
        "sample_split = pd.DataFrame()\n",
        "sampled_indices = []\n",
        "\n",
        "for index, row in label_counts.iterrows():\n",
        "    wildfire_label = row['Wildfire']\n",
        "    distress_label = row['distress']\n",
        "    count = row['counts']\n",
        "\n",
        "    n_samples = min(count, sample_size_per_group)\n",
        "    group_sample = ground_truth_df[\n",
        "        (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "    ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "    sample_split = pd.concat([sample_split, group_sample])\n",
        "    sampled_indices.extend(group_sample.index)\n",
        "\n",
        "print(\"sample_split created.\")\n",
        "\n",
        "# --- Step 6: Check Columns of sample_split ---\n",
        "print(\"Columns in sample_split after creation:\")\n",
        "print(sample_split.columns)\n",
        "\n",
        "# --- Step 7: Prepare Ground Truth for Evaluation ---\n",
        "ground_truth_wildfire = sample_split['Wildfire'].tolist()\n",
        "ground_truth_distress = sample_split['distress'].tolist()\n",
        "ground_truth_location = sample_split['Location'].tolist()\n",
        "ground_truth_action = sample_split['take_action'].tolist()\n",
        "ground_truth_responders = sample_split['Responders (Suggested)'].tolist()\n",
        "print(\"Ground truth prepared for evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnaXD4LcbsfL",
        "outputId": "f4d11d25-a43a-4f08-8b88-1c32baf8cfaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded.\n",
            "'Wildfire' column created.\n",
            "'Location' column created.\n",
            "'Responders (Suggested)' column created.\n",
            "sample_split created.\n",
            "Columns in sample_split after creation:\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire', 'Location', 'Responders (Suggested)'],\n",
            "      dtype='object')\n",
            "Ground truth prepared for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare prompts for the entire sample split ---\n",
        "prompts = []\n",
        "sample_tweets = sample_split['tweet_text'].tolist()\n",
        "\n",
        "for tweet in sample_tweets:\n",
        "    prompts.append(f\"Is this tweet about a California wildfire? Tweet: {tweet}\")\n",
        "    prompts.append(f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What location is mentioned in this tweet? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What action and responders are needed based on this tweet? Tweet: {tweet}\")\n",
        "\n",
        "# --- Tokenize and generate predictions for the entire sample split ---\n",
        "inputs = tokenizer_base.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model_base.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model_base.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "predictions = tokenizer_base.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# --- Reshape predictions to align with the four questions per tweet ---\n",
        "reshaped_predictions = [predictions[i:i + 4] for i in range(0, len(predictions), 4)]\n",
        "\n",
        "print(\"Predictions generated for the entire Sample Split.\")\n",
        "print(f\"Number of tweets in Sample Split: {len(sample_tweets)}\")\n",
        "print(f\"Number of sets of predictions: {len(reshaped_predictions)}\")\n",
        "print(\"First example:\")\n",
        "print(f\"Tweet: {sample_tweets[0][:50]}...\")\n",
        "print(f\"Predictions: {reshaped_predictions[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL-Xe8r_cOJL",
        "outputId": "be15134c-ca39-4829-8751-dd009ccddb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions generated for the entire Sample Split.\n",
            "Number of tweets in Sample Split: 100\n",
            "Number of sets of predictions: 100\n",
            "First example:\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Predictions: ['Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.', 'Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery? Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.', 'Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.', 'Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# --- Map T5 predictions to Yes/No for Wildfire ---\n",
        "predicted_wildfire = []\n",
        "for prediction in reshaped_predictions:\n",
        "    if any(keyword in prediction[0].lower() for keyword in [\"yes\", \"it is\", \"wildfire\", \"fire\", \"burn\"]):\n",
        "        predicted_wildfire.append(\"Yes\")\n",
        "    else:\n",
        "        predicted_wildfire.append(\"No\")\n",
        "\n",
        "# --- Map T5 predictions to Yes/No for Distress ---\n",
        "predicted_distress = []\n",
        "for prediction in reshaped_predictions:\n",
        "    if any(keyword in prediction[1].lower() for keyword in [\"help\", \"urgent\", \"emergency\", \"need\", \"assistance\", \"critical\", \"danger\"]):\n",
        "        predicted_distress.append(\"Yes\")\n",
        "    else:\n",
        "        predicted_distress.append(\"No\")\n",
        "\n",
        "# --- Evaluate Wildfire detection ---\n",
        "wildfire_accuracy = accuracy_score(ground_truth_wildfire, predicted_wildfire)\n",
        "wildfire_f1 = f1_score(\n",
        "    [1 if label == \"Yes\" else 0 for label in ground_truth_wildfire],\n",
        "    [1 if label == \"Yes\" else 0 for label in predicted_wildfire]\n",
        ")\n",
        "\n",
        "print(f\"Wildfire Detection Accuracy: {wildfire_accuracy:.4f}\")\n",
        "print(f\"Wildfire Detection F1 Score: {wildfire_f1:.4f}\")\n",
        "\n",
        "# --- Evaluate Distress detection ---\n",
        "# Note: ground_truth_distress is 0 or 1, so we map predicted_distress accordingly\n",
        "distress_accuracy = accuracy_score(\n",
        "    ground_truth_distress, [1 if label == \"Yes\" else 0 for label in predicted_distress]\n",
        ")\n",
        "distress_f1 = f1_score(ground_truth_distress, [1 if label == \"Yes\" else 0 for label in predicted_distress])\n",
        "\n",
        "print(f\"Distress Detection Accuracy: {distress_accuracy:.4f}\")\n",
        "print(f\"Distress Detection F1 Score: {distress_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiaQBRSmcOMe",
        "outputId": "12c21bbe-1363-41ca-bcc8-16313aacf13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wildfire Detection Accuracy: 0.9000\n",
            "Wildfire Detection F1 Score: 0.9057\n",
            "Distress Detection Accuracy: 0.6400\n",
            "Distress Detection F1 Score: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import inspect\n",
        "\n",
        "print(\"Contents of nltk.metrics:\")\n",
        "print(inspect.getmembers(nltk.metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU4YYsTocOQw",
        "outputId": "0c367f5e-d774-45bd-e2fd-defc895aba34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of nltk.metrics:\n",
            "[('__builtins__', {'__name__': 'builtins', '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x79a205e47090>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'aiter': <built-in function aiter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'anext': <built-in function anext>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'BaseExceptionGroup': <class 'BaseExceptionGroup'>, 'Exception': <class 'Exception'>, 'GeneratorExit': <class 'GeneratorExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'SystemExit': <class 'SystemExit'>, 'ArithmeticError': <class 'ArithmeticError'>, 'AssertionError': <class 'AssertionError'>, 'AttributeError': <class 'AttributeError'>, 'BufferError': <class 'BufferError'>, 'EOFError': <class 'EOFError'>, 'ImportError': <class 'ImportError'>, 'LookupError': <class 'LookupError'>, 'MemoryError': <class 'MemoryError'>, 'NameError': <class 'NameError'>, 'OSError': <class 'OSError'>, 'ReferenceError': <class 'ReferenceError'>, 'RuntimeError': <class 'RuntimeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'SyntaxError': <class 'SyntaxError'>, 'SystemError': <class 'SystemError'>, 'TypeError': <class 'TypeError'>, 'ValueError': <class 'ValueError'>, 'Warning': <class 'Warning'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'BytesWarning': <class 'BytesWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'EncodingWarning': <class 'EncodingWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'UserWarning': <class 'UserWarning'>, 'BlockingIOError': <class 'BlockingIOError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionError': <class 'ConnectionError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'InterruptedError': <class 'InterruptedError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'IndentationError': <class 'IndentationError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'RecursionError': <class 'RecursionError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'UnicodeError': <class 'UnicodeError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'TabError': <class 'TabError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'ExceptionGroup': <class 'ExceptionGroup'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 2000 BeOpen.com.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
            "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
            "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x79a2254ef2e0>, 'runfile': <function runfile at 0x79a225398f40>, '__IPYTHON__': True, 'display': <function display at 0x79a2263f1e40>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x79a205ec2880>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x79a19679d230>, '__pybind11_internals_v4_clang_libstdcpp_cxxabi1002__': <capsule object NULL at 0x79a1fc8e8570>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x79a205e64410>>}), ('__cached__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/__pycache__/metrics.cpython-311.pyc'), ('__doc__', None), ('__file__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py'), ('__loader__', <_frozen_importlib_external.SourceFileLoader object at 0x799f98521590>), ('__name__', 'nltk.translate.metrics'), ('__package__', 'nltk.translate'), ('__spec__', ModuleSpec(name='nltk.translate.metrics', loader=<_frozen_importlib_external.SourceFileLoader object at 0x799f98521590>, origin='/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py')), ('alignment_error_rate', <function alignment_error_rate at 0x799f9834b740>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import inspect\n",
        "\n",
        "print(\"Contents of nltk.translate.metrics:\")\n",
        "print(inspect.getmembers(nltk.translate.metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nv_hoOAccf1",
        "outputId": "49264c09-935e-40ae-daf7-51a6fca7f67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of nltk.translate.metrics:\n",
            "[('__builtins__', {'__name__': 'builtins', '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x79a205e47090>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'aiter': <built-in function aiter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'anext': <built-in function anext>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'BaseExceptionGroup': <class 'BaseExceptionGroup'>, 'Exception': <class 'Exception'>, 'GeneratorExit': <class 'GeneratorExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'SystemExit': <class 'SystemExit'>, 'ArithmeticError': <class 'ArithmeticError'>, 'AssertionError': <class 'AssertionError'>, 'AttributeError': <class 'AttributeError'>, 'BufferError': <class 'BufferError'>, 'EOFError': <class 'EOFError'>, 'ImportError': <class 'ImportError'>, 'LookupError': <class 'LookupError'>, 'MemoryError': <class 'MemoryError'>, 'NameError': <class 'NameError'>, 'OSError': <class 'OSError'>, 'ReferenceError': <class 'ReferenceError'>, 'RuntimeError': <class 'RuntimeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'SyntaxError': <class 'SyntaxError'>, 'SystemError': <class 'SystemError'>, 'TypeError': <class 'TypeError'>, 'ValueError': <class 'ValueError'>, 'Warning': <class 'Warning'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'BytesWarning': <class 'BytesWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'EncodingWarning': <class 'EncodingWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'UserWarning': <class 'UserWarning'>, 'BlockingIOError': <class 'BlockingIOError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionError': <class 'ConnectionError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'InterruptedError': <class 'InterruptedError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'IndentationError': <class 'IndentationError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'RecursionError': <class 'RecursionError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'UnicodeError': <class 'UnicodeError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'TabError': <class 'TabError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'ExceptionGroup': <class 'ExceptionGroup'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 2000 BeOpen.com.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
            "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
            "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x79a2254ef2e0>, 'runfile': <function runfile at 0x79a225398f40>, '__IPYTHON__': True, 'display': <function display at 0x79a2263f1e40>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x79a205ec2880>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x79a19679d230>, '__pybind11_internals_v4_clang_libstdcpp_cxxabi1002__': <capsule object NULL at 0x79a1fc8e8570>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x79a205e64410>>}), ('__cached__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/__pycache__/metrics.cpython-311.pyc'), ('__doc__', None), ('__file__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py'), ('__loader__', <_frozen_importlib_external.SourceFileLoader object at 0x799f98521590>), ('__name__', 'nltk.translate.metrics'), ('__package__', 'nltk.translate'), ('__spec__', ModuleSpec(name='nltk.translate.metrics', loader=<_frozen_importlib_external.SourceFileLoader object at 0x799f98521590>, origin='/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py')), ('alignment_error_rate', <function alignment_error_rate at 0x799f9834b740>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0ySKTd-ccj6",
        "outputId": "9ab16273-10ba-4264-8f65-006a25d49c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "# --- Evaluate Location Prediction ---\n",
        "location_bleu_scores = []\n",
        "location_rouge_scores = []\n",
        "\n",
        "for i in range(len(sample_tweets)):\n",
        "    reference = ground_truth_location[i].lower()\n",
        "    prediction = reshaped_predictions[i][2].lower()\n",
        "    if reference and prediction:  # Avoid errors with empty strings\n",
        "        reference_list = [reference.split()]\n",
        "        prediction_list = prediction.split()\n",
        "        bleu_score = sentence_bleu(reference_list, prediction_list)\n",
        "        scores = rouge.get_scores(prediction, reference)\n",
        "        location_bleu_scores.append(bleu_score)\n",
        "        if scores:\n",
        "            location_rouge_scores.append(scores[0])\n",
        "        else:\n",
        "            location_rouge_scores.append({'rouge-1': {'f': 0}, 'rouge-l': {'f': 0}}) # Handle cases with no scores\n",
        "\n",
        "avg_location_bleu = sum(location_bleu_scores) / len(location_bleu_scores) if location_bleu_scores else 0\n",
        "avg_location_rouge_1 = sum(score['rouge-1']['f'] for score in location_rouge_scores) / len(location_rouge_scores) if location_rouge_scores else 0\n",
        "avg_location_rouge_l = sum(score['rouge-l']['f'] for score in location_rouge_scores) / len(location_rouge_scores) if location_rouge_scores else 0\n",
        "\n",
        "print(f\"\\nAverage BLEU Score (Location): {avg_location_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score (Location): {avg_location_rouge_1:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score (Location): {avg_location_rouge_l:.4f}\")\n",
        "\n",
        "# --- Evaluate Action/Responders Prediction ---\n",
        "action_bleu_scores = []\n",
        "action_rouge_scores = []\n",
        "\n",
        "for i in range(len(sample_tweets)):\n",
        "    reference = (str(ground_truth_action[i]) + \" \" + str(ground_truth_responders[i])).lower()\n",
        "    prediction = reshaped_predictions[i][3].lower()\n",
        "    if reference and prediction:  # Avoid errors with empty strings\n",
        "        reference_list = [reference.split()]\n",
        "        prediction_list = prediction.split()\n",
        "        bleu_score = sentence_bleu(reference_list, prediction_list)\n",
        "        scores = rouge.get_scores(prediction, reference)\n",
        "        action_bleu_scores.append(bleu_score)\n",
        "        if scores:\n",
        "            action_rouge_scores.append(scores[0])\n",
        "        else:\n",
        "            action_rouge_scores.append({'rouge-1': {'f': 0}, 'rouge-l': {'f': 0}}) # Handle cases with no scores\n",
        "\n",
        "avg_action_bleu = sum(action_bleu_scores) / len(action_bleu_scores) if action_bleu_scores else 0\n",
        "avg_action_rouge_1 = sum(score['rouge-1']['f'] for score in action_rouge_scores) / len(action_rouge_scores) if action_rouge_scores else 0\n",
        "avg_action_rouge_l = sum(score['rouge-l']['f'] for score in action_rouge_scores) / len(action_rouge_scores) if action_rouge_scores else 0\n",
        "\n",
        "print(f\"\\nAverage BLEU Score (Action/Responders): {avg_action_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score (Action/Responders): {avg_action_rouge_1:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score (Action/Responders): {avg_action_rouge_l:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp-dvkQxccpG",
        "outputId": "9e5dd217-3279-47fc-d91d-e6fe9060da4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score (Location): 0.0000\n",
            "Average ROUGE-1 F1 Score (Location): 0.0732\n",
            "Average ROUGE-L F1 Score (Location): 0.0732\n",
            "\n",
            "Average BLEU Score (Action/Responders): 0.0000\n",
            "Average ROUGE-1 F1 Score (Action/Responders): 0.0130\n",
            "Average ROUGE-L F1 Score (Action/Responders): 0.0130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 4: Instruction Fine-Tuning."
      ],
      "metadata": {
        "id": "_M6mtdjZcnwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Step 1: Load Data ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "print(\"First few rows of ground_truth_df after loading:\")\n",
        "print(ground_truth_df.head())\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "# --- Step 2: Create 'Wildfire' Column ---\n",
        "ground_truth_df['Wildfire'] = 'No'\n",
        "ground_truth_df.loc[ground_truth_df['state'] == 'California', 'Wildfire'] = 'Yes'\n",
        "print(\"'Wildfire' column created.\")\n",
        "\n",
        "# --- Step 3: Create 'Location' Column ---\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',\n",
        "    axis=1\n",
        ")\n",
        "print(\"'Location' column created.\")\n",
        "\n",
        "# --- Step 4: Create 'Responders (Suggested)' Column ---\n",
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "}\n",
        "\n",
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "print(\"'Responders (Suggested)' column created.\")\n",
        "\n",
        "# --- Step 5: Create sample_split ---\n",
        "label_counts = ground_truth_df.groupby(['Wildfire', 'distress']).size().reset_index(name='counts')\n",
        "\n",
        "sample_size_per_group = 25\n",
        "sample_split = pd.DataFrame()\n",
        "sampled_indices = []\n",
        "\n",
        "for index, row in label_counts.iterrows():\n",
        "    wildfire_label = row['Wildfire']\n",
        "    distress_label = row['distress']\n",
        "    count = row['counts']\n",
        "\n",
        "    n_samples = min(count, sample_size_per_group)\n",
        "    group_sample = ground_truth_df[\n",
        "        (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "    ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "    sample_split = pd.concat([sample_split, group_sample])\n",
        "    sampled_indices.extend(group_sample.index)\n",
        "\n",
        "print(\"sample_split created.\")\n",
        "\n",
        "# --- Step 6: Check Columns of sample_split ---\n",
        "print(\"Columns in sample_split after creation:\")\n",
        "print(sample_split.columns)\n",
        "\n",
        "# --- Step 7: Prepare Ground Truth for Evaluation ---\n",
        "ground_truth_wildfire = sample_split['Wildfire'].tolist()\n",
        "ground_truth_distress = sample_split['distress'].tolist()\n",
        "ground_truth_location = sample_split['Location'].tolist()\n",
        "ground_truth_action = sample_split['take_action'].tolist()\n",
        "ground_truth_responders = sample_split['Responders (Suggested)'].tolist()\n",
        "print(\"Ground truth prepared for evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-NHq3cTcctU",
        "outputId": "23cb6a30-dd69-4468-f11c-c5c5ecd07ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of ground_truth_df after loading:\n",
            "       tweet_id                  image_id  \\\n",
            "0  9.177910e+17  917791044158185473_0.jpg   \n",
            "1  9.177911e+17  917791130590183424_0.jpg   \n",
            "2  9.177913e+17  917791291823591425_0.jpg   \n",
            "3  9.177913e+17  917791291823591425_1.jpg   \n",
            "4  9.177921e+17  917792092100988929_0.jpg   \n",
            "\n",
            "                                      raw_tweet_text  \\\n",
            "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
            "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
            "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "3  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "4  RT @TIME: California's raging wildfires as you...   \n",
            "\n",
            "                                          tweet_text tweet_hashtags  \\\n",
            "0  wildfires raging through northern california a...            NaN   \n",
            "1         photos deadly wildfires rage in california            NaN   \n",
            "2  pls share were capturing wildfire response rec...            NaN   \n",
            "3  pls share were capturing wildfire response rec...            NaN   \n",
            "4  californias raging wildfires as youve never se...            NaN   \n",
            "\n",
            "                                       image_caption  distress take_action  \\\n",
            "0  a fire is seen burning through the trees in th...         0         NaN   \n",
            "1  two people are standing in a burned area with ...         0         NaN   \n",
            "2         a fire burns in a burned area near a house         0         NaN   \n",
            "3  several people standing in front of a screen w...         0         NaN   \n",
            "4  a night sky with a mountain and a milky in the...         0         NaN   \n",
            "\n",
            "        state sub_location Wildfire  \n",
            "0  California     northern      Yes  \n",
            "1  California          NaN      Yes  \n",
            "2         NaN          NaN       No  \n",
            "3         NaN          NaN       No  \n",
            "4  California          NaN      Yes  \n",
            "Data loaded.\n",
            "'Wildfire' column created.\n",
            "'Location' column created.\n",
            "'Responders (Suggested)' column created.\n",
            "sample_split created.\n",
            "Columns in sample_split after creation:\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire', 'Location', 'Responders (Suggested)'],\n",
            "      dtype='object')\n",
            "Ground truth prepared for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Split the data into training, validation, and test sets ---\n",
        "train_df, temp_df = train_test_split(ground_truth_df, test_size=0.2, random_state=42, stratify=ground_truth_df[['Wildfire', 'distress']])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[['Wildfire', 'distress']])\n",
        "\n",
        "print(f\"Size of training set: {len(train_df)}\")\n",
        "print(f\"Size of validation set: {len(val_df)}\")\n",
        "print(f\"Size of test set: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txATzG0HcOUo",
        "outputId": "9af3ec98-9a9a-454d-928c-b3727c7ac274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: 14465\n",
            "Size of validation set: 1808\n",
            "Size of test set: 1809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: Prepare the training data from train_df (handling NaN locations) ---\n",
        "train_data = []\n",
        "for index, row in train_df.iterrows():\n",
        "    tweet = row['tweet_text']\n",
        "    wildfire_answer = 'yes' if row['Wildfire'] == 1 else 'no'\n",
        "    distress_answer = 'distress' if row['distress'] == 1 else 'not distress'\n",
        "    location_answer = row['Location']\n",
        "    action_responders_answer = f\"{row['take_action']} {row['Responders (Suggested)']}\"\n",
        "\n",
        "    # Wildfire Classifier with Instruction\n",
        "    train_data.append({\n",
        "        'prompt': f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California, not wildfires in general or other locations. Only respond with one word: 'yes' or 'no'. Do not explain. If the tweet references smoke, flames, evacuations, or fire-related events and mentions a California city or region (e.g., LA, San Francisco, Bay Area), respond with 'yes'. Otherwise, respond 'no'. Tweet: {tweet}\",\n",
        "        'target': wildfire_answer\n",
        "    })\n",
        "\n",
        "    # Emergency Detection System with Instruction\n",
        "    train_data.append({\n",
        "        'prompt': f\"You are an emergency detection system. Determine if the tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: {tweet}\",\n",
        "        'target': distress_answer\n",
        "    })\n",
        "\n",
        "    # Location Extraction with Instruction (Handle NaN)\n",
        "    train_data.append({\n",
        "        'prompt': f\"Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: {tweet}\",\n",
        "        'target': location_answer if pd.notna(location_answer) else 'unknown'\n",
        "    })\n",
        "\n",
        "    # Disaster Response Coordinator with Instruction\n",
        "    train_data.append({\n",
        "        'prompt': f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action that responders should take. Choose only one from the following types: evacuation, medical aid, fire suppression, rescue, or resource delivery. If none of these apply or there's no clear threat, respond with 'monitor only'. Respond with only one action. Tweet: {tweet}\",\n",
        "        'target': row['take_action'] if pd.notna(row['take_action']) else 'monitor only' # Handle potential NaN actions as well (though less likely)\n",
        "    })\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(\"First training example:\")\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWhx4DM1cuAg",
        "outputId": "01658198-fc89-45c6-d06f-2d3bb809bb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 57860\n",
            "First training example:\n",
            "{'prompt': \"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California, not wildfires in general or other locations. Only respond with one word: 'yes' or 'no'. Do not explain. If the tweet references smoke, flames, evacuations, or fire-related events and mentions a California city or region (e.g., LA, San Francisco, Bay Area), respond with 'yes'. Otherwise, respond 'no'. Tweet: irma victims need our help they cant recover on their own #irmarecovery #irmavictims 9donate medical suppliesb\", 'target': 'no'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 9: Prepare the training data from train_df with instructions ---\n",
        "train_data = []\n",
        "for index, row in train_df.iterrows():\n",
        "    tweet = row['tweet_text']\n",
        "    wildfire_answer = 'yes' if row['Wildfire'] == 1 else 'no'\n",
        "    distress_answer = 'distress' if row['distress'] == 1 else 'not distress'\n",
        "    location_answer = row['Location']\n",
        "    action_responders_answer = f\"{row['take_action']} {row['Responders (Suggested)']}\"\n",
        "\n",
        "    # Wildfire Classifier with Instruction\n",
        "    train_data.append({\n",
        "        'prompt': f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California, not wildfires in general or other locations. Only respond with one word: 'yes' or 'no'. Do not explain. If the tweet references smoke, flames, evacuations, or fire-related events and mentions a California city or region (e.g., LA, San Francisco, Bay Area), respond with 'yes'. Otherwise, respond 'no'. Tweet: {tweet}\",\n",
        "        'target': wildfire_answer\n",
        "    })\n",
        "\n",
        "    # Emergency Detection System with Instruction\n",
        "    train_data.append({\n",
        "        'prompt': f\"You are an emergency detection system. Determine if the tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: {tweet}\",\n",
        "        'target': distress_answer\n",
        "    })\n",
        "\n",
        "    # Location Extraction with Instruction (Handle NaN)\n",
        "    train_data.append({\n",
        "        'prompt': f\"Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: {tweet}\",\n",
        "        'target': location_answer if pd.notna(location_answer) else 'unknown'\n",
        "    })\n",
        "\n",
        "    # Disaster Response Coordinator with Instruction\n",
        "    train_data.append({\n",
        "        'prompt': f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action that responders should take. Choose only one from the following types: evacuation, medical aid, fire suppression, rescue, or resource delivery. If none of these apply or there's no clear threat, respond with 'monitor only'. Respond with only one action. Tweet: {tweet}\",\n",
        "        'target': row['take_action'] if pd.notna(row['take_action']) else 'monitor only'\n",
        "    })\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(\"First training example:\")\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m1lg0B5nCYI",
        "outputId": "c7caa6ee-21ab-4995-dbc4-39710e573cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 57860\n",
            "First training example:\n",
            "{'prompt': \"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California, not wildfires in general or other locations. Only respond with one word: 'yes' or 'no'. Do not explain. If the tweet references smoke, flames, evacuations, or fire-related events and mentions a California city or region (e.g., LA, San Francisco, Bay Area), respond with 'yes'. Otherwise, respond 'no'. Tweet: irma victims need our help they cant recover on their own #irmarecovery #irmavictims 9donate medical suppliesb\", 'target': 'no'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Llama"
      ],
      "metadata": {
        "id": "X8tQdUj-c1HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch peft accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVDSC0hEcuEk",
        "outputId": "777201f2-0f6f-4758-f955-f4613b1c0e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"***********************\")"
      ],
      "metadata": {
        "id": "GZqOF8GZcuJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "try:\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",  # Automatically put the model on available GPU(s)\n",
        "    )\n",
        "    print(\"LLaMA-2-7b loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LLaMA-2-7b: {e}\")\n",
        "    print(\"Please ensure you have accepted the terms on Hugging Face and have a valid access token if required.\")\n",
        "    print(\"We might need to consider a different model or approach.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f1ebebf55e67486a9ec4b8ca41b03dc4",
            "c012d6ab9f84429f8878c53d0e372fd3",
            "e54df062433d47f88736bd5022ad289c",
            "25ce15978d794864ad145b54ee78c6a3",
            "6ecae96315e84ab783a40c94a6a25f4b",
            "b006e5790fd74ca88b34c12d1193424a",
            "70666b207d1a46489365f079d22f41c8",
            "8e1e701db75e425ca5669e7398ac40b4",
            "24b17dceaa924839a3e8685646661436",
            "64d36fe81e1146d99fe6847d046fc01b",
            "acd6df2db53b4210976cefe81094e5d4"
          ]
        },
        "id": "vjBQvcVXcuNY",
        "outputId": "cfb7140c-1daf-44c3-9f59-06c949928ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1ebebf55e67486a9ec4b8ca41b03dc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLaMA-2-7b loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "print(f\"Padding token set to: {tokenizer_llama.pad_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVzmWDUucuRj",
        "outputId": "795ed2d0-a22a-461b-e36c-1833cb9d6974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding token set to: </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_llama_data(data, tokenizer, max_length=512):\n",
        "    tokenized_inputs = []\n",
        "    for i, item in enumerate(data):\n",
        "        if not isinstance(item, dict) or 'prompt' not in item or 'target' not in item:\n",
        "            print(f\"Error at index {i}: Invalid data item - {item}\")\n",
        "            continue  # Skip this item\n",
        "\n",
        "        prompt = item['prompt']\n",
        "        target = item['target']\n",
        "\n",
        "        if not isinstance(prompt, str):\n",
        "            print(f\"Error at index {i}: Prompt is not a string - {prompt}\")\n",
        "            continue\n",
        "\n",
        "        if not isinstance(target, str):\n",
        "            print(f\"Error at index {i}: Target is not a string - {target}\")\n",
        "            continue\n",
        "\n",
        "        prompt_encodings = tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encodings = tokenizer(\n",
        "            target,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        tokenized_inputs.append({\n",
        "            'input_ids': prompt_encodings['input_ids'][0],\n",
        "            'attention_mask': prompt_encodings['attention_mask'][0],\n",
        "            'labels': target_encodings['input_ids'][0],\n",
        "        })\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Prepare the training data\n",
        "processed_train_data_llama = prepare_llama_data(train_data, tokenizer_llama)\n",
        "\n",
        "print(f\"Number of processed training examples: {len(processed_train_data_llama)}\")\n",
        "if processed_train_data_llama:\n",
        "    print(\"First processed training example:\")\n",
        "    print(processed_train_data_llama[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcnN0jHxcuVB",
        "outputId": "c7223532-5890-4185-f7df-3e356dac678b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of processed training examples: 57860\n",
            "First processed training example:\n",
            "{'input_ids': tensor([    1,   887,   526,   263,  8775,  8696,   770,  3709, 29889,  5953,\n",
            "          837,   457,   565,   278,  7780,   300,   338,  9479,  1048,   263,\n",
            "         8775,  8696, 10464,   297,  8046, 29892,   451,  8775, 29888,  2658,\n",
            "          297,  2498,   470,   916, 14354, 29889,  9333, 10049,   411,   697,\n",
            "         1734, 29901,   525,  3582, 29915,   470,   525,  1217,  4286,  1938,\n",
            "          451,  5649, 29889,   960,   278,  7780,   300,  9282, 25158, 29892,\n",
            "         1652,  1280, 29892,  3415, 22061,   800, 29892,   470,  3974, 29899,\n",
            "        12817,  4959,   322, 26649,   263,  8046,  4272,   470,  5120,   313,\n",
            "        29872, 29889, 29887,  1696, 17900, 29892,  3087,  8970, 29892,  6211,\n",
            "        18320,   511, 10049,   411,   525,  3582,  4286, 13466, 29892, 10049,\n",
            "          525,  1217,  4286,   323, 16668, 29901,  3805,   655,  6879,  9893,\n",
            "          817,  1749,  1371,   896,  5107,  9792,   373,  1009,  1914,   396,\n",
            "         3568,   598, 11911, 29891,   396,  3568,   485,   919,  9893, 29871,\n",
            "        29929,  9176,   403, 16083, 28075, 29890,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([  1, 694,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the size of the LLaMA 2 7B model, it's highly likely that we'll run into memory issues if we try to fine-tune the entire model on a standard Colab GPU. To address this, we'll use LoRA (Low-Rank Adaptation).\n",
        "\n",
        "What is LoRA?\n",
        "\n",
        "LoRA is a Parameter-Efficient Fine-Tuning (PEFT) technique that freezes the pre-trained model weights and adds a small number of new trainable layers (called \"adapters\"). These adapters are low-rank matrices, which means they have far fewer parameters than the original model. During fine-tuning, only these adapter weights are updated, significantly reducing the memory footprint and training time."
      ],
      "metadata": {
        "id": "7Rkz38uYfl85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model loading and LoRA Configuration and Application:\n",
        "\n"
      ],
      "metadata": {
        "id": "y8VfJ8u35eEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        llm_int8_enable_fp32_cpu_offload=False,\n",
        "    )\n",
        "\n",
        "    # Load the base model directly onto the GPU with quantization\n",
        "    model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map={\"\": device},  # Load directly to GPU\n",
        "    )\n",
        "    print(\"LLaMA-2-7b loaded with 4-bit quantization onto:\", device)\n",
        "\n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"gate_proj\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Get the LoRA model\n",
        "    model_lora = get_peft_model(model_llama, lora_config)\n",
        "    model_lora.print_trainable_parameters()\n",
        "\n",
        "    from transformers import TrainingArguments, Trainer\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./llama-2-7b-lora-fine-tune\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=3,\n",
        "        fp16=True if device == \"cuda\" else False,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "\n",
        "    # Create the Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model_lora,\n",
        "        train_dataset=processed_train_data_llama,\n",
        "        eval_dataset=None,\n",
        "        args=training_args,\n",
        "        data_collator=lambda data: {k: torch.stack([f[k] for f in data]) for k in data[0]},\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check the error message for more details.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "1ef05f224aa64fb68ce4f51f50335742",
            "bac2ab631021451fb52a08f4768fb890",
            "f5e74949343e4f349b123159bafe192d",
            "2c44435fcdc04f12a9b1940f080b4392",
            "e721f090a31c4aababe3ab1bb5a17cd6",
            "bcb9991902bc45469d5cd3e19ca6384e",
            "e0caa17743bf4b5f9b48f95f9eda1c25",
            "ab71615002ff47ce80b0390168128a5a",
            "0ce9ee6c7dcf475bb200d4a829aae284",
            "d18491b1e87743bfa2eec05fdff99fa2",
            "50a2b82d6ac14d84ac1ac18b835f0f0a"
          ]
        },
        "id": "geEFTVwEeZ9W",
        "outputId": "71d57882-3589-44a9-a394-382bfefc9d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ef05f224aa64fb68ce4f51f50335742"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLaMA-2-7b loaded with 4-bit quantization onto: cuda\n",
            "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: device() received an invalid combination of arguments - got (NoneType), but expected one of:\n",
            " * (torch.device device)\n",
            "      didn't match because some of the arguments have invalid types: (!NoneType!)\n",
            " * (str type, int index = -1)\n",
            "\n",
            "Please check the error message for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output confirms that LoRA has been successfully applied to the LLaMA 2 model.\n",
        "\n",
        "As you can see:\n",
        "\n",
        "Trainable parameters: 19,988,480\n",
        "Total parameters: 6,758,404,096\n",
        "Trainable percentage: 0.2958%\n",
        "This is a dramatic reduction in the number of parameters that will be updated during training. Only about 0.3% of the model's total parameters will be trained, which will significantly reduce memory usage and speed up the fine-tuning process, making it feasible to run on a Colab GPU.\n",
        "\n",
        "Now that we have our LoRA-adapted LLaMA 2 model and our processed training data, the next step is to set up the training using the Hugging Face Trainer API."
      ],
      "metadata": {
        "id": "CtsdlBk8fwIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD9-hVt9gKMK",
        "outputId": "46c25efa-9e7f-4d08-96f7-036a908bac81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Aj4bCMgfO8",
        "outputId": "d0f2c1d2-1ecc-466d-ef02-bc21b823d674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: bitsandbytes\n",
            "Version: 0.45.5\n",
            "Summary: k-bit optimizers and matrix multiplication routines.\n",
            "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
            "Author: \n",
            "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
            "License: MIT License\n",
            "\n",
            "Copyright (c) Facebook, Inc. and its affiliates.\n",
            "\n",
            "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
            "of this software and associated documentation files (the \"Software\"), to deal\n",
            "in the Software without restriction, including without limitation the rights\n",
            "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
            "copies of the Software, and to permit persons to whom the Software is\n",
            "furnished to do so, subject to the following conditions:\n",
            "\n",
            "The above copyright notice and this permission notice shall be included in all\n",
            "copies or substantial portions of the Software.\n",
            "\n",
            "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
            "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
            "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
            "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
            "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
            "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
            "SOFTWARE.\n",
            "\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: numpy, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_81Zw8Kgo50",
        "outputId": "382c94ce-6af2-46d6-e98e-7a4b325c3984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the validation data\n",
        "val_data = []\n",
        "for index, row in val_df.iterrows():\n",
        "    tweet = row['tweet_text']\n",
        "    wildfire_answer = 'yes' if row['Wildfire'] == 1 else 'no'\n",
        "    distress_answer = 'distress' if row['distress'] == 1 else 'not distress'\n",
        "    location_answer = row['Location']\n",
        "    action_responders_answer = f\"{row['take_action']} {row['Responders (Suggested)']}\"\n",
        "\n",
        "    # Wildfire Classifier with Instruction\n",
        "    val_data.append({\n",
        "        'prompt': f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California, not wildfires in general or other locations. Only respond with one word: 'yes' or 'no'. Do not explain. If the tweet references smoke, flames, evacuations, or fire-related events and mentions a California city or region (e.g., LA, San Francisco, Bay Area), respond with 'yes'. Otherwise, respond 'no'. Tweet: {tweet}\",\n",
        "        'target': wildfire_answer\n",
        "    })\n",
        "\n",
        "    # Emergency Detection System with Instruction\n",
        "    val_data.append({\n",
        "        'prompt': f\"You are an emergency detection system. Determine if the tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: {tweet}\",\n",
        "        'target': distress_answer\n",
        "    })\n",
        "\n",
        "    # Location Extraction with Instruction (Handle NaN)\n",
        "    val_data.append({\n",
        "        'prompt': f\"Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: {tweet}\",\n",
        "        'target': location_answer if pd.notna(location_answer) else 'unknown'\n",
        "    })\n",
        "\n",
        "    # Disaster Response Coordinator with Instruction\n",
        "    val_data.append({\n",
        "        'prompt': f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action that responders should take. Choose only one from the following types: evacuation, medical aid, fire suppression, rescue, or resource delivery. If none of these apply or there's no clear threat, respond with 'monitor only'. Respond with only one action. Tweet: {tweet}\",\n",
        "        'target': row['take_action'] if pd.notna(row['take_action']) else 'monitor only'\n",
        "    })\n",
        "\n",
        "processed_val_data_llama = prepare_llama_data(val_data, tokenizer_llama)\n",
        "\n",
        "print(f\"Number of processed validation examples: {len(processed_val_data_llama)}\")\n",
        "if processed_val_data_llama:\n",
        "    print(\"First processed validation example:\")\n",
        "    print(processed_val_data_llama[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-GuBDenp7IO",
        "outputId": "d419c1c8-5d32-4c53-9d8e-4cfa662bc145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of processed validation examples: 7232\n",
            "First processed validation example:\n",
            "{'input_ids': tensor([    1,   887,   526,   263,  8775,  8696,   770,  3709, 29889,  5953,\n",
            "          837,   457,   565,   278,  7780,   300,   338,  9479,  1048,   263,\n",
            "         8775,  8696, 10464,   297,  8046, 29892,   451,  8775, 29888,  2658,\n",
            "          297,  2498,   470,   916, 14354, 29889,  9333, 10049,   411,   697,\n",
            "         1734, 29901,   525,  3582, 29915,   470,   525,  1217,  4286,  1938,\n",
            "          451,  5649, 29889,   960,   278,  7780,   300,  9282, 25158, 29892,\n",
            "         1652,  1280, 29892,  3415, 22061,   800, 29892,   470,  3974, 29899,\n",
            "        12817,  4959,   322, 26649,   263,  8046,  4272,   470,  5120,   313,\n",
            "        29872, 29889, 29887,  1696, 17900, 29892,  3087,  8970, 29892,  6211,\n",
            "        18320,   511, 10049,   411,   525,  3582,  4286, 13466, 29892, 10049,\n",
            "          525,  1217,  4286,   323, 16668, 29901,   864,   304,  1073,   920,\n",
            "          396, 29876,  4378, 29883,  6911,  7902,   983,   337,  4282,  1156,\n",
            "          263,  5613,   766,  1901,   396,   381,   655,   396,  8222,  6950,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([  1, 694,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
            "          2,   2,   2,   2,   2,   2,   2,   2])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caXxr6fMqWzT",
        "outputId": "3f29c98c-6c0a-47bf-b0f9-1763a6bd7032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: accelerate\n",
            "Version: 1.6.0\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: zach.mueller@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: peft\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate>=0.26.0"
      ],
      "metadata": {
        "id": "Lbup49gC42Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model loading"
      ],
      "metadata": {
        "id": "W_JRENOk5KW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Determine the device to use\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "# Configure 4-bit quantization (optional, but recommended for memory efficiency)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=False,\n",
        ")\n",
        "\n",
        "# Load the pre-trained model with the quantization configuration (without device_map initially)\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        ")\n",
        "print(\"LLaMA-2-7b loaded with 4-bit quantization (initially on CPU).\")\n",
        "\n",
        "# Define the LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,             # Rank of the LoRA matrices\n",
        "    lora_alpha=32,       # Scaling factor for the LoRA matrices\n",
        "    lora_dropout=0.05,   # Dropout probability for LoRA layers\n",
        "    bias=\"none\",\n",
        "    target_modules=[    # The names of the modules to apply LoRA to\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply LoRA to the base model\n",
        "model_lora = get_peft_model(model_llama, lora_config)\n",
        "\n",
        "# Move the LoRA model to the GPU\n",
        "model_lora.to(device)\n",
        "\n",
        "model_lora.print_trainable_parameters()\n",
        "\n",
        "print(f\"\\nLoRA adapters applied and model moved to {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "c02e05ae64fe48ff9cf399e1d835e3a0",
            "6d42c32aa5384920a5b9804068736d72",
            "f4d1382dbd1d482abf4f0017da0cb88a",
            "41d71cb85e1f4c80b6218ad7a996609e",
            "1ded3436ebc74681ad00d21a887ddcec",
            "d6ea2b54ee53419bbb34e68ec1fd2398",
            "e1f6ab9fee6146d6b8269c5de87c458f",
            "9c73333580ee41b8a933de484130a3ff",
            "df02b75cc435416a92598076f953047a",
            "787777a990054dee804ff6bac37b38c4",
            "a22a00f41f7f4ceea9788885f1b55c2a"
          ]
        },
        "id": "txB7o9314jHA",
        "outputId": "e23eaf74-5380-45de-84b5-498344b58756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c02e05ae64fe48ff9cf399e1d835e3a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLaMA-2-7b loaded with 4-bit quantization (initially on CPU).\n",
            "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958\n",
            "\n",
            "LoRA adapters applied and model moved to cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating sample dataset"
      ],
      "metadata": {
        "id": "2Wur3Obk8zRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets peft accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83hziZ-Y_Ruq",
        "outputId": "dcf255a9-20bc-4d67-b283-b3eccb99672e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.30.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Collecting peft\n",
            "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.0\n",
            "    Uninstalling transformers-4.30.0:\n",
            "      Successfully uninstalled transformers-4.30.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.3.0\n",
            "    Uninstalling peft-0.3.0:\n",
            "      Successfully uninstalled peft-0.3.0\n",
            "Successfully installed peft-0.15.2 tokenizers-0.21.1 transformers-4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.35.0 accelerate>=0.26.0 peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvRjQli9AK6A",
        "outputId": "4f4ba64e-d942-4cf1-99bc-94a49ce6e1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.5.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load your dataset ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "\n",
        "# --- Check class balance of 'distress' ---\n",
        "print(\"Class balance of 'distress' before sampling:\")\n",
        "print(ground_truth_df['distress'].value_counts(normalize=True))\n",
        "\n",
        "# --- Handle underrepresented classes (if needed - you'll need to analyze the balance) ---\n",
        "# This might involve techniques like oversampling or undersampling.\n",
        "# For now, we'll proceed with the split, but keep this in mind.\n",
        "\n",
        "# --- 1. Create a stratified and balanced sample of 50 based on 'distress' ---\n",
        "sample_size = 50\n",
        "balanced_sample = ground_truth_df.groupby('distress', group_keys=False).apply(lambda x: x.sample(min(len(x), sample_size // ground_truth_df['distress'].nunique()), random_state=42))\n",
        "if len(balanced_sample) < sample_size:\n",
        "    remaining_needed = sample_size - len(balanced_sample)\n",
        "    remaining_sample = ground_truth_df[~ground_truth_df.index.isin(balanced_sample.index)].sample(remaining_needed, random_state=42)\n",
        "    balanced_sample = pd.concat([balanced_sample, remaining_sample])\n",
        "\n",
        "print(f\"\\nSize of the balanced sample: {len(balanced_sample)}\")\n",
        "print(\"Class balance of 'distress' in the balanced sample:\")\n",
        "print(balanced_sample['distress'].value_counts(normalize=True))\n",
        "\n",
        "# --- 2. Remaining data for training, validation, and testing ---\n",
        "remaining_df = ground_truth_df[~ground_truth_df.index.isin(balanced_sample.index)]\n",
        "\n",
        "# --- 3. Split remaining into training (80%), validation (10%), and testing (10%) ---\n",
        "train_df, temp_df = train_test_split(remaining_df, test_size=0.2, stratify=remaining_df['distress'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['distress'], random_state=42)\n",
        "\n",
        "print(f\"\\nSize of Training Set: {len(train_df)}\")\n",
        "print(\"Class balance of 'distress' in Training Set:\")\n",
        "print(train_df['distress'].value_counts(normalize=True))\n",
        "\n",
        "print(f\"\\nSize of Validation Set: {len(val_df)}\")\n",
        "print(\"Class balance of 'distress' in Validation Set:\")\n",
        "print(val_df['distress'].value_counts(normalize=True))\n",
        "\n",
        "print(f\"\\nSize of Testing Set: {len(test_df)}\")\n",
        "print(\"Class balance of 'distress' in Testing Set:\")\n",
        "print(test_df['distress'].value_counts(normalize=True))\n",
        "\n",
        "# --- Save the splits to CSV files ---\n",
        "balanced_sample.to_csv('/content/drive/MyDrive/llama_sample_50.csv', index=False)\n",
        "train_df.to_csv('/content/drive/MyDrive/llama_train.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/llama_val.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/llama_test.csv', index=False)\n",
        "\n",
        "print(\"\\nData splits saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFqfzgir4jRK",
        "outputId": "8ec8bbfc-411c-4f65-8b80-596e7d0e3ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class balance of 'distress' before sampling:\n",
            "distress\n",
            "0    0.890388\n",
            "1    0.109612\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Size of the balanced sample: 50\n",
            "Class balance of 'distress' in the balanced sample:\n",
            "distress\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Size of Training Set: 14425\n",
            "Class balance of 'distress' in Training Set:\n",
            "distress\n",
            "0    0.891438\n",
            "1    0.108562\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Size of Validation Set: 1803\n",
            "Class balance of 'distress' in Validation Set:\n",
            "distress\n",
            "0    0.891847\n",
            "1    0.108153\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Size of Testing Set: 1804\n",
            "Class balance of 'distress' in Testing Set:\n",
            "distress\n",
            "0    0.891353\n",
            "1    0.108647\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-6759c6723115>:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  balanced_sample = ground_truth_df.groupby('distress', group_keys=False).apply(lambda x: x.sample(min(len(x), sample_size // ground_truth_df['distress'].nunique()), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data splits saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load the saved splits ---\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/llama_train.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/llama_val.csv')\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "def prepare_llama_data(data, tokenizer, max_length=512):\n",
        "    tokenized_inputs = []\n",
        "    for index, row in data.iterrows():\n",
        "        tweet = row['tweet_text']\n",
        "        distress_answer = 'distress' if row['distress'] == 1 else 'not distress'\n",
        "        tokenized_inputs.append({\n",
        "            'prompt': f\"You are an emergency detection system... Tweet: {tweet}\",\n",
        "            'target': distress_answer\n",
        "        })\n",
        "    processed_data = tokenizer(\n",
        "        [item['prompt'] for item in tokenized_inputs],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = tokenizer([item['target'] for item in tokenized_inputs], truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\").input_ids\n",
        "    labels[labels == tokenizer.pad_token_id] = -100 # Ignore padding tokens for loss\n",
        "\n",
        "    return {\n",
        "        'input_ids': processed_data['input_ids'],\n",
        "        'attention_mask': processed_data['attention_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "processed_train_data_llama = prepare_llama_data(train_df, tokenizer_llama)\n",
        "processed_val_data_llama = prepare_llama_data(val_df, tokenizer_llama)\n",
        "\n",
        "print(f\"Number of processed training examples: {len(train_df)}\")\n",
        "print(f\"Number of processed validation examples: {len(val_df)}\")\n",
        "print(\"First processed training example keys:\", processed_train_data_llama.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCJEPmAY82MF",
        "outputId": "2107d9b6-4ed5-4dcf-c85c-86c8c2945c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of processed training examples: 14425\n",
            "Number of processed validation examples: 1803\n",
            "First processed training example keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "XP5T_yvJAwPa",
        "outputId": "a2257590-8a3d-4bbe-9224-bce3ed62d611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.30.2 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.30.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "420b92c45ac84e2ba91e060edd9b3203"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "def prepare_llama_data(data, tokenizer, max_length=512):\n",
        "    tokenized_inputs = []\n",
        "    for index, row in data.iterrows():\n",
        "        tweet = row['tweet_text']\n",
        "        distress_answer = 'distress' if row['distress'] == 1 else 'not distress'\n",
        "        tokenized_inputs.append({\n",
        "            'prompt': f\"You are an emergency detection system... Tweet: {tweet}\",\n",
        "            'target': distress_answer\n",
        "        })\n",
        "    processed_data = tokenizer(\n",
        "        [item['prompt'] for item in tokenized_inputs],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = tokenizer([item['target'] for item in tokenized_inputs], truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\").input_ids\n",
        "    labels[labels == tokenizer.pad_token_id] = -100 # Ignore padding tokens for loss\n",
        "\n",
        "    return {\n",
        "        'input_ids': processed_data['input_ids'],\n",
        "        'attention_mask': processed_data['attention_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "processed_train_data_llama = prepare_llama_data(train_df, tokenizer_llama)\n",
        "processed_val_data_llama = prepare_llama_data(val_df, tokenizer_llama)\n",
        "\n",
        "print(f\"Number of processed training examples: {len(train_df)}\")\n",
        "print(f\"Number of processed validation examples: {len(val_df)}\")\n",
        "print(\"First processed training example keys:\", processed_train_data_llama.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bQLYYuWBg-7",
        "outputId": "79570a96-5847-40e6-88fb-b4989608e6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of processed training examples: 14425\n",
            "Number of processed validation examples: 1803\n",
            "First processed training example keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Determine the device to use\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "# Configure 4-bit quantization (optional, but recommended for memory efficiency)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=False,\n",
        ")\n",
        "\n",
        "# Load the pre-trained model with the quantization configuration\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "print(\"LLaMA-2-7b loaded with 4-bit quantization.\")\n",
        "\n",
        "# Define the LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,             # Rank of the LoRA matrices\n",
        "    lora_alpha=32,       # Scaling factor for the LoRA matrices\n",
        "    lora_dropout=0.1,   # Dropout probability for LoRA layers\n",
        "    bias=\"none\",\n",
        "    target_modules=[    # The names of the modules to apply LoRA to\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply LoRA to the base model\n",
        "model_lora = get_peft_model(model_llama, lora_config)\n",
        "\n",
        "model_lora.print_trainable_parameters()\n",
        "\n",
        "print(\"\\nLoRA adapters applied to the model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "c256213a80a34285bcfd5f4f69a11ba8",
            "aa9311b279094058b7cc6d589ad37341",
            "89180d53f4c04a268df6faf96220c914",
            "28755c3b913349399099d57100b19fc8",
            "f2ff0925cb624001ade2951352a0409b",
            "8c97aaa893d14159829c15656cb384f1",
            "e5927c1abea0414180e3294868c1e4c9",
            "2ed14425688b4ce7ad64dafef79d8e24",
            "75d4d68c9a38454aa04e0afc8228cf91",
            "fa900aa073cd48ec905928d07a2c26fe",
            "48d432be093e4dd18963acd6e4790489"
          ]
        },
        "id": "yZ9-QhKG82Re",
        "outputId": "7bdf6d83-a7d3-47ed-e4e3-0a736614c8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c256213a80a34285bcfd5f4f69a11ba8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLaMA-2-7b loaded with 4-bit quantization.\n",
            "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958\n",
            "\n",
            "LoRA adapters applied to the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LlamaDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.n_samples = len(data['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: value[idx] for key, value in self.data.items()}\n",
        "\n",
        "processed_train_dataset = LlamaDataset(processed_train_data_llama)\n",
        "processed_val_dataset = LlamaDataset(processed_val_data_llama)\n",
        "\n",
        "output_dir = \"./llama-2-7b-lora-distress\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4, # Adjust as needed\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True if device == \"cuda\" else False,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\", # Match save strategy to eval strategy\n",
        "    eval_steps=500,      # Evaluate every N steps\n",
        "    evaluation_strategy=\"steps\", # Set evaluation strategy to \"steps\"\n",
        "    save_total_limit=2,\n",
        "    report_to=\"tensorboard\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_lora,\n",
        "    train_dataset=processed_train_dataset, # Use the new Dataset object\n",
        "    eval_dataset=processed_val_dataset,   # Use the new Dataset object\n",
        "    args=training_args,\n",
        "    data_collator=lambda data: {k: torch.stack([f[k] for f in data]) for k in data[0]},\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# --- Save the trained model ---\n",
        "trainer.save_model(\"./llama-2-7b-lora-distress-trained\")\n",
        "print(\"Trained model saved.\")\n",
        "\n",
        "# --- Evaluate on the validation set ---\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation results on validation set:\")\n",
        "print(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "-YhawVqJ82Xo",
        "outputId": "bfe41e34-779a-4e55-98eb-4bde9cc0f975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2703' max='2703' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2703/2703 1:54:32, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.261600</td>\n",
              "      <td>0.276083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.249400</td>\n",
              "      <td>0.269990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.252900</td>\n",
              "      <td>0.244797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.241600</td>\n",
              "      <td>0.245865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.267000</td>\n",
              "      <td>0.244845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/226 01:45]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation results on validation set:\n",
            "{'eval_loss': 0.24479655921459198, 'eval_runtime': 105.6066, 'eval_samples_per_second': 17.073, 'eval_steps_per_second': 2.14, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "lora_model_path = \"./llama-2-7b-lora-distress-trained\" # Or the path to your best checkpoint\n",
        "\n",
        "# Determine the device to use\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the base LLaMA model (with quantization if you used it)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=False,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load the LoRA adapters\n",
        "trained_model_lora = PeftModel.from_pretrained(base_model, lora_model_path)\n",
        "trained_model_lora = trained_model_lora.to(device) # Move to GPU if available\n",
        "\n",
        "# Load the tokenizer (you'll need this for evaluation and inference)\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "print(\"Trained LoRA model loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "6095ab49dc5441f99c248aadfb246f56",
            "c5f1636b3d5a435892c182ddebdd626a",
            "b12ac360c7e344518edd5f8e82ca7ed6",
            "52709daf75d3498c9ed52887e4df3d13",
            "026cf883e22c4e44bc697a688d5bcbf9",
            "6e99926854744734b028e1399d0d253d",
            "74b2c21e65844720b1bc0f3ae93a8ac4",
            "efe69e70a8aa4a72a46b7e8aadd1b3ac",
            "6c1b0869b0ba480f9d7358ff87e3010c",
            "d3f4dab2036448659eea3ba86e6c9078",
            "f580781227ab4cbf886a510fd3272da2"
          ]
        },
        "id": "1H2dMkE1diTz",
        "outputId": "b0e479fd-4011-4db3-fc31-1b168e560533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6095ab49dc5441f99c248aadfb246f56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained LoRA model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of processed training data: {len(processed_train_data_llama['input_ids'])}\")\n",
        "print(f\"Length of processed validation data: {len(processed_val_data_llama['input_ids'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkGHDpdfB8lJ",
        "outputId": "939fb2fc-e485-4509-d22d-bcd789aaa0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of processed training data: 14425\n",
            "Length of processed validation data: 1803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/MyDrive/checkpointfolder\""
      ],
      "metadata": {
        "id": "71HJokdIeIEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# --- Define the source and destination paths ---\n",
        "colab_checkpoint_1500_path = \"./llama-2-7b-lora-distress/checkpoint-1500\"\n",
        "colab_checkpoint_2500_path = \"./llama-2-7b-lora-distress/checkpoint-2500\"\n",
        "drive_checkpoint_root_path = \"/content/drive/MyDrive/llama-lora-checkpoints-saved\" # Choose a root folder in your Drive\n",
        "\n",
        "# --- Create the destination root folder if it doesn't exist ---\n",
        "os.makedirs(drive_checkpoint_root_path, exist_ok=True)\n",
        "\n",
        "drive_checkpoint_1500_path = os.path.join(drive_checkpoint_root_path, \"checkpoint-1500\")\n",
        "drive_checkpoint_2500_path = os.path.join(drive_checkpoint_root_path, \"checkpoint-2500\")\n",
        "\n",
        "# --- Copy checkpoint-1500 ---\n",
        "try:\n",
        "    if os.path.exists(colab_checkpoint_1500_path):\n",
        "        shutil.copytree(colab_checkpoint_1500_path, drive_checkpoint_1500_path)\n",
        "        print(f\"Folder '{colab_checkpoint_1500_path}' copied to '{drive_checkpoint_1500_path}' in your Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Warning: Folder '{colab_checkpoint_1500_path}' not found in the current runtime.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Warning: Folder '{drive_checkpoint_1500_path}' already exists in your Google Drive. Skipping copy.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying '{colab_checkpoint_1500_path}': {e}\")\n",
        "\n",
        "# --- Copy checkpoint-2500 ---\n",
        "try:\n",
        "    if os.path.exists(colab_checkpoint_2500_path):\n",
        "        shutil.copytree(colab_checkpoint_2500_path, drive_checkpoint_2500_path)\n",
        "        print(f\"Folder '{colab_checkpoint_2500_path}' copied to '{drive_checkpoint_2500_path}' in your Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Warning: Folder '{colab_checkpoint_2500_path}' not found in the current runtime.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Warning: Folder '{drive_checkpoint_2500_path}' already exists in your Google Drive. Skipping copy.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying '{colab_checkpoint_2500_path}': {e}\")\n",
        "\n",
        "print(\"\\nAttempted to save checkpoint folders to your Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vur1GnhftQ5",
        "outputId": "e80ab4c6-945d-475b-dcd6-77ed35dc2fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder './llama-2-7b-lora-distress/checkpoint-1500' copied to '/content/drive/MyDrive/llama-lora-checkpoints-saved/checkpoint-1500' in your Google Drive.\n",
            "Folder './llama-2-7b-lora-distress/checkpoint-2500' copied to '/content/drive/MyDrive/llama-lora-checkpoints-saved/checkpoint-2500' in your Google Drive.\n",
            "\n",
            "Attempted to save checkpoint folders to your Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "colab_training_args_path = \"./llama-2-7b-lora-distress-trained/training_args.bin\"\n",
        "drive_output_root_path = \"/content/drive/MyDrive/llama-lora-output-saved\" # The root directory\n",
        "drive_training_args_path = os.path.join(drive_output_root_path, \"training_args.bin\") # Full path to the file\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(drive_output_root_path, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    if os.path.exists(colab_training_args_path):\n",
        "        shutil.copy(colab_training_args_path, drive_training_args_path)\n",
        "        print(f\"Training arguments saved to '{drive_training_args_path}' in your Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Warning: Training arguments file '{colab_training_args_path}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying training arguments: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euqZTB-og01c",
        "outputId": "766f11da-0920-4b91-981e-f031bb48e879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments saved to '/content/drive/MyDrive/llama-lora-output-saved/training_args.bin' in your Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"./llama-2-7b-lora-distress-trained\"\n",
        "destination_dir = \"/content/drive/MyDrive/llama-lora-output-saved\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "files_to_save = [\"README.md\", \"adapter_config.json\", \"adapter_model.safetensors\"]\n",
        "\n",
        "for filename in files_to_save:\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    destination_path = os.path.join(destination_dir, filename)\n",
        "    try:\n",
        "        if os.path.exists(source_path):\n",
        "            shutil.copy(source_path, destination_path)\n",
        "            print(f\"File '{filename}' saved to '{destination_path}' in your Google Drive.\")\n",
        "        else:\n",
        "            print(f\"Warning: File '{filename}' not found in the source directory.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying '{filename}': {e}\")\n",
        "\n",
        "print(\"\\nAttempted to save additional training files to your Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5OfFtUAh059",
        "outputId": "20e807c3-30d3-4469-8c70-6b2ea7dc6915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'README.md' saved to '/content/drive/MyDrive/llama-lora-output-saved/README.md' in your Google Drive.\n",
            "File 'adapter_config.json' saved to '/content/drive/MyDrive/llama-lora-output-saved/adapter_config.json' in your Google Drive.\n",
            "File 'adapter_model.safetensors' saved to '/content/drive/MyDrive/llama-lora-output-saved/adapter_model.safetensors' in your Google Drive.\n",
            "\n",
            "Attempted to save additional training files to your Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First training example:\")\n",
        "for key, value in processed_train_data_llama.items():\n",
        "    print(f\"{key}: {value[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTcNfsDdCAJR",
        "outputId": "76f23ed4-c7f5-48d1-e88a-1337cc8af1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First training example:\n",
            "input_ids: torch.Size([512])\n",
            "attention_mask: torch.Size([512])\n",
            "labels: torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator=lambda data: {k: torch.stack([f[k] for f in data]) for k in data[0]}"
      ],
      "metadata": {
        "id": "IOk8yUHtCAQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Assuming processed_val_data_llama is a dictionary with 'input_ids'\n",
        "if isinstance(processed_val_data_llama, dict) and 'input_ids' in processed_val_data_llama:\n",
        "    # Select a few sample indices from the validation set\n",
        "    sample_indices = [0, 10, 20, 30, 40]  # Adjust indices as needed\n",
        "\n",
        "    sample_tweets_from_val = []\n",
        "    for index in sample_indices:\n",
        "        input_ids = processed_val_data_llama['input_ids'][index]\n",
        "        # Decode the input_ids back to text\n",
        "        tweet_text = tokenizer_llama.decode(input_ids, skip_special_tokens=True)\n",
        "        sample_tweets_from_val.append(tweet_text)\n",
        "else:\n",
        "    print(\"Error: processed_val_data_llama is not in the expected dictionary format.\")\n",
        "    sample_tweets_from_val = []\n",
        "\n",
        "if sample_tweets_from_val:\n",
        "    # Create a pipeline with our fine-tuned model and tokenizer\n",
        "    pipe = pipeline(\"text-generation\", model=trained_model_lora, tokenizer=tokenizer_llama) # Removed device argument\n",
        "\n",
        "    generated_responses = {}\n",
        "\n",
        "    # Instruction 1: Wildfire classifier\n",
        "    instruction1_responses = []\n",
        "    instruction1_prompt = \"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California, not wildfires in general or other locations. Only respond with one word: 'yes' or 'no'. Do not explain. If the tweet references smoke, flames, evacuations, or fire-related events and mentions a California city or region (e.g., LA, San Francisco, Bay Area), respond with 'yes'. Otherwise, respond 'no'. Tweet:\"\n",
        "    for tweet in sample_tweets_from_val:\n",
        "        prompt = f\"{instruction1_prompt} {tweet} Respond with one word: 'yes' or 'no'.\"\n",
        "        output = pipe(prompt, max_length=10, num_return_sequences=1)[0]['generated_text']\n",
        "        instruction1_responses.append(output.split()[-1].lower() if output.split() else \"no\")\n",
        "    generated_responses['wildfire_classifier'] = list(zip(sample_tweets_from_val, instruction1_responses))\n",
        "\n",
        "    # Instruction 2: Emergency detection system (our fine-tuned task)\n",
        "    instruction2_responses = []\n",
        "    instruction2_prompt = \"You are an emergency detection system. Determine if the tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet:\"\n",
        "    for tweet in sample_tweets_from_val:\n",
        "        prompt = f\"{instruction2_prompt} {tweet} Respond with one word: 'distress' or 'not distress'.\"\n",
        "        output = pipe(prompt, max_length=10, num_return_sequences=1)[0]['generated_text']\n",
        "        instruction2_responses.append(output.split()[-1].lower() if output.split() else \"not distress\")\n",
        "    generated_responses['emergency_detection'] = list(zip(sample_tweets_from_val, instruction2_responses))\n",
        "\n",
        "    # Instruction 3: Geographic location extraction\n",
        "    instruction3_responses = []\n",
        "    instruction3_prompt = \"Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet:\"\n",
        "    for tweet in sample_tweets_from_val:\n",
        "        prompt = f\"{instruction3_prompt} {tweet} Respond with only the location name, no extra words.\"\n",
        "        output = pipe(prompt, max_length=20, num_return_sequences=1)[0]['generated_text']\n",
        "        instruction3_responses.append(output.strip() if output else \"unknown\")\n",
        "    generated_responses['location_extraction'] = list(zip(sample_tweets_from_val, instruction3_responses))\n",
        "\n",
        "    # Instruction 4: Disaster response coordinator\n",
        "    instruction4_responses = []\n",
        "    instruction4_prompt = \"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action that responders should take. Choose only one from the following types: evacuation, medical aid, fire suppression, rescue, or resource delivery. If none of these apply or there's no clear threat, respond with 'monitor only'. Respond with only one action. Tweet:\"\n",
        "    possible_actions = [\"evacuation\", \"medical aid\", \"fire suppression\", \"rescue\", \"resource delivery\", \"monitor only\"]\n",
        "    for tweet in sample_tweets_from_val:\n",
        "        prompt = f\"{instruction4_prompt} {tweet} Respond with only one action.\"\n",
        "        output = pipe(prompt, max_length=20, num_return_sequences=1)[0]['generated_text']\n",
        "        predicted_action = \"monitor only\"\n",
        "        for action in possible_actions:\n",
        "            if action in output.lower():\n",
        "                predicted_action = action\n",
        "                break\n",
        "        instruction4_responses.append(predicted_action)\n",
        "    generated_responses['disaster_response'] = list(zip(sample_tweets_from_val, instruction4_responses))\n",
        "\n",
        "    # Print the generated responses\n",
        "    for task, results in generated_responses.items():\n",
        "        print(f\"\\n--- {task} ---\")\n",
        "        for tweet, response in results:\n",
        "            print(f\"Tweet: '{tweet}' -> Response: '{response}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeVO1OgXCAXU",
        "outputId": "0b3a0425-11dd-43d1-fc62-534427ebc0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 148, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 155, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 144, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 139, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 142, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 153, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 160, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 149, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 147, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 111, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 118, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 107, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 102, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 105, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 125, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 132, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 121, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 116, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- wildfire_classifier ---\n",
            "Tweet: 'You are an emergency detection system... Tweet: premium times buhari commiserates with iran iraq over earthquake' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: pets orphaned by hurricane harvey headed to san diego the san diegouniontribune' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: fearful cedants return to market as harvey and irma losses rise' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: life after maria teachers going to save the day' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: looks like a garage fire in allston at california' -> Response: 'dist'\n",
            "\n",
            "--- emergency_detection ---\n",
            "Tweet: 'You are an emergency detection system... Tweet: premium times buhari commiserates with iran iraq over earthquake' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: pets orphaned by hurricane harvey headed to san diego the san diegouniontribune' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: fearful cedants return to market as harvey and irma losses rise' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: life after maria teachers going to save the day' -> Response: 'dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: looks like a garage fire in allston at california' -> Response: 'dist'\n",
            "\n",
            "--- location_extraction ---\n",
            "Tweet: 'You are an emergency detection system... Tweet: premium times buhari commiserates with iran iraq over earthquake' -> Response: 'Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: You are an emergency detection system... Tweet: premium times buhari commiserates with iran iraq over earthquake Respond with only the location name, no extra words. dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: pets orphaned by hurricane harvey headed to san diego the san diegouniontribune' -> Response: 'Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: You are an emergency detection system... Tweet: pets orphaned by hurricane harvey headed to san diego the san diegouniontribune Respond with only the location name, no extra words. dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: fearful cedants return to market as harvey and irma losses rise' -> Response: 'Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: You are an emergency detection system... Tweet: fearful cedants return to market as harvey and irma losses rise Respond with only the location name, no extra words. dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: life after maria teachers going to save the day' -> Response: 'Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: You are an emergency detection system... Tweet: life after maria teachers going to save the day Respond with only the location name, no extra words. dist'\n",
            "Tweet: 'You are an emergency detection system... Tweet: looks like a garage fire in allston at california' -> Response: 'Extract the most specific real-world geographic location mentioned in the tweet that refers to where the California wildfire is happening. This can be a city, neighborhood, street, highway, or region. If no valid place is mentioned, respond with 'unknown'. Respond with only the location name, no extra words. Tweet: You are an emergency detection system... Tweet: looks like a garage fire in allston at california Respond with only the location name, no extra words. dist'\n",
            "\n",
            "--- disaster_response ---\n",
            "Tweet: 'You are an emergency detection system... Tweet: premium times buhari commiserates with iran iraq over earthquake' -> Response: 'evacuation'\n",
            "Tweet: 'You are an emergency detection system... Tweet: pets orphaned by hurricane harvey headed to san diego the san diegouniontribune' -> Response: 'evacuation'\n",
            "Tweet: 'You are an emergency detection system... Tweet: fearful cedants return to market as harvey and irma losses rise' -> Response: 'evacuation'\n",
            "Tweet: 'You are an emergency detection system... Tweet: life after maria teachers going to save the day' -> Response: 'evacuation'\n",
            "Tweet: 'You are an emergency detection system... Tweet: looks like a garage fire in allston at california' -> Response: 'evacuation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 119, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your CSV and force tweet_id to be string\n",
        "df = pd.read_csv('/content/ground_truth_dataset_with_wildfire.csv', dtype={'tweet_id': str})\n",
        "\n",
        "# Check\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl_uFLPEmnx4",
        "outputId": "213ad500-9701-43f3-ca52-ba57e176341b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               tweet_id                  image_id  \\\n",
            "0  9.17791044158185e+17  917791044158185473_0.jpg   \n",
            "1  9.17791130590183e+17  917791130590183424_0.jpg   \n",
            "2  9.17791291823591e+17  917791291823591425_0.jpg   \n",
            "3  9.17791291823591e+17  917791291823591425_1.jpg   \n",
            "4  9.17792092100988e+17  917792092100988929_0.jpg   \n",
            "\n",
            "                                      raw_tweet_text  \\\n",
            "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
            "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
            "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "3  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "4  RT @TIME: California's raging wildfires as you...   \n",
            "\n",
            "                                          tweet_text tweet_hashtags  \\\n",
            "0  wildfires raging through northern california a...            NaN   \n",
            "1         photos deadly wildfires rage in california            NaN   \n",
            "2  pls share were capturing wildfire response rec...            NaN   \n",
            "3  pls share were capturing wildfire response rec...            NaN   \n",
            "4  californias raging wildfires as youve never se...            NaN   \n",
            "\n",
            "                                       image_caption  distress take_action  \\\n",
            "0  a fire is seen burning through the trees in th...         0         NaN   \n",
            "1  two people are standing in a burned area with ...         0         NaN   \n",
            "2         a fire burns in a burned area near a house         0         NaN   \n",
            "3  several people standing in front of a screen w...         0         NaN   \n",
            "4  a night sky with a mountain and a milky in the...         0         NaN   \n",
            "\n",
            "        state sub_location Wildfire  \n",
            "0  California     northern      Yes  \n",
            "1  California          NaN      Yes  \n",
            "2         NaN          NaN       No  \n",
            "3         NaN          NaN       No  \n",
            "4  California          NaN      Yes  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the file normally\n",
        "df = pd.read_csv('/content/ground_truth_dataset_with_wildfire.csv')\n",
        "\n",
        "# Extract the correct tweet_id from the image_id\n",
        "df['tweet_id'] = df['image_id'].apply(lambda x: x.split('_')[0])\n",
        "\n",
        "# Check the first few rows\n",
        "print(df[['tweet_id', 'image_id']].head())\n",
        "\n",
        "# Save the fixed version\n",
        "df.to_csv('/content/ground_truth_dataset_fixed.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dwfpXLfm26q",
        "outputId": "25ec5aa7-bffb-413e-f9b3-5cc73288605b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             tweet_id                  image_id\n",
            "0  917791044158185473  917791044158185473_0.jpg\n",
            "1  917791130590183424  917791130590183424_0.jpg\n",
            "2  917791291823591425  917791291823591425_0.jpg\n",
            "3  917791291823591425  917791291823591425_1.jpg\n",
            "4  917792092100988929  917792092100988929_0.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming val_df is still loaded and accessible\n",
        "if 'val_df' in locals():\n",
        "    sample_indices = [50, 60, 70, 80, 90]\n",
        "    try:\n",
        "        sample_ground_truth = val_df.iloc[sample_indices][['tweet_text', 'distress']]\n",
        "        print(\"\\n--- Ground Truth for Sample Tweets ---\")\n",
        "        for index, row in sample_ground_truth.iterrows():\n",
        "            print(f\"Tweet: '{row['tweet_text']}' -> Distress: {row['distress']}\")\n",
        "\n",
        "        print(\"\\n--- Model Responses ---\")\n",
        "        model_responses = ['dist', 'dist', 'dist', 'dist', 'dist'] # Based on the previous output\n",
        "        for i in range(len(sample_ground_truth)):\n",
        "            print(f\"Tweet: '{sample_ground_truth.iloc[i]['tweet_text']}' -> Model Response: '{model_responses[i]}'\")\n",
        "\n",
        "    except KeyError:\n",
        "        print(\"Error: 'tweet_text' or 'distress' column not found in val_df.\")\n",
        "    except IndexError:\n",
        "        print(\"Error: One or more of the sample indices are out of bounds for val_df.\")\n",
        "else:\n",
        "    print(\"Error: val_df DataFrame is not currently loaded in the environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BptM7Fssm3Bk",
        "outputId": "26de38d3-3c76-4d70-d881-8d5d0a31aa36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Ground Truth for Sample Tweets ---\n",
            "Tweet: 'walleye hold jersey raffle to benefit hurricane victims in puerto rico' -> Distress: 0\n",
            "Tweet: '#money #irma foxbusiness northrop grumman to buy missile maker orbital for 78b fox' -> Distress: 0\n",
            "Tweet: 'what its like to ride out ferocious hurricane maria' -> Distress: 0\n",
            "Tweet: 'can the lessons of harvey save us #equal #time #news' -> Distress: 1\n",
            "Tweet: 'see alert #9 on hurricane maria issued by the bahamas department of meteorology' -> Distress: 0\n",
            "\n",
            "--- Model Responses ---\n",
            "Tweet: 'walleye hold jersey raffle to benefit hurricane victims in puerto rico' -> Model Response: 'dist'\n",
            "Tweet: '#money #irma foxbusiness northrop grumman to buy missile maker orbital for 78b fox' -> Model Response: 'dist'\n",
            "Tweet: 'what its like to ride out ferocious hurricane maria' -> Model Response: 'dist'\n",
            "Tweet: 'can the lessons of harvey save us #equal #time #news' -> Model Response: 'dist'\n",
            "Tweet: 'see alert #9 on hurricane maria issued by the bahamas department of meteorology' -> Model Response: 'dist'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming train_df is loaded\n",
        "if 'train_df' in locals():\n",
        "    distress_counts = train_df['distress'].value_counts()\n",
        "    total_samples = len(train_df)\n",
        "    distress_percentage = (distress_counts[1] / total_samples) * 100 if 1 in distress_counts else 0\n",
        "    not_distress_percentage = (distress_counts[0] / total_samples) * 100 if 0 in distress_counts else 0\n",
        "\n",
        "    print(\"\\n--- Training Data Class Distribution ---\")\n",
        "    print(f\"Total Samples: {total_samples}\")\n",
        "    print(f\"Distress (1) Count: {distress_counts.get(1, 0)}\")\n",
        "    print(f\"Not Distress (0) Count: {distress_counts.get(0, 0)}\")\n",
        "    print(f\"Distress Percentage: {distress_percentage:.2f}%\")\n",
        "    print(f\"Not Distress Percentage: {not_distress_percentage:.2f}%\")\n",
        "else:\n",
        "    print(\"Error: train_df DataFrame is not currently loaded in the environment.\")\n",
        "    print(\"Please run the code that loads your training data (llama_train.csv) first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-wAx9PPm3IM",
        "outputId": "f130738b-8e0c-449b-f1ff-1842439a9061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Data Class Distribution ---\n",
            "Total Samples: 14425\n",
            "Distress (1) Count: 1566\n",
            "Not Distress (0) Count: 12859\n",
            "Distress Percentage: 10.86%\n",
            "Not Distress Percentage: 89.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load the sample dataset\n",
        "try:\n",
        "    sample_df = pd.read_csv(\"/content/drive/MyDrive/llama_sample_50.csv\") # Adjust path if necessary\n",
        "    print(\"\\nSample dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nError: llama_sample_50.csv not found. Please check the path.\")\n",
        "    sample_df = None\n",
        "\n",
        "if sample_df is not None:\n",
        "    def prepare_llama_data_inference(data, tokenizer, max_length=512):\n",
        "        tokenized_inputs = []\n",
        "        for index, row in data.iterrows():\n",
        "            tweet = row['tweet_text']\n",
        "            tokenized_inputs.append({\n",
        "                'prompt': f\"You are an emergency detection system. Tweet: {tweet}\"\n",
        "            })\n",
        "        processed_data = tokenizer(\n",
        "            [item['prompt'] for item in tokenized_inputs],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': processed_data['input_ids'],\n",
        "            'attention_mask': processed_data['attention_mask']\n",
        "        }\n",
        "\n",
        "    processed_sample_data_llama = prepare_llama_data_inference(sample_df, tokenizer_llama)\n",
        "\n",
        "    # Generate predictions directly using the model\n",
        "    predictions = []\n",
        "    for i in range(len(processed_sample_data_llama['input_ids'])):\n",
        "        input_ids = processed_sample_data_llama['input_ids'][i].unsqueeze(0).to(trained_model_lora.device)\n",
        "        attention_mask = processed_sample_data_llama['attention_mask'][i].unsqueeze(0).to(trained_model_lora.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = trained_model_lora.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=10,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer_llama.decode(output[0], skip_special_tokens=True)\n",
        "        predictions.append(generated_text.split()[-1].lower() if generated_text.split() else \"not distress\")\n",
        "\n",
        "    # Display predictions\n",
        "    print(\"\\n--- Predictions on Sample Dataset ---\")\n",
        "    for i, tweet in enumerate(sample_df['tweet_text']):\n",
        "        print(f\"Tweet: '{tweet}' -> Prediction: '{predictions[i]}'\")\n",
        "\n",
        "    # If the sample_df has a 'distress' column, we can evaluate accuracy here\n",
        "    if 'distress' in sample_df.columns:\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        ground_truth = sample_df['distress'].tolist()\n",
        "        # Convert 'distress'/'not distress' predictions to 1/0 for comparison\n",
        "        numerical_predictions = [1 if p == 'distress' or p == 'dist' else 0 for p in predictions]\n",
        "        accuracy = accuracy_score(ground_truth, numerical_predictions)\n",
        "        print(f\"\\nAccuracy on Sample Dataset: {accuracy:.2f}\")\n",
        "    else:\n",
        "        print(\"\\nNote: 'distress' column not found in sample dataset, so accuracy cannot be calculated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBF12Wm4oX-W",
        "outputId": "9315e1e5-4be5-431b-cfe9-da92c194a626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample dataset loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 512, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Predictions on Sample Dataset ---\n",
            "Tweet: 'thieves loot houston home with body of harvey victim still inside' -> Prediction: 'dist'\n",
            "Tweet: 'pay con mora 3' -> Prediction: 'dist'\n",
            "Tweet: 'as hurricane irma strengthens puerto rico could face life threatening flash floods' -> Prediction: 'dist'\n",
            "Tweet: 'dozens of dogs relocated here after irma' -> Prediction: 'dist'\n",
            "Tweet: 'miami real estate mostly spared from irmas fury industry players say #cre' -> Prediction: 'dist'\n",
            "Tweet: 'news telsas solar panels going live in puerto rico #funny #hilarious #lol #pics #fun #meme' -> Prediction: 'dist'\n",
            "Tweet: 'the family by drops at all harvey nichols stores and online at 8am tomorrow #fentybeautyxhn' -> Prediction: 'dist'\n",
            "Tweet: 'no injuries at this plant city mobile home fire home was being rented out by folks who were out of town because of' -> Prediction: 'dist'\n",
            "Tweet: '1 scene of sentinel ingested to hdds for event 201709_earthquake_mex #mexicoearthquake' -> Prediction: 'dist'\n",
            "Tweet: 'november 12 2017 the strong earthquake on iraniraq border killed about 450 people' -> Prediction: 'dist'\n",
            "Tweet: 'red state moochers to puerto rico i got mine your country can drop dead' -> Prediction: 'dist'\n",
            "Tweet: 'do not connect your portable generator into your homes circuits plug appliances directly into the generator #irma' -> Prediction: 'dist'\n",
            "Tweet: 'its not much but i just donated 50 to hurricane maria relief with red cross#godblesspuertorico' -> Prediction: 'dist'\n",
            "Tweet: 'caribbean travel marketplace still on for san juanpuerto rico in jan2018more' -> Prediction: 'dist'\n",
            "Tweet: 'global warming no its something else #hurricaneharvey #hurricaneirma from talk40news commentary' -> Prediction: 'dist'\n",
            "Tweet: 'blog post why caribbean islands went brown and how long will they stay that way #irma n' -> Prediction: 'dist'\n",
            "Tweet: 'where to donate to #mexico earthquake victims #cdmx #sismomexico2017 #sismomx #cdmxsismo' -> Prediction: 'dist'\n",
            "Tweet: 'photos show how wildfires are ravaging parts of californias wine country' -> Prediction: 'dist'\n",
            "Tweet: 'mexicos earthquake is a sobering and graphic reminder of whats in store for la someday' -> Prediction: 'dist'\n",
            "Tweet: 'as hurricane irma loomed embryriddle aeronautical university safely relocated 62 aircraft from florida to alabama' -> Prediction: 'dist'\n",
            "Tweet: 'runoff from irma floods could hold lots of pollution please stay safe out there' -> Prediction: 'dist'\n",
            "Tweet: '#business #member emora construction joined jun 13' -> Prediction: 'dist'\n",
            "Tweet: '#georgia residents affected by #irma may register for assistance #savannah' -> Prediction: 'dist'\n",
            "Tweet: 'arriving to puerto rico with and #prstrong' -> Prediction: 'dist'\n",
            "Tweet: 'the human tragedy of mexicos 1985 earthquake video #latest #news #followback' -> Prediction: 'dist'\n",
            "Tweet: '#feedingbodyandsoul in texas amp florida help out donate now #irmarecovery #harvey' -> Prediction: 'dist'\n",
            "Tweet: 'whats next at voice foundation #lka #floodsl if you wish to help out contact moses on 0772512374' -> Prediction: 'dist'\n",
            "Tweet: 'pudge rodriguez has literally gathered tons of supplies to help puerto ricans impacted by' -> Prediction: 'dist'\n",
            "Tweet: 'how jose could help guide hurricane maria away from us mainland #news' -> Prediction: 'dist'\n",
            "Tweet: 'help hurricane #irma amp #harvey victims shop #v2cigs sept 1824 and well donate 1 per or' -> Prediction: 'dist'\n",
            "Tweet: 'san diegans helping california wildfire victims #sandiego' -> Prediction: 'dist'\n",
            "Tweet: 'farmers affected by harvey #farmstrong #helpforfarmers' -> Prediction: 'dist'\n",
            "Tweet: 'couple stuck on st thomas during hurricane irma finally returns to cny' -> Prediction: 'dist'\n",
            "Tweet: 'donated supplies for puerto rico stuck in tampa storage facility' -> Prediction: 'not'\n",
            "Tweet: '#humanity and to everyone in #iran amp #iraq 5 #earthquake #rescue #help #aid' -> Prediction: 'dist'\n",
            "Tweet: '4 575757 help rebuild puerto ricos sustainable farming after hurricane maria 575757 4' -> Prediction: 'dist'\n",
            "Tweet: 'whats next at voice foundation #lka #floodsl if you wish to help out contact moses on 0772512374' -> Prediction: 'dist'\n",
            "Tweet: 'cowboys 1st amp 10 helping with harvey cutting to 53 zeke leak #bucs' -> Prediction: 'dist'\n",
            "Tweet: 'puerto rico dam is releasing water governor fears it will break #socialmedia' -> Prediction: 'dist'\n",
            "Tweet: 'maria helps the red cross crew at loza in puerto rico f 8 carlos rivera giusti' -> Prediction: 'dist'\n",
            "Tweet: 'brown sabbath offers a helping hand of doom for puerto rico' -> Prediction: 'dist'\n",
            "Tweet: 'dozens rushed to hospitals in #kermanshah after strong #earthquake hit western #iran' -> Prediction: 'dist'\n",
            "Tweet: 'hurricane maria survivor i feared for our lives' -> Prediction: 'dist'\n",
            "Tweet: '3 ways you can help those affected by california wildfires' -> Prediction: 'dist'\n",
            "Tweet: 'ricky martin shares intensely moving photos as he helps rebuild puerto rico' -> Prediction: 'dist'\n",
            "Tweet: 'next on #gmj get help with insurance questions after irma event at prime osborne today from 8a6p' -> Prediction: 'dist'\n",
            "Tweet: 'devastating earthquake in iran has civilians begging for help #ciaravinoecon' -> Prediction: 'dist'\n",
            "Tweet: 'irma evacuation nightmare next time some may not leave #usnews #usrc #usnews' -> Prediction: 'dist'\n",
            "Tweet: 'help us map impacts of hurricane #maria in our latest campaign' -> Prediction: 'dist'\n",
            "Tweet: 'man trapped in flood saved by human chain from the weather channel iphone app' -> Prediction: 'dist'\n",
            "\n",
            "Accuracy on Sample Dataset: 0.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Re-evaluation and Splitting."
      ],
      "metadata": {
        "id": "PRAikO3fptGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "if 'train_df' in locals():\n",
        "    distress_tweets = train_df[train_df['distress'] == 1]['tweet_text'].sample(n=10, random_state=42)\n",
        "    print(\"\\n--- Sample of 'Distress' Tweets from Training Data ---\")\n",
        "    for tweet in distress_tweets:\n",
        "        print(tweet)\n",
        "else:\n",
        "    print(\"Error: train_df not loaded. Please load your training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idttM6vKpsBX",
        "outputId": "fb9f90c1-dc63-4ab4-a702-5550d73f09f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample of 'Distress' Tweets from Training Data ---\n",
            "how the local hispanic community is helping puerto ricorecover\n",
            "here are some ways you can help harvey relief and irma relief\n",
            "army north is proud to be a crucial part of the total army effort to help those affected by #hurricanemaria\n",
            "florida church team who helped in houston returns to help their own city after\n",
            "prayers for all the people affected n hurting due to #hurricaneharvey f\n",
            "crews have restored 1818933 #fl customers who lost power from #irma 99465 outages remain county list\n",
            "back in florida will head home in a couple days missing puerto rico already\n",
            "do good today stop hogan street by from 11am6pm to donate supplies to help puerto rico after hurricane maria\n",
            "florida residents receive mixed messages about hurricane irma evacuations\n",
            "joes can you help us with a rt mexicos earthquake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if 'train_df' in locals():\n",
        "    # Separate distress and not distress tweets\n",
        "    distress_df = train_df[train_df['distress'] == 1]\n",
        "    not_distress_df = train_df[train_df['distress'] == 0]\n",
        "\n",
        "    # Sample 25 from each class\n",
        "    distress_sample = distress_df.sample(n=25, random_state=42)\n",
        "    not_distress_sample = not_distress_df.sample(n=25, random_state=42)\n",
        "\n",
        "    # Combine the balanced sample\n",
        "    balanced_sample_df = pd.concat([distress_sample, not_distress_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n--- Balanced Sample (50 count) ---\")\n",
        "    print(balanced_sample_df['distress'].value_counts())\n",
        "\n",
        "    # Save the balanced sample (optional)\n",
        "    balanced_sample_df.to_csv(\"/content/drive/MyDrive/llama_balanced_sample_50.csv\", index=False)\n",
        "    print(\"\\nBalanced sample saved to /content/drive/MyDrive/llama_balanced_sample_50.csv\")\n",
        "\n",
        "    # Remaining data for splits\n",
        "    remaining_df = train_df[~train_df.index.isin(balanced_sample_df.index)]\n",
        "\n",
        "else:\n",
        "    print(\"Error: train_df not loaded. Please load your training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKPSfHW3oYCs",
        "outputId": "5a50ef1e-dc12-4da6-d970-f4301c5c3512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Balanced Sample (50 count) ---\n",
            "distress\n",
            "1    25\n",
            "0    25\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balanced sample saved to /content/drive/MyDrive/llama_balanced_sample_50.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Balanced Sample (50 count)"
      ],
      "metadata": {
        "id": "KtE59rp6qOg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if 'train_df' in locals():\n",
        "    # Separate distress and not distress tweets\n",
        "    distress_df = train_df[train_df['distress'] == 1]\n",
        "    not_distress_df = train_df[train_df['distress'] == 0]\n",
        "\n",
        "    # Sample 25 from each class\n",
        "    distress_sample = distress_df.sample(n=25, random_state=42)\n",
        "    not_distress_sample = not_distress_df.sample(n=25, random_state=42)\n",
        "\n",
        "    # Combine the balanced sample\n",
        "    balanced_sample_df = pd.concat([distress_sample, not_distress_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n--- Balanced Sample (50 count) ---\")\n",
        "    print(balanced_sample_df['distress'].value_counts())\n",
        "\n",
        "    # Save the balanced sample (optional)\n",
        "    balanced_sample_df.to_csv(\"/content/drive/MyDrive/llama_balanced_sample_50.csv\", index=False)\n",
        "    print(\"\\nBalanced sample saved to /content/drive/MyDrive/llama_balanced_sample_50.csv\")\n",
        "\n",
        "    # Remaining data for splits\n",
        "    remaining_df = train_df[~train_df.index.isin(balanced_sample_df.index)]\n",
        "\n",
        "else:\n",
        "    print(\"Error: train_df not loaded. Please load your training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a50ef1e-dc12-4da6-d970-f4301c5c3512",
        "id": "0DzqahpmqEqg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Balanced Sample (50 count) ---\n",
            "distress\n",
            "1    25\n",
            "0    25\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balanced sample saved to /content/drive/MyDrive/llama_balanced_sample_50.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " split the remaining training data (which excludes these 50 samples) into training (80%), validation (10%), and testing (10%) sets, ensuring that each split maintains a balanced class distribution as much as possible using stratified sampling."
      ],
      "metadata": {
        "id": "SYttJrkhqYkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if 'remaining_df' in locals():\n",
        "    # Perform stratified split for training and a temporary test/val set\n",
        "    train_df_split, temp_test_val_df = train_test_split(\n",
        "        remaining_df,\n",
        "        test_size=0.2,  # 20% for temp test/val\n",
        "        stratify=remaining_df['distress'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Split the temporary test/val set into validation and test sets\n",
        "    val_df_split, test_df_split = train_test_split(\n",
        "        temp_test_val_df,\n",
        "        test_size=0.5,  # 50% of temp is 10% of total\n",
        "        stratify=temp_test_val_df['distress'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Training Set Split ---\")\n",
        "    print(train_df_split['distress'].value_counts(normalize=True))\n",
        "\n",
        "    print(\"\\n--- Validation Set Split ---\")\n",
        "    print(val_df_split['distress'].value_counts(normalize=True))\n",
        "\n",
        "    print(\"\\n--- Testing Set Split ---\")\n",
        "    print(test_df_split['distress'].value_counts(normalize=True))\n",
        "\n",
        "    # Save the splits (optional)\n",
        "    train_df_split.to_csv(\"/content/drive/MyDrive/llama_train_balanced.csv\", index=False)\n",
        "    val_df_split.to_csv(\"/content/drive/MyDrive/llama_val_balanced.csv\", index=False)\n",
        "    test_df_split.to_csv(\"/content/drive/MyDrive/llama_test_balanced.csv\", index=False)\n",
        "    print(\"\\nBalanced train, validation, and test sets saved to /content/drive/MyDrive/\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: remaining_df not found. Please run the previous step.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyJNXAQZoYHe",
        "outputId": "b76a2751-0acc-4051-d8f3-7cf5011f7b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Set Split ---\n",
            "distress\n",
            "0    0.891565\n",
            "1    0.108435\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Validation Set Split ---\n",
            "distress\n",
            "0    0.891441\n",
            "1    0.108559\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "--- Testing Set Split ---\n",
            "distress\n",
            "0    0.891516\n",
            "1    0.108484\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Balanced train, validation, and test sets saved to /content/drive/MyDrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that the 'distress' class has significantly fewer samples, oversampling (or a combination) is generally preferred to avoid losing potentially valuable information from the majority class.\n",
        "\n",
        "Let's proceed with oversampling the 'distress' class in our train_df_split to create a balanced training set for the re-training phase."
      ],
      "metadata": {
        "id": "RL_A8PNrqo_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the balanced training split\n",
        "try:\n",
        "    train_df_balanced = pd.read_csv(\"/content/drive/MyDrive/llama_train_balanced.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: llama_train_balanced.csv not found. Please ensure the previous step was run.\")\n",
        "    train_df_balanced = None\n",
        "\n",
        "if train_df_balanced is not None:\n",
        "    # Separate majority and minority classes\n",
        "    distress_minority = train_df_balanced[train_df_balanced['distress'] == 1]\n",
        "    not_distress_majority = train_df_balanced[train_df_balanced['distress'] == 0]\n",
        "\n",
        "    # Oversample minority class\n",
        "    distress_oversampled = resample(\n",
        "        distress_minority,\n",
        "        replace=True,  # sample with replacement\n",
        "        n_samples=len(not_distress_majority),  # match majority class\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Combine majority class with oversampled minority class\n",
        "    train_df_oversampled = pd.concat([not_distress_majority, distress_oversampled])\n",
        "\n",
        "    # Shuffle the oversampled training data\n",
        "    train_df_oversampled = train_df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n--- Oversampled Training Set Distribution ---\")\n",
        "    print(train_df_oversampled['distress'].value_counts())\n",
        "\n",
        "    # Save the oversampled training set (optional)\n",
        "    train_df_oversampled.to_csv(\"/content/drive/MyDrive/llama_train_oversampled.csv\", index=False)\n",
        "    print(\"\\nOversampled training set saved to /content/drive/MyDrive/llama_train_oversampled.csv\")\n",
        "\n",
        "else:\n",
        "    print(\"Error loading balanced training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBc4hoeKoYLr",
        "outputId": "6474658f-7f39-48c4-8d53-964dbb1625f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Oversampled Training Set Distribution ---\n",
            "distress\n",
            "1    10253\n",
            "0    10253\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Oversampled training set saved to /content/drive/MyDrive/llama_train_oversampled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2: Model Re-training."
      ],
      "metadata": {
        "id": "NK93EWnTqwTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# --- Define paths and model names ---\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "output_dir = \"/content/drive/MyDrive/llama-2-7b-lora-distress-balanced\"\n",
        "\n",
        "# --- Determine device ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "# --- Load base LLaMA model with quantization ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=False,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# --- Configure LoRA with dropout ---\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,  # Added dropout\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# --- Add LoRA adapters to the base model ---\n",
        "model_lora_balanced = get_peft_model(base_model, lora_config)\n",
        "model_lora_balanced.print_trainable_parameters()\n",
        "\n",
        "# --- Freeze most base model layers ---\n",
        "for name, param in model_lora_balanced.named_parameters():\n",
        "    if \"lora_\" not in name:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "fd012cecca024051ab433bb46813c0ab",
            "b71591e9ab1b4f78a0f2f1153af47f71",
            "9ef52d0c6b6743fbb52ecfc559920f44",
            "d79aa50ccc91439697fa5294cae2721b",
            "03d1206b681f49e29bdf0a0d67ab3850",
            "63b1c1c5ed5549a59fd4ff66d5625281",
            "2b530f0f299747fb8017a5b8b8a10035",
            "cb8a4274ebb54d87a0ffa1d0be38b0f8",
            "3167ce4b91194703aabe148d63659159",
            "ef68e5f7c6764f98972c5048713f5a09",
            "808fee9b53d84a34889772e42e754253"
          ]
        },
        "id": "rF8RSKf6qxTG",
        "outputId": "1aefc449-6788-459a-d4c3-bbf81b4ca5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd012cecca024051ab433bb46813c0ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'train_df_oversampled' in locals() and 'tokenizer_llama' in locals():\n",
        "    def prepare_llama_data_debug(data, tokenizer, max_length=512):\n",
        "        tokenized_inputs = []\n",
        "        labels_list = []\n",
        "        for index, row in data.iterrows():\n",
        "            tweet = row['tweet_text']\n",
        "            distress_answer = 1 if row['distress'] == 1 else 0\n",
        "            tokenized_inputs.append({\n",
        "                'prompt': f\"You are an emergency detection system. Tweet: {tweet}\"\n",
        "            })\n",
        "            labels_list.append(distress_answer)\n",
        "\n",
        "        processed_data = tokenizer(\n",
        "            [item['prompt'] for item in tokenized_inputs],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        labels = torch.tensor(labels_list)\n",
        "\n",
        "        return {\n",
        "            'input_ids': processed_data['input_ids'],\n",
        "            'attention_mask': processed_data['attention_mask'],\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    processed_train_data_debug = prepare_llama_data_debug(train_df_oversampled.head(), tokenizer_llama)\n",
        "    print(\"\\n--- Debug Output of processed_train_data_debug (First 5 samples) ---\")\n",
        "    for key, value in processed_train_data_debug.items():\n",
        "        print(f\"{key}: {value.shape} \\n{value[:2]}\\n\")\n",
        "\n",
        "    if 'processed_val_data_llama' in locals():\n",
        "        print(\"\\n--- Debug Output of processed_val_data_llama (First 2 samples) ---\")\n",
        "        for key, value in processed_val_data_llama.items():\n",
        "            print(f\"{key}: {value.shape} \\n{value[:2]}\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"Error: train_df_oversampled or tokenizer_llama not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHuJZvothXq",
        "outputId": "93ce0e9b-f437-4f8d-935b-cc0c6a7f542b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debug Output of processed_train_data_debug (First 5 samples) ---\n",
            "input_ids: torch.Size([5, 512]) \n",
            "tensor([[  1, 887, 526,  ...,   2,   2,   2],\n",
            "        [  1, 887, 526,  ...,   2,   2,   2]])\n",
            "\n",
            "attention_mask: torch.Size([5, 512]) \n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "\n",
            "labels: torch.Size([5]) \n",
            "tensor([1, 0])\n",
            "\n",
            "\n",
            "--- Debug Output of processed_val_data_llama (First 2 samples) ---\n",
            "input_ids: torch.Size([1437, 512]) \n",
            "tensor([[  1, 887, 526,  ...,   2,   2,   2],\n",
            "        [  1, 887, 526,  ...,   2,   2,   2]])\n",
            "\n",
            "attention_mask: torch.Size([1437, 512]) \n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "\n",
            "labels: torch.Size([0]) \n",
            "tensor([])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'val_df_split' in locals():\n",
        "    print(\"\\n--- First 5 rows of val_df_split ---\")\n",
        "    print(val_df_split.head())\n",
        "    print(\"\\n--- 'distress' column value counts in val_df_split ---\")\n",
        "    print(val_df_split['distress'].value_counts())\n",
        "else:\n",
        "    print(\"Error: val_df_split not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qr_TR3rtssM",
        "outputId": "182634d9-a939-460b-e7f6-40ca76999a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First 5 rows of val_df_split ---\n",
            "       tweet_id                  image_id  \\\n",
            "0  9.077799e+17  907779904254734341_0.jpg   \n",
            "1  9.234752e+17  923475169800200193_0.jpg   \n",
            "2  9.202494e+17  920249445979181056_0.jpg   \n",
            "3  9.117325e+17  911732538950873088_0.jpg   \n",
            "4  9.098030e+17  909802988532912132_0.jpg   \n",
            "\n",
            "                                      raw_tweet_text  \\\n",
            "0  Possible tornado destroys condo buildings in C...   \n",
            "1  How St. Joseph And The Blessed Virgin Mary Sav...   \n",
            "2  California wildfires hit home for Bills rookie...   \n",
            "3  Magnitude 6.1 aftershock hits Mexico as search...   \n",
            "4  RT @CNTraveler: During Hurricane Irma, This Fl...   \n",
            "\n",
            "                                          tweet_text tweet_hashtags  \\\n",
            "0  possible tornado destroys condo buildings in c...            NaN   \n",
            "1  how st joseph and the blessed virgin mary save...            NaN   \n",
            "2  california wildfires hit home for bills rookie...          bills   \n",
            "3  magnitude 61 aftershock hits mexico as search ...            NaN   \n",
            "4  during hurricane irma this florida hotel accom...            NaN   \n",
            "\n",
            "                                       image_caption  distress take_action  \\\n",
            "0                                    Image not found         0         NaN   \n",
            "1  there is a statue of a person standing in a field         0         NaN   \n",
            "2  firefighters are fighting a fire in the woods ...         0         NaN   \n",
            "3                                    Image not found         0         NaN   \n",
            "4                                    Image not found         0         NaN   \n",
            "\n",
            "        state    sub_location Wildfire  \n",
            "0         NaN  crescent beach       No  \n",
            "1  California             NaN      Yes  \n",
            "2  California             NaN      Yes  \n",
            "3         NaN          mexico       No  \n",
            "4     Florida         florida       No  \n",
            "\n",
            "--- 'distress' column value counts in val_df_split ---\n",
            "distress\n",
            "0    1281\n",
            "1     156\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " load the oversampled training data and the stratified validation data, and then configure the Trainer with early stopping and model saving."
      ],
      "metadata": {
        "id": "fRFTY-ybrChv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "training on 50 samples balanced dataset"
      ],
      "metadata": {
        "id": "J3krTWHVxp2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- Define paths and model names ---\n",
        "model_name = \"gpt2\"\n",
        "output_dir = \"/content/drive/MyDrive/gpt2-lora-distress-sample\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "\n",
        "# --- Load base GPT-2 model ---\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# --- Prepare data for the 50-tweet sample ---\n",
        "try:\n",
        "    balanced_sample_df = pd.read_csv(\"/content/drive/MyDrive/llama_balanced_sample_50.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: llama_balanced_sample_50.csv not found.\")\n",
        "    balanced_sample_df = None\n",
        "\n",
        "if balanced_sample_df is not None:\n",
        "    def prepare_data_for_dataloader_sample(data, tokenizer, max_length=128): # Reduced max_length\n",
        "        inputs = []\n",
        "        labels = []\n",
        "        for index, row in data.iterrows():\n",
        "            tweet = row['tweet_text']\n",
        "            distress_answer = 'distress' if row['distress'] == 1 else 'not distress'\n",
        "            prompt = f\"You are an emergency detection system. Tweet: {tweet}\"\n",
        "            tokenized_prompt = tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "            tokenized_target = tokenizer(distress_answer, truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "            inputs.append({\n",
        "                'input_ids': tokenized_prompt['input_ids'].squeeze(0),\n",
        "                'attention_mask': tokenized_prompt['attention_mask'].squeeze(0)\n",
        "            })\n",
        "            labels.append(tokenized_target['input_ids'].squeeze(0))\n",
        "        return inputs, labels\n",
        "\n",
        "    sample_inputs, sample_labels = prepare_data_for_dataloader_sample(balanced_sample_df, tokenizer_gpt2)\n",
        "\n",
        "    # --- Create DataLoader for the sample ---\n",
        "    sample_dataset = TensorDataset(torch.stack([item['input_ids'] for item in sample_inputs]), torch.stack([item['attention_mask'] for item in sample_inputs]), torch.stack(sample_labels))\n",
        "    sample_dataloader = DataLoader(sample_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # --- Set up Optimizer and Loss ---\n",
        "    optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-4)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # --- Manual Training Loop on the sample ---\n",
        "    num_epochs = 5  # Train for a few epochs on the small sample\n",
        "    base_model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        for batch in sample_dataloader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            optimizer.zero_grad()\n",
        "            outputs = base_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Training Loss: {loss.item():.4f}\", end='\\r')\n",
        "\n",
        "    # --- Save the (sample-trained) model ---\n",
        "    base_model.save_pretrained(output_dir)\n",
        "    print(f\"\\n--- Trained (on sample) model saved to {output_dir} ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Error loading the balanced sample data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHNzUpPPxpKI",
        "outputId": "44136d12-43cf-47de-819c-ce6a3159c584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training Loss: 0.0100\n",
            "Epoch 2\n",
            "Training Loss: 0.0381\n",
            "Epoch 3\n",
            "Training Loss: 0.0061\n",
            "Epoch 4\n",
            "Training Loss: 0.0061\n",
            "Epoch 5\n",
            "\n",
            "--- Trained (on sample) model saved to /content/drive/MyDrive/gpt2-lora-distress-sample ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# --- Define paths and model names ---\n",
        "model_name = \"gpt2\"\n",
        "output_dir = \"/content/drive/MyDrive/gpt2-sequence-classifier-manual\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_labels = 2 # 'distress' or 'not distress'\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "tokenizer_gpt2.padding_side = \"right\"\n",
        "\n",
        "# --- Load GPT-2 model for sequence classification ---\n",
        "model_classifier = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)\n",
        "model_classifier.config.pad_token_id = tokenizer_gpt2.pad_token_id # Explicitly set pad_token_id in config\n",
        "model_classifier.train()\n",
        "\n",
        "# --- Load the training data ---\n",
        "try:\n",
        "    train_df = pd.read_csv(\"/content/drive/MyDrive/llama_train.csv\") # Replace with your actual path if different\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: llama_train.csv not found.\")\n",
        "    train_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # --- Create a balanced sample of 200 tweets for initial training ---\n",
        "    distress_df = train_df[train_df['distress'] == 1]\n",
        "    not_distress_df = train_df[train_df['distress'] == 0]\n",
        "    distress_sample_200 = resample(distress_df, replace=False, n_samples=100, random_state=42)\n",
        "    not_distress_sample_200 = resample(not_distress_df, replace=False, n_samples=100, random_state=42)\n",
        "    balanced_sample_200_df = pd.concat([distress_sample_200, not_distress_sample_200]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # --- Prepare dataset for DataLoader ---\n",
        "    class SimpleDatasetManual(torch.utils.data.Dataset):\n",
        "        def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "            self.data = dataframe\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.data.iloc[idx]\n",
        "            tweet = row['tweet_text']\n",
        "            label = int(row['distress'])\n",
        "            inputs = self.tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "            return {\n",
        "                'input_ids': inputs['input_ids'].squeeze(),\n",
        "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(label)\n",
        "            }\n",
        "\n",
        "    train_dataset_manual = SimpleDatasetManual(balanced_sample_200_df, tokenizer_gpt2, max_length=128)\n",
        "    train_dataloader_manual = DataLoader(train_dataset_manual, batch_size=4, shuffle=True)\n",
        "\n",
        "    # --- Set up Optimizer and Loss ---\n",
        "    optimizer = torch.optim.AdamW(model_classifier.parameters(), lr=2e-5)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # --- Manual Training Loop ---\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        for batch in train_dataloader_manual:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Training Loss: {loss.item():.4f}\", end='\\r')\n",
        "\n",
        "    print(\"\\n--- Manual Training Loop Finished ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Error loading training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rPb7DkUzAXH",
        "outputId": "52c97043-63c2-4eac-f413-59a74cdab867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training Loss: 0.6255\n",
            "Epoch 2\n",
            "Training Loss: 0.3232\n",
            "Epoch 3\n",
            "Training Loss: 0.3809\n",
            "Epoch 4\n",
            "Training Loss: 0.2059\n",
            "Epoch 5\n",
            "Training Loss: 0.0426\n",
            "--- Manual Training Loop Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# --- Define paths and model names ---\n",
        "model_name = \"gpt2\"\n",
        "output_dir = \"/content/drive/MyDrive/gpt2-sequence-classifier-manual\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_labels = 2 # 'distress' or 'not distress'\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "tokenizer_gpt2.padding_side = \"right\"\n",
        "\n",
        "# --- Load GPT-2 model for sequence classification ---\n",
        "model_classifier = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)\n",
        "model_classifier.config.pad_token_id = tokenizer_gpt2.pad_token_id\n",
        "model_classifier.train()\n",
        "\n",
        "# --- Load the training data ---\n",
        "try:\n",
        "    train_df = pd.read_csv(\"/content/drive/MyDrive/llama_train.csv\") # Replace with your actual path if different\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: llama_train.csv not found.\")\n",
        "    train_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # --- Create a balanced sample of 200 tweets for training and evaluation ---\n",
        "    distress_df = train_df[train_df['distress'] == 1]\n",
        "    not_distress_df = train_df[train_df['distress'] == 0]\n",
        "    distress_sample_200 = resample(distress_df, replace=False, n_samples=100, random_state=42)\n",
        "    not_distress_sample_200 = resample(not_distress_df, replace=False, n_samples=100, random_state=42)\n",
        "    balanced_sample_200_df = pd.concat([distress_sample_200, not_distress_sample_200]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # --- Prepare dataset for DataLoader ---\n",
        "    class SimpleDatasetManual(torch.utils.data.Dataset):\n",
        "        def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "            self.data = dataframe\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.data.iloc[idx]\n",
        "            tweet = row['tweet_text']\n",
        "            label = int(row['distress'])\n",
        "            inputs = self.tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "            return {\n",
        "                'input_ids': inputs['input_ids'].squeeze(),\n",
        "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(label)\n",
        "            }\n",
        "\n",
        "    train_dataset_manual = SimpleDatasetManual(balanced_sample_200_df, tokenizer_gpt2, max_length=128)\n",
        "    train_dataloader_manual = DataLoader(train_dataset_manual, batch_size=4, shuffle=True)\n",
        "\n",
        "    # --- Set up Optimizer and Loss ---\n",
        "    optimizer = torch.optim.AdamW(model_classifier.parameters(), lr=2e-5)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # --- Manual Training Loop ---\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        for batch in train_dataloader_manual:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Training Loss: {loss.item():.4f}\", end='\\r')\n",
        "\n",
        "    print(\"\\n--- Manual Training Loop Finished ---\")\n",
        "\n",
        "    # --- Save the trained model and tokenizer ---\n",
        "    model_classifier.save_pretrained(output_dir)\n",
        "    tokenizer_gpt2.save_pretrained(output_dir)\n",
        "    print(f\"\\n--- Trained model and tokenizer saved to {output_dir} ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Error loading training data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSJBSiTUzAjp",
        "outputId": "833116d3-3a16-44fb-a347-02859884b173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training Loss: 0.5498\n",
            "Epoch 2\n",
            "Training Loss: 0.4416\n",
            "Epoch 3\n",
            "Training Loss: 0.2102\n",
            "Epoch 4\n",
            "Training Loss: 0.5181\n",
            "Epoch 5\n",
            "Training Loss: 0.0983\n",
            "--- Manual Training Loop Finished ---\n",
            "\n",
            "--- Trained model and tokenizer saved to /content/drive/MyDrive/gpt2-sequence-classifier-manual ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Prepare the 200-tweet balanced sample for evaluation ---\n",
        "class EvalTrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        tweet = row['tweet_text']\n",
        "        label = int(row['distress'])\n",
        "        inputs = self.tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze().to(device),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze().to(device),\n",
        "            'labels': torch.tensor(label).to(device)\n",
        "        }\n",
        "\n",
        "eval_train_dataset = EvalTrainDataset(balanced_sample_200_df, tokenizer_gpt2, max_length=128)\n",
        "eval_train_dataloader = DataLoader(eval_train_dataset, batch_size=4)\n",
        "\n",
        "# --- Evaluate the model on the training sample ---\n",
        "model_classifier.eval()\n",
        "train_predictions = []\n",
        "train_actual_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in eval_train_dataloader:\n",
        "        outputs = model_classifier(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        logits = outputs.logits\n",
        "        predicted_labels = torch.argmax(logits, dim=-1)\n",
        "        train_predictions.extend(predicted_labels.cpu().numpy())\n",
        "        train_actual_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "print(\"\\n--- Evaluation on 200-Tweet Training Sample ---\")\n",
        "print(classification_report(train_actual_labels, train_predictions))\n",
        "print(f\"Accuracy: {accuracy_score(train_actual_labels, train_predictions)}\")\n",
        "print(f\"F1-Score: {f1_score(train_actual_labels, train_predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kbxRIU_4yQg",
        "outputId": "08aa969d-71ec-4368-aace-af54d9cbd1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation on 200-Tweet Training Sample ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98       100\n",
            "           1       0.97      1.00      0.99       100\n",
            "\n",
            "    accuracy                           0.98       200\n",
            "   macro avg       0.99      0.98      0.98       200\n",
            "weighted avg       0.99      0.98      0.98       200\n",
            "\n",
            "Accuracy: 0.985\n",
            "F1-Score: 0.9852216748768473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluating on test set of 200 balanced samples"
      ],
      "metadata": {
        "id": "jgc7ATe32jiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# --- Define paths and model names ---\n",
        "model_name = \"gpt2\"\n",
        "output_dir = \"/content/drive/MyDrive/gpt2-sequence-classifier-manual\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_labels = 2\n",
        "\n",
        "# --- Load tokenizer ---\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "tokenizer_gpt2.padding_side = \"right\"\n",
        "\n",
        "# --- Load the fine-tuned GPT-2 model ---\n",
        "config = AutoConfig.from_pretrained(output_dir)\n",
        "model_eval = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=output_dir, config=config).to(device)\n",
        "model_eval.eval()\n",
        "\n",
        "# --- Load the test data ---\n",
        "try:\n",
        "    test_df = pd.read_csv(\"/content/drive/MyDrive/llama_test_balanced.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: llama_test_balanced.csv not found.\")\n",
        "    test_df = None\n",
        "\n",
        "\n",
        "if test_df is not None:\n",
        "    # --- Prepare dataset for evaluation ---\n",
        "    class EvalDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "            self.data = dataframe\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.data.iloc[idx]\n",
        "            tweet = row['tweet_text']\n",
        "            label = int(row['distress'])\n",
        "            inputs = self.tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "            return {\n",
        "                'input_ids': inputs['input_ids'].squeeze(),\n",
        "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(label)\n",
        "            }\n",
        "\n",
        "    eval_dataset = EvalDataset(test_df, tokenizer_gpt2, max_length=128)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=4)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model_eval(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"\\n--- Evaluation on Held-Out Test Set ---\")\n",
        "    print(classification_report(actual_labels, predictions))\n",
        "    print(f\"Accuracy: {accuracy_score(actual_labels, predictions)}\")\n",
        "    print(f\"F1-Score: {f1_score(actual_labels, predictions)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Error loading test data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eup3w9E2in4",
        "outputId": "9cb19cd7-70f5-4a82-c42a-82836f2daac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation on Held-Out Test Set ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.73      0.83      1282\n",
            "           1       0.26      0.78      0.39       156\n",
            "\n",
            "    accuracy                           0.73      1438\n",
            "   macro avg       0.61      0.75      0.61      1438\n",
            "weighted avg       0.89      0.73      0.78      1438\n",
            "\n",
            "Accuracy: 0.7336578581363005\n",
            "F1-Score: 0.38915470494417864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save the trained model and tokenizer (again, for safety) ---\n",
        "model_eval.save_pretrained(\"/content/drive/MyDrive/gpt2-sequence-classifier-manual\")\n",
        "tokenizer_gpt2.save_pretrained(\"/content/drive/MyDrive/gpt2-sequence-classifier-manual\")\n",
        "\n",
        "print(\"\\n--- Model and tokenizer saved to /content/drive/MyDrive/gpt2-sequence-classifier-manual ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GCn-Lxm2iup",
        "outputId": "54865cc3-4b0f-4115-cbc9-86403ee0d014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model and tokenizer saved to /content/drive/MyDrive/gpt2-sequence-classifier-manual ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_summary = \"\"\"\n",
        "--- Evaluation on Held-Out Test Set (GPT-2 on 200-tweet training sample) ---\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.96      0.73      0.83      1282\n",
        "           1       0.26      0.78      0.39       156\n",
        "\n",
        "Accuracy: 0.7337\n",
        "F1-Score (Class 1 - Distress): 0.3892\n",
        "\"\"\"\n",
        "\n",
        "output_file_path = \"/content/drive/MyDrive/gpt2_distress_classification_results.txt\"\n",
        "\n",
        "with open(output_file_path, \"w\") as f:\n",
        "    f.write(evaluation_summary)\n",
        "\n",
        "print(f\"\\n--- Evaluation results saved to {output_file_path} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gy-B0s82i12",
        "outputId": "c9a73652-e6cc-452b-cc41-f49e12849d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation results saved to /content/drive/MyDrive/gpt2_distress_classification_results.txt ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "\n",
        "instruction_response_pairs = []\n",
        "for index, row in balanced_sample_200_df.iterrows():\n",
        "    tweet_text = row['tweet_text']\n",
        "    distress_label = row['distress']\n",
        "    instruction_text = instruction + tweet_text\n",
        "    response = \"distress\" if distress_label == 1 else \"not distress\"\n",
        "    instruction_response_pairs.append({\"instruction\": instruction_text, \"response\": response})\n",
        "\n",
        "# Print the first few examples to verify\n",
        "for i in range(5):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Instruction: {instruction_response_pairs[i]['instruction']}\")\n",
        "    print(f\"Response: {instruction_response_pairs[i]['response']}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRGDB41468cy",
        "outputId": "1cb35923-2583-429e-8081-687371ed8710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: elementary schools and food pantry collaborate to help puerto rico\n",
            "Response: distress\n",
            "--------------------\n",
            "Example 2:\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: jewish volunteers find body of missing rabbi in mexico earthquake rubble\n",
            "Response: distress\n",
            "--------------------\n",
            "Example 3:\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: coming up at 6 the latest on the deadly earthquake in #mexico at least 225 lost their lives\n",
            "Response: distress\n",
            "--------------------\n",
            "Example 4:\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: last tuesday a day after hurricane irma struck central florida a curious bobcat kitten sat observing on wheeler\n",
            "Response: not distress\n",
            "--------------------\n",
            "Example 5:\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: took in over 250 dogs today in texas donate at\n",
            "Response: not distress\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "\n",
        "def tokenize_instruction_response(pair):\n",
        "    instruction = pair['instruction']\n",
        "    response = pair['response']\n",
        "    tokenized_instruction = tokenizer_gpt2(instruction, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    tokenized_response = tokenizer_gpt2(response, truncation=True, padding='max_length', max_length=16, return_tensors='pt') # Shorter max length for response\n",
        "    return {\n",
        "        'input_ids': tokenized_instruction['input_ids'].squeeze(),\n",
        "        'attention_mask': tokenized_instruction['attention_mask'].squeeze(),\n",
        "        'labels': tokenized_response['input_ids'].squeeze()\n",
        "    }\n",
        "\n",
        "tokenized_data = [tokenize_instruction_response(pair) for pair in instruction_response_pairs]\n",
        "\n",
        "# Print the first tokenized example to verify\n",
        "print(\"First Tokenized Example:\")\n",
        "print(\"Input IDs:\", tokenized_data[0]['input_ids'])\n",
        "print(\"Attention Mask:\", tokenized_data[0]['attention_mask'])\n",
        "print(\"Labels:\", tokenized_data[0]['labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du86kPjf68fd",
        "outputId": "480ea748-fb2f-4acc-b75a-e11ce2f7fef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Tokenized Example:\n",
            "Input IDs: tensor([ 1639,   389,   281,  6334, 13326,  1080,    13, 45559,  3810,   611,\n",
            "          262,  1708,  6126,  4084,  9217,   326,  2130,   318,   287,  3514,\n",
            "           11, 13640,    11,  6686,    11, 20623,  1037,    11,   393,  6476,\n",
            "          257,  1204,    12, 26159,  6334,    13,  5514,  3031,   351,   530,\n",
            "         1573,    25,   705, 17080,   601,     6,   393,   705,  1662, 17087,\n",
            "         4458,  2141,   407,  4727,    13, 24205,  1039,   351,  7016,  8216,\n",
            "          389,   407,  1576,   532,   691,  3853,   705, 17080,   601,     6,\n",
            "          611,   612,   338,   257,  1598,    11, 18039,   761,   329,  1037,\n",
            "           13,  1002, 22147,    11,  4277,   284,   705,  1662, 17087,  4458,\n",
            "        18752,    25, 19823,  4266,   290,  2057, 15857,   563, 30081,   284,\n",
            "         1037, 47574, 13806,   374,  3713, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Labels: tensor([17080,   601, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "        50256, 50256, 50256, 50256, 50256, 50256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class InstructionResponseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.tokenized_data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokenized_data[idx]\n",
        "\n",
        "instruction_response_dataset = InstructionResponseDataset(tokenized_data)\n",
        "train_dataloader = torch.utils.data.DataLoader(instruction_response_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Verify the dataloader\n",
        "first_batch = next(iter(train_dataloader))\n",
        "print(\"\\nFirst Batch from DataLoader:\")\n",
        "print(\"Input IDs shape:\", first_batch['input_ids'].shape)\n",
        "print(\"Attention Mask shape:\", first_batch['attention_mask'].shape)\n",
        "print(\"Labels shape:\", first_batch['labels'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7x6iHvk68jF",
        "outputId": "92d8fb78-dd41-48af-a770-04bff333b8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First Batch from DataLoader:\n",
            "Input IDs shape: torch.Size([4, 128])\n",
            "Attention Mask shape: torch.Size([4, 128])\n",
            "Labels shape: torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model_causal = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model_causal.resize_token_embeddings(len(tokenizer_gpt2)) # Ensure proper embedding size\n",
        "\n",
        "optimizer = torch.optim.AdamW(model_causal.parameters(), lr=5e-5) # You might need to experiment with the learning rate\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Freeze the base layers (optional, but can save memory and speed up training initially)\n",
        "for name, param in model_causal.named_parameters():\n",
        "    if \"ln_\" not in name and \"wpe\" not in name and \"wte\" not in name and \"score\" not in name:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "mcKWLZj47dx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "\n",
        "def tokenize_instruction_response_for_causal_lm(pair):\n",
        "    instruction = pair['instruction']\n",
        "    response = pair['response']\n",
        "    combined_text = instruction + \" \" + response + tokenizer_gpt2.eos_token\n",
        "    tokenized_input = tokenizer_gpt2(combined_text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "    input_ids = tokenized_input['input_ids'].squeeze()\n",
        "    attention_mask = tokenized_input['attention_mask'].squeeze()\n",
        "    labels = input_ids.clone() # Initially, labels are the same as input IDs\n",
        "\n",
        "    # Mask the instruction part of the labels\n",
        "    instruction_len = len(tokenizer_gpt2(instruction, truncation=True)['input_ids'])\n",
        "    labels[:instruction_len] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "tokenized_data_causal = [tokenize_instruction_response_for_causal_lm(pair) for pair in instruction_response_pairs]\n",
        "\n",
        "class InstructionResponseCausalLMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.tokenized_data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokenized_data[idx]\n",
        "\n",
        "instruction_response_dataset_causal = InstructionResponseCausalLMDataset(tokenized_data_causal)\n",
        "train_dataloader_causal = torch.utils.data.DataLoader(instruction_response_dataset_causal, batch_size=4, shuffle=True)\n",
        "\n",
        "# --- Training Loop (Revised) ---\n",
        "num_epochs = 3\n",
        "logging_interval = 50\n",
        "\n",
        "model_causal.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader_causal):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_causal(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step + 1) % logging_interval == 0:\n",
        "            avg_loss = total_loss / logging_interval\n",
        "            print(f\"Step {step+1}/{len(train_dataloader_causal)}, Average Loss: {avg_loss:.4f}\")\n",
        "            total_loss = 0\n",
        "\n",
        "print(\"\\n--- Instruction Fine-tuning Finished (Revised) ---\")\n",
        "\n",
        "# Save the fine-tuned model (Revised path to avoid overwriting)\n",
        "output_dir_instruction_tuned_causal = \"/content/drive/MyDrive/gpt2-instruction-tuned-causal\"\n",
        "model_causal.save_pretrained(output_dir_instruction_tuned_causal)\n",
        "tokenizer_gpt2.save_pretrained(output_dir_instruction_tuned_causal)\n",
        "print(f\"\\n--- Instruction-tuned model and tokenizer saved to {output_dir_instruction_tuned_causal} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsRulMDm7d0b",
        "outputId": "8298fcc4-d2c5-4f7b-acf6-838aef06689d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "Step 50/50, Average Loss: 7.9315\n",
            "\n",
            "Epoch 2/3\n",
            "Step 50/50, Average Loss: 1.1594\n",
            "\n",
            "Epoch 3/3\n",
            "Step 50/50, Average Loss: 0.3012\n",
            "\n",
            "--- Instruction Fine-tuning Finished (Revised) ---\n",
            "\n",
            "--- Instruction-tuned model and tokenizer saved to /content/drive/MyDrive/gpt2-instruction-tuned-causal ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# --- Load the fine-tuned model and tokenizer ---\n",
        "output_dir_instruction_tuned_causal = \"/content/drive/MyDrive/gpt2-instruction-tuned-causal\"\n",
        "model_eval_instruction_tuned = AutoModelForCausalLM.from_pretrained(output_dir_instruction_tuned_causal).to(device)\n",
        "tokenizer_eval_instruction_tuned = AutoTokenizer.from_pretrained(output_dir_instruction_tuned_causal)\n",
        "tokenizer_eval_instruction_tuned.pad_token = tokenizer_eval_instruction_tuned.eos_token\n",
        "\n",
        "# --- Load the test data ---\n",
        "try:\n",
        "    test_df = pd.read_csv(\"/content/drive/MyDrive/llama_test_balanced.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: llama_test_balanced.csv not found.\")\n",
        "    test_df = None\n",
        "\n",
        "if test_df is not None:\n",
        "    # --- Prepare evaluation instructions ---\n",
        "    evaluation_instructions = []\n",
        "    for index, row in test_df.iterrows():\n",
        "        tweet_text = row['tweet_text']\n",
        "        instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "        instruction_text = instruction + tweet_text\n",
        "        evaluation_instructions.append(instruction_text)\n",
        "\n",
        "    # --- Prepare actual labels ---\n",
        "    actual_labels = test_df['distress'].tolist()\n",
        "\n",
        "    print(f\"Prepared {len(evaluation_instructions)} evaluation instructions.\")\n",
        "else:\n",
        "    print(\"Error loading test data, cannot prepare evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWm6ccWP7d4M",
        "outputId": "e291c43d-4b51-4d0e-f288-1f0d069d97a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 1438 evaluation instructions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if test_df is not None:\n",
        "    predicted_responses = []\n",
        "    model_eval_instruction_tuned.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for instruction in evaluation_instructions:\n",
        "            input_ids = tokenizer_eval_instruction_tuned(instruction, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "            output = model_eval_instruction_tuned.generate(\n",
        "                input_ids=input_ids['input_ids'],\n",
        "                attention_mask=input_ids['attention_mask'],\n",
        "                max_new_tokens=10, # Generate up to 10 new tokens for the response\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            generated_response = tokenizer_eval_instruction_tuned.decode(output[:, input_ids['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
        "            predicted_responses.append(generated_response.strip().lower())\n",
        "\n",
        "    print(f\"Generated {len(predicted_responses)} responses.\")\n",
        "\n",
        "    # --- Evaluate the generated responses ---\n",
        "    predicted_labels = []\n",
        "    for response in predicted_responses:\n",
        "        if \"distress\" in response:\n",
        "            predicted_labels.append(1)\n",
        "        elif \"not distress\" in response:\n",
        "            predicted_labels.append(0)\n",
        "        else:\n",
        "            # Handle cases where the response is not as expected\n",
        "            predicted_labels.append(-1) # Or some other indicator\n",
        "\n",
        "    from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "    # Filter out any invalid predictions\n",
        "    valid_predictions = [p for i, p in enumerate(predicted_labels) if p != -1]\n",
        "    valid_actual_labels = [actual_labels[i] for i, p in enumerate(predicted_labels) if p != -1]\n",
        "\n",
        "    if valid_predictions:\n",
        "        print(\"\\n--- Evaluation of Instruction-Tuned GPT-2 ---\")\n",
        "        print(classification_report(valid_actual_labels, valid_predictions))\n",
        "        print(f\"Accuracy: {accuracy_score(valid_actual_labels, valid_predictions)}\")\n",
        "        print(f\"F1-Score: {f1_score(valid_actual_labels, valid_predictions)}\")\n",
        "    else:\n",
        "        print(\"\\n--- No valid predictions generated for evaluation ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Test data not loaded, cannot evaluate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_3unjVT9eHE",
        "outputId": "2b8fabd8-e8c5-4730-fcd3-bb59f11a50d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1438 responses.\n",
            "\n",
            "--- Evaluation of Instruction-Tuned GPT-2 ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        30\n",
            "           1       0.06      1.00      0.12         2\n",
            "\n",
            "    accuracy                           0.06        32\n",
            "   macro avg       0.03      0.50      0.06        32\n",
            "weighted avg       0.00      0.06      0.01        32\n",
            "\n",
            "Accuracy: 0.0625\n",
            "F1-Score: 0.11764705882352941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "500 samples"
      ],
      "metadata": {
        "id": "HUw-CfZw-h0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create a balanced sample of 500 tweets for training ---\n",
        "if train_df is not None:\n",
        "    distress_df = train_df[train_df['distress'] == 1]\n",
        "    not_distress_df = train_df[train_df['distress'] == 0]\n",
        "    distress_sample_500 = resample(distress_df, replace=False, n_samples=250, random_state=42)\n",
        "    not_distress_sample_500 = resample(not_distress_df, replace=False, n_samples=250, random_state=42)\n",
        "    balanced_sample_500_df = pd.concat([distress_sample_500, not_distress_sample_500]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Created a balanced training sample of {len(balanced_sample_500_df)} tweets.\")\n",
        "\n",
        "    # --- Prepare instruction-response pairs for the 500-tweet sample ---\n",
        "    instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "    instruction_response_pairs_500 = []\n",
        "    for index, row in balanced_sample_500_df.iterrows():\n",
        "        tweet_text = row['tweet_text']\n",
        "        distress_label = row['distress']\n",
        "        instruction_text = instruction + tweet_text\n",
        "        response = \"distress\" if distress_label == 1 else \"not distress\"\n",
        "        instruction_response_pairs_500.append({\"instruction\": instruction_text, \"response\": response})\n",
        "\n",
        "    print(f\"Prepared {len(instruction_response_pairs_500)} instruction-response pairs.\")\n",
        "\n",
        "    # --- Tokenize the 500-tweet data ---\n",
        "    def tokenize_instruction_response_for_causal_lm(pair):\n",
        "        instruction = pair['instruction']\n",
        "        response = pair['response']\n",
        "        combined_text = instruction + \" \" + response + tokenizer_gpt2.eos_token\n",
        "        tokenized_input = tokenizer_gpt2(combined_text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "        input_ids = tokenized_input['input_ids'].squeeze()\n",
        "        attention_mask = tokenized_input['attention_mask'].squeeze()\n",
        "        labels = input_ids.clone()\n",
        "        instruction_len = len(tokenizer_gpt2(instruction, truncation=True)['input_ids'])\n",
        "        labels[:instruction_len] = -100\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    tokenized_data_500 = [tokenize_instruction_response_for_causal_lm(pair) for pair in instruction_response_pairs_500]\n",
        "\n",
        "    class InstructionResponseCausalLMDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, tokenized_data):\n",
        "            self.tokenized_data = tokenized_data\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.tokenized_data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.tokenized_data[idx]\n",
        "\n",
        "    instruction_response_dataset_500 = InstructionResponseCausalLMDataset(tokenized_data_500)\n",
        "    train_dataloader_500 = torch.utils.data.DataLoader(instruction_response_dataset_500, batch_size=4, shuffle=True)\n",
        "\n",
        "    print(\"Prepared DataLoader for the 500-tweet training sample.\")\n",
        "\n",
        "else:\n",
        "    print(\"Training data not loaded, cannot prepare 500-tweet sample.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rVl10Fb9eK1",
        "outputId": "f059780e-bc1f-4e2b-a162-4259e5f7831e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created a balanced training sample of 500 tweets.\n",
            "Prepared 500 instruction-response pairs.\n",
            "Prepared DataLoader for the 500-tweet training sample.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "logging_interval = 50\n",
        "\n",
        "model_causal.train()\n",
        "\n",
        "print(f\"\\n--- Starting Instruction Fine-tuning on 500 Tweets for {num_epochs} Epochs ---\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader_500):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_causal(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step + 1) % logging_interval == 0:\n",
        "            avg_loss = total_loss / logging_interval\n",
        "            print(f\"Step {step+1}/{len(train_dataloader_500)}, Average Loss: {avg_loss:.4f}\")\n",
        "            total_loss = 0\n",
        "\n",
        "print(\"\\n--- Instruction Fine-tuning Finished (50 Epochs on 500 Tweets) ---\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "output_dir_instruction_tuned_500 = \"/content/drive/MyDrive/gpt2-instruction-tuned-500\"\n",
        "model_causal.save_pretrained(output_dir_instruction_tuned_500)\n",
        "tokenizer_gpt2.save_pretrained(output_dir_instruction_tuned_500)\n",
        "print(f\"\\n--- Instruction-tuned model and tokenizer saved to {output_dir_instruction_tuned_500} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXg8Ol_L-lyL",
        "outputId": "8f944964-1679-4444-cbe7-428f89d8e964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Instruction Fine-tuning on 500 Tweets for 50 Epochs ---\n",
            "\n",
            "Epoch 1/50\n",
            "Step 50/125, Average Loss: 0.2385\n",
            "Step 100/125, Average Loss: 0.1746\n",
            "\n",
            "Epoch 2/50\n",
            "Step 50/125, Average Loss: 0.1383\n",
            "Step 100/125, Average Loss: 0.1173\n",
            "\n",
            "Epoch 3/50\n",
            "Step 50/125, Average Loss: 0.1017\n",
            "Step 100/125, Average Loss: 0.0938\n",
            "\n",
            "Epoch 4/50\n",
            "Step 50/125, Average Loss: 0.0752\n",
            "Step 100/125, Average Loss: 0.0794\n",
            "\n",
            "Epoch 5/50\n",
            "Step 50/125, Average Loss: 0.0701\n",
            "Step 100/125, Average Loss: 0.0687\n",
            "\n",
            "Epoch 6/50\n",
            "Step 50/125, Average Loss: 0.0641\n",
            "Step 100/125, Average Loss: 0.0607\n",
            "\n",
            "Epoch 7/50\n",
            "Step 50/125, Average Loss: 0.0548\n",
            "Step 100/125, Average Loss: 0.0623\n",
            "\n",
            "Epoch 8/50\n",
            "Step 50/125, Average Loss: 0.0567\n",
            "Step 100/125, Average Loss: 0.0488\n",
            "\n",
            "Epoch 9/50\n",
            "Step 50/125, Average Loss: 0.0518\n",
            "Step 100/125, Average Loss: 0.0473\n",
            "\n",
            "Epoch 10/50\n",
            "Step 50/125, Average Loss: 0.0500\n",
            "Step 100/125, Average Loss: 0.0481\n",
            "\n",
            "Epoch 11/50\n",
            "Step 50/125, Average Loss: 0.0407\n",
            "Step 100/125, Average Loss: 0.0469\n",
            "\n",
            "Epoch 12/50\n",
            "Step 50/125, Average Loss: 0.0406\n",
            "Step 100/125, Average Loss: 0.0434\n",
            "\n",
            "Epoch 13/50\n",
            "Step 50/125, Average Loss: 0.0413\n",
            "Step 100/125, Average Loss: 0.0357\n",
            "\n",
            "Epoch 14/50\n",
            "Step 50/125, Average Loss: 0.0375\n",
            "Step 100/125, Average Loss: 0.0356\n",
            "\n",
            "Epoch 15/50\n",
            "Step 50/125, Average Loss: 0.0343\n",
            "Step 100/125, Average Loss: 0.0349\n",
            "\n",
            "Epoch 16/50\n",
            "Step 50/125, Average Loss: 0.0363\n",
            "Step 100/125, Average Loss: 0.0343\n",
            "\n",
            "Epoch 17/50\n",
            "Step 50/125, Average Loss: 0.0302\n",
            "Step 100/125, Average Loss: 0.0318\n",
            "\n",
            "Epoch 18/50\n",
            "Step 50/125, Average Loss: 0.0308\n",
            "Step 100/125, Average Loss: 0.0276\n",
            "\n",
            "Epoch 19/50\n",
            "Step 50/125, Average Loss: 0.0295\n",
            "Step 100/125, Average Loss: 0.0288\n",
            "\n",
            "Epoch 20/50\n",
            "Step 50/125, Average Loss: 0.0300\n",
            "Step 100/125, Average Loss: 0.0226\n",
            "\n",
            "Epoch 21/50\n",
            "Step 50/125, Average Loss: 0.0251\n",
            "Step 100/125, Average Loss: 0.0256\n",
            "\n",
            "Epoch 22/50\n",
            "Step 50/125, Average Loss: 0.0237\n",
            "Step 100/125, Average Loss: 0.0197\n",
            "\n",
            "Epoch 23/50\n",
            "Step 50/125, Average Loss: 0.0221\n",
            "Step 100/125, Average Loss: 0.0197\n",
            "\n",
            "Epoch 24/50\n",
            "Step 50/125, Average Loss: 0.0172\n",
            "Step 100/125, Average Loss: 0.0178\n",
            "\n",
            "Epoch 25/50\n",
            "Step 50/125, Average Loss: 0.0160\n",
            "Step 100/125, Average Loss: 0.0142\n",
            "\n",
            "Epoch 26/50\n",
            "Step 50/125, Average Loss: 0.0176\n",
            "Step 100/125, Average Loss: 0.0153\n",
            "\n",
            "Epoch 27/50\n",
            "Step 50/125, Average Loss: 0.0142\n",
            "Step 100/125, Average Loss: 0.0159\n",
            "\n",
            "Epoch 28/50\n",
            "Step 50/125, Average Loss: 0.0141\n",
            "Step 100/125, Average Loss: 0.0119\n",
            "\n",
            "Epoch 29/50\n",
            "Step 50/125, Average Loss: 0.0126\n",
            "Step 100/125, Average Loss: 0.0128\n",
            "\n",
            "Epoch 30/50\n",
            "Step 50/125, Average Loss: 0.0114\n",
            "Step 100/125, Average Loss: 0.0113\n",
            "\n",
            "Epoch 31/50\n",
            "Step 50/125, Average Loss: 0.0074\n",
            "Step 100/125, Average Loss: 0.0107\n",
            "\n",
            "Epoch 32/50\n",
            "Step 50/125, Average Loss: 0.0095\n",
            "Step 100/125, Average Loss: 0.0103\n",
            "\n",
            "Epoch 33/50\n",
            "Step 50/125, Average Loss: 0.0089\n",
            "Step 100/125, Average Loss: 0.0072\n",
            "\n",
            "Epoch 34/50\n",
            "Step 50/125, Average Loss: 0.0065\n",
            "Step 100/125, Average Loss: 0.0076\n",
            "\n",
            "Epoch 35/50\n",
            "Step 50/125, Average Loss: 0.0072\n",
            "Step 100/125, Average Loss: 0.0074\n",
            "\n",
            "Epoch 36/50\n",
            "Step 50/125, Average Loss: 0.0071\n",
            "Step 100/125, Average Loss: 0.0046\n",
            "\n",
            "Epoch 37/50\n",
            "Step 50/125, Average Loss: 0.0061\n",
            "Step 100/125, Average Loss: 0.0034\n",
            "\n",
            "Epoch 38/50\n",
            "Step 50/125, Average Loss: 0.0040\n",
            "Step 100/125, Average Loss: 0.0044\n",
            "\n",
            "Epoch 39/50\n",
            "Step 50/125, Average Loss: 0.0034\n",
            "Step 100/125, Average Loss: 0.0050\n",
            "\n",
            "Epoch 40/50\n",
            "Step 50/125, Average Loss: 0.0025\n",
            "Step 100/125, Average Loss: 0.0028\n",
            "\n",
            "Epoch 41/50\n",
            "Step 50/125, Average Loss: 0.0039\n",
            "Step 100/125, Average Loss: 0.0036\n",
            "\n",
            "Epoch 42/50\n",
            "Step 50/125, Average Loss: 0.0043\n",
            "Step 100/125, Average Loss: 0.0027\n",
            "\n",
            "Epoch 43/50\n",
            "Step 50/125, Average Loss: 0.0023\n",
            "Step 100/125, Average Loss: 0.0024\n",
            "\n",
            "Epoch 44/50\n",
            "Step 50/125, Average Loss: 0.0019\n",
            "Step 100/125, Average Loss: 0.0023\n",
            "\n",
            "Epoch 45/50\n",
            "Step 50/125, Average Loss: 0.0027\n",
            "Step 100/125, Average Loss: 0.0022\n",
            "\n",
            "Epoch 46/50\n",
            "Step 50/125, Average Loss: 0.0013\n",
            "Step 100/125, Average Loss: 0.0024\n",
            "\n",
            "Epoch 47/50\n",
            "Step 50/125, Average Loss: 0.0012\n",
            "Step 100/125, Average Loss: 0.0016\n",
            "\n",
            "Epoch 48/50\n",
            "Step 50/125, Average Loss: 0.0009\n",
            "Step 100/125, Average Loss: 0.0019\n",
            "\n",
            "Epoch 49/50\n",
            "Step 50/125, Average Loss: 0.0012\n",
            "Step 100/125, Average Loss: 0.0010\n",
            "\n",
            "Epoch 50/50\n",
            "Step 50/125, Average Loss: 0.0007\n",
            "Step 100/125, Average Loss: 0.0013\n",
            "\n",
            "--- Instruction Fine-tuning Finished (50 Epochs on 500 Tweets) ---\n",
            "\n",
            "--- Instruction-tuned model and tokenizer saved to /content/drive/MyDrive/gpt2-instruction-tuned-500 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# --- Load the fine-tuned model and tokenizer (500 tweets) ---\n",
        "output_dir_instruction_tuned_500 = \"/content/drive/MyDrive/gpt2-instruction-tuned-500\"\n",
        "model_eval_instruction_tuned_500 = AutoModelForCausalLM.from_pretrained(output_dir_instruction_tuned_500).to(device)\n",
        "tokenizer_eval_instruction_tuned_500 = AutoTokenizer.from_pretrained(output_dir_instruction_tuned_500)\n",
        "tokenizer_eval_instruction_tuned_500.pad_token = tokenizer_eval_instruction_tuned_500.eos_token\n",
        "\n",
        "# --- Load the test data (if not already loaded) ---\n",
        "if 'test_df' not in locals():\n",
        "    try:\n",
        "        test_df = pd.read_csv(\"/content/drive/MyDrive/llama_test_balanced.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: llama_test_balanced.csv not found.\")\n",
        "        test_df = None\n",
        "\n",
        "if test_df is not None:\n",
        "    # --- Prepare evaluation instructions ---\n",
        "    evaluation_instructions = []\n",
        "    for index, row in test_df.iterrows():\n",
        "        tweet_text = row['tweet_text']\n",
        "        instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "        instruction_text = instruction + tweet_text\n",
        "        evaluation_instructions.append(instruction_text)\n",
        "\n",
        "    # --- Prepare actual labels ---\n",
        "    actual_labels = test_df['distress'].tolist()\n",
        "\n",
        "    # --- Generate responses ---\n",
        "    predicted_responses = []\n",
        "    model_eval_instruction_tuned_500.eval()\n",
        "    with torch.no_grad():\n",
        "        for instruction in evaluation_instructions:\n",
        "            input_ids = tokenizer_eval_instruction_tuned_500(instruction, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "            output = model_eval_instruction_tuned_500.generate(\n",
        "                input_ids=input_ids['input_ids'],\n",
        "                attention_mask=input_ids['attention_mask'],\n",
        "                max_new_tokens=10,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            generated_response = tokenizer_eval_instruction_tuned_500.decode(output[:, input_ids['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
        "            predicted_responses.append(generated_response.strip().lower())\n",
        "\n",
        "    # --- Evaluate the generated responses ---\n",
        "    predicted_labels = []\n",
        "    for response in predicted_responses:\n",
        "        if \"distress\" in response:\n",
        "            predicted_labels.append(1)\n",
        "        elif \"not distress\" in response:\n",
        "            predicted_labels.append(0)\n",
        "        else:\n",
        "            predicted_labels.append(-1)\n",
        "\n",
        "    valid_predictions = [p for i, p in enumerate(predicted_labels) if p != -1]\n",
        "    valid_actual_labels = [actual_labels[i] for i, p in enumerate(predicted_labels) if p != -1]\n",
        "\n",
        "    if valid_predictions:\n",
        "        print(\"\\n--- Evaluation of Instruction-Tuned GPT-2 (500 Tweets, 50 Epochs) ---\")\n",
        "        print(classification_report(valid_actual_labels, valid_predictions))\n",
        "        print(f\"Accuracy: {accuracy_score(valid_actual_labels, valid_predictions)}\")\n",
        "        print(f\"F1-Score: {f1_score(valid_actual_labels, valid_predictions)}\")\n",
        "    else:\n",
        "        print(\"\\n--- No valid predictions generated for evaluation ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Test data not loaded, cannot evaluate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGN4_78J-ltr",
        "outputId": "3ee4053b-fce1-41d3-9e67-d4e7fe7898a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation of Instruction-Tuned GPT-2 (500 Tweets, 50 Epochs) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1273\n",
            "           1       0.11      1.00      0.20       156\n",
            "\n",
            "    accuracy                           0.11      1429\n",
            "   macro avg       0.05      0.50      0.10      1429\n",
            "weighted avg       0.01      0.11      0.02      1429\n",
            "\n",
            "Accuracy: 0.10916724982505248\n",
            "F1-Score: 0.1968454258675079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Check the class distribution in the 500-tweet training sample ---\n",
        "if 'balanced_sample_500_df' in locals():\n",
        "    print(\"\\n--- Class Distribution in balanced_sample_500_df (Training Data) ---\")\n",
        "    print(balanced_sample_500_df['distress'].value_counts())\n",
        "else:\n",
        "    print(\"\\n--- balanced_sample_500_df not found ---\")\n",
        "\n",
        "# --- Check the class distribution in the test data ---\n",
        "if 'test_df' in locals():\n",
        "    print(\"\\n--- Class Distribution in test_df (Evaluation Data) ---\")\n",
        "    print(test_df['distress'].value_counts())\n",
        "else:\n",
        "    print(\"\\n--- test_df not found ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLbE_cYs-lpu",
        "outputId": "c55c5ad0-2a6c-4265-e594-a27a2567bad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Class Distribution in balanced_sample_500_df (Training Data) ---\n",
            "distress\n",
            "0    250\n",
            "1    250\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Class Distribution in test_df (Evaluation Data) ---\n",
            "distress\n",
            "0    1282\n",
            "1     156\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'balanced_test_df' in locals():\n",
        "    for i in range(10):\n",
        "        instruction = evaluation_instructions_balanced[i]\n",
        "        input_ids = tokenizer_eval_instruction_tuned_500(instruction, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "        output = model_eval_instruction_tuned_500.generate(\n",
        "            input_ids=input_ids['input_ids'],\n",
        "            attention_mask=input_ids['attention_mask'],\n",
        "            max_new_tokens=10,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        generated_response = tokenizer_eval_instruction_tuned_500.decode(output[:, input_ids['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
        "        actual_label = \"distress\" if actual_labels_balanced[i] == 1 else \"not distress\"\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"Generated Response: {generated_response.strip().lower()}\")\n",
        "        print(f\"Actual Label: {actual_label}\")\n",
        "        print(\"-\" * 30)\n",
        "else:\n",
        "    print(\"Balanced test data not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQPg87LGBdBs",
        "outputId": "572fd124-6fdd-45d9-a560-246c3773cec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: #caneel bay before #irma july 2017 view from our room which was destroyed\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: commit helping clean up neighborhood in irma aftermath\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: grandma trapped in yard following irma finally freed\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: 0730 morning brief for volunteers ready to roll and help tx #harvey\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: 6th #firefighter #injured battling #bear #fire #santa\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: how to help the victims of hurricane maria has been published on beautiful life\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: dramatic photo shows a sunny moment of zen in the chaos of hurricane irma all the storm\n",
            "Generated Response: not distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: hurricane harvey liberty fire department receives life supporting supplies\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: #sentinel2 was used to map the #flood in matara #srilanka on 28 may\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: california takes washington on fire help #seattle\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'balanced_test_df' in locals():\n",
        "    for i in range(10):\n",
        "        instruction = evaluation_instructions_balanced[i]\n",
        "        input_ids = tokenizer_eval_instruction_tuned_500(instruction, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "        output = model_eval_instruction_tuned_500.generate(\n",
        "            input_ids=input_ids['input_ids'],\n",
        "            attention_mask=input_ids['attention_mask'],\n",
        "            max_new_tokens=10,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        generated_response = tokenizer_eval_instruction_tuned_500.decode(output[:, input_ids['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
        "        actual_label = \"distress\" if actual_labels_balanced[i] == 1 else \"not distress\"\n",
        "        print(f\"Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\")\n",
        "        print(f\"Tweet: {balanced_test_df['tweet_text'].iloc[i]}\")\n",
        "        print(f\"Generated Response: {generated_response.strip().lower()}\")\n",
        "        print(f\"Actual Label: {actual_label}\")\n",
        "        print(\"-\" * 30)\n",
        "else:\n",
        "    print(\"Balanced test data not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szb-1rRgD2MQ",
        "outputId": "70f51033-b530-42b0-f6dc-651d716099ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: #caneel bay before #irma july 2017 view from our room which was destroyed\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: commit helping clean up neighborhood in irma aftermath\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: grandma trapped in yard following irma finally freed\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: 0730 morning brief for volunteers ready to roll and help tx #harvey\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: 6th #firefighter #injured battling #bear #fire #santa\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: how to help the victims of hurricane maria has been published on beautiful life\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: dramatic photo shows a sunny moment of zen in the chaos of hurricane irma all the storm\n",
            "Generated Response: not distress\n",
            "Actual Label: distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: hurricane harvey liberty fire department receives life supporting supplies\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: #sentinel2 was used to map the #flood in matara #srilanka on 28 may\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'.\n",
            "Tweet: california takes washington on fire help #seattle\n",
            "Generated Response: distress\n",
            "Actual Label: distress\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# --- Load the fine-tuned model and tokenizer (500 tweets) ---\n",
        "output_dir_instruction_tuned_500 = \"/content/drive/MyDrive/gpt2-instruction-tuned-500\"\n",
        "model_eval_instruction_tuned_500 = AutoModelForCausalLM.from_pretrained(output_dir_instruction_tuned_500).to(device)\n",
        "tokenizer_eval_instruction_tuned_500 = AutoTokenizer.from_pretrained(output_dir_instruction_tuned_500)\n",
        "tokenizer_eval_instruction_tuned_500.pad_token = tokenizer_eval_instruction_tuned_500.eos_token\n",
        "\n",
        "# --- Load the balanced test data (if not already created) ---\n",
        "if 'balanced_test_df' not in locals():\n",
        "    if 'test_df' in locals():\n",
        "        distress_test_df = test_df[test_df['distress'] == 1]\n",
        "        not_distress_test_df = test_df[test_df['distress'] == 0]\n",
        "        not_distress_sample_test_df = resample(not_distress_test_df, replace=False, n_samples=len(distress_test_df), random_state=42)\n",
        "        balanced_test_df = pd.concat([distress_test_df, not_distress_sample_test_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        evaluation_instructions_balanced = []\n",
        "        actual_labels_balanced = balanced_test_df['distress'].tolist()\n",
        "\n",
        "        instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "        for index, row in balanced_test_df.iterrows():\n",
        "            tweet_text = row['tweet_text']\n",
        "            instruction_text = instruction + tweet_text\n",
        "            evaluation_instructions_balanced.append(instruction_text)\n",
        "    else:\n",
        "        print(\"Test data not loaded, cannot perform balanced evaluation.\")\n",
        "\n",
        "if 'balanced_test_df' in locals():\n",
        "    # --- Generate responses on the balanced test set ---\n",
        "    predicted_responses_balanced = []\n",
        "    model_eval_instruction_tuned_500.eval()\n",
        "    with torch.no_grad():\n",
        "        for instruction in evaluation_instructions_balanced:\n",
        "            input_ids = tokenizer_eval_instruction_tuned_500(instruction, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "            output = model_eval_instruction_tuned_500.generate(\n",
        "                input_ids=input_ids['input_ids'],\n",
        "                attention_mask=input_ids['attention_mask'],\n",
        "                max_new_tokens=10,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            generated_response = tokenizer_eval_instruction_tuned_500.decode(output[:, input_ids['input_ids'].shape[1]:][0], skip_special_tokens=True)\n",
        "            predicted_responses_balanced.append(generated_response.strip().lower())\n",
        "\n",
        "    # --- Evaluate on the balanced test set (revised to take only the first word) ---\n",
        "    predicted_labels_balanced = []\n",
        "    for response in predicted_responses_balanced:\n",
        "        first_word = response.split()[0] if response.split() else \"\"\n",
        "        if first_word == \"distress\":\n",
        "            predicted_labels_balanced.append(1)\n",
        "        elif first_word == \"not\":\n",
        "            predicted_labels_balanced.append(0)\n",
        "        else:\n",
        "            predicted_labels_balanced.append(-1)\n",
        "\n",
        "    valid_predictions_balanced = [p for i, p in enumerate(predicted_labels_balanced) if p != -1]\n",
        "    valid_actual_labels_balanced = [actual_labels_balanced[i] for i, p in enumerate(predicted_labels_balanced) if p != -1]\n",
        "\n",
        "    if valid_predictions_balanced:\n",
        "        print(\"\\n--- Evaluation on Balanced Test Set (Instruction-Tuned GPT-2 - 500 Tweets, 50 Epochs - Revised) ---\")\n",
        "        print(classification_report(valid_actual_labels_balanced, valid_predictions_balanced))\n",
        "        print(f\"Accuracy: {accuracy_score(valid_actual_labels_balanced, valid_predictions_balanced)}\")\n",
        "        print(f\"F1-Score: {f1_score(valid_actual_labels_balanced, valid_predictions_balanced)}\")\n",
        "    else:\n",
        "        print(\"\\n--- No valid predictions generated for balanced evaluation ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Balanced test data not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whRlDq-2BdFB",
        "outputId": "8a54b455-7454-4e67-c0d3-535c8aa34c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation on Balanced Test Set (Instruction-Tuned GPT-2 - 500 Tweets, 50 Epochs - Revised) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.94      0.92       154\n",
            "           1       0.93      0.90      0.92       156\n",
            "\n",
            "    accuracy                           0.92       310\n",
            "   macro avg       0.92      0.92      0.92       310\n",
            "weighted avg       0.92      0.92      0.92       310\n",
            "\n",
            "Accuracy: 0.9161290322580645\n",
            "F1-Score: 0.9150326797385621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir_instruction_tuned_500 = \"/content/drive/MyDrive/gpt2-instruction-tuned-500\"\n",
        "model_causal.save_pretrained(output_dir_instruction_tuned_500)\n",
        "tokenizer_gpt2.save_pretrained(output_dir_instruction_tuned_500)\n",
        "print(f\"\\n--- Instruction-tuned model and tokenizer saved to {output_dir_instruction_tuned_500} ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yHErx-6BdIH",
        "outputId": "f61c363f-835b-4b59-d085-d270b4fc9360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Instruction-tuned model and tokenizer saved to /content/drive/MyDrive/gpt2-instruction-tuned-500 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Assuming you have 'balanced_sample_500_df' loaded\n",
        "\n",
        "if 'balanced_sample_500_df' in locals():\n",
        "    # --- Perform the train-validation-test split ---\n",
        "    train_df_temp, test_df_split = train_test_split(balanced_sample_500_df, test_size=0.15, random_state=42)\n",
        "    train_df, val_df = train_test_split(train_df_temp, test_size=(0.15/0.85), random_state=42) # Adjust test_size to get approx. 15% validation\n",
        "\n",
        "    print(f\"Training set size: {len(train_df)}\")\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "    print(f\"Test set size: {len(test_df_split)}\")\n",
        "\n",
        "    # --- Prepare instruction-response pairs ---\n",
        "    instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "\n",
        "    def prepare_instruction_response_pairs(df, instruction_text):\n",
        "        pairs = []\n",
        "        for index, row in df.iterrows():\n",
        "            tweet_text = row['tweet_text']\n",
        "            distress_label = row['distress']\n",
        "            instruction = instruction_text + tweet_text\n",
        "            response = \"distress\" if distress_label == 1 else \"not distress\"\n",
        "            pairs.append({\"instruction\": instruction, \"response\": response})\n",
        "        return pairs\n",
        "\n",
        "    train_pairs = prepare_instruction_response_pairs(train_df, instruction)\n",
        "    val_pairs = prepare_instruction_response_pairs(val_df, instruction)\n",
        "    test_pairs = prepare_instruction_response_pairs(test_df_split, instruction)\n",
        "\n",
        "    # --- Tokenize the data ---\n",
        "    tokenizer = tokenizer_gpt2 # Assuming tokenizer_gpt2 is already loaded\n",
        "\n",
        "    def tokenize_instruction_response_for_causal_lm(pair):\n",
        "        instruction = pair['instruction']\n",
        "        response = pair['response']\n",
        "        combined_text = instruction + \" \" + response + tokenizer.eos_token\n",
        "        tokenized_input = tokenizer(combined_text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "        input_ids = tokenized_input['input_ids'].squeeze()\n",
        "        attention_mask = tokenized_input['attention_mask'].squeeze()\n",
        "        labels = input_ids.clone()\n",
        "        instruction_len = len(tokenizer(instruction, truncation=True)['input_ids'])\n",
        "        labels[:instruction_len] = -100\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    train_tokenized_data = [tokenize_instruction_response_for_causal_lm(pair) for pair in train_pairs]\n",
        "    val_tokenized_data = [tokenize_instruction_response_for_causal_lm(pair) for pair in val_pairs]\n",
        "    test_tokenized_data = [tokenize_instruction_response_for_causal_lm(pair) for pair in test_pairs]\n",
        "\n",
        "    # --- Create Datasets and DataLoaders ---\n",
        "    class InstructionResponseCausalLMDataset(Dataset):\n",
        "        def __init__(self, tokenized_data):\n",
        "            self.tokenized_data = tokenized_data\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.tokenized_data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.tokenized_data[idx]\n",
        "\n",
        "    train_dataset = InstructionResponseCausalLMDataset(train_tokenized_data)\n",
        "    val_dataset = InstructionResponseCausalLMDataset(val_tokenized_data)\n",
        "    test_dataset = InstructionResponseCausalLMDataset(test_tokenized_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "    print(\"\\nPrepared train, validation, and test datasets and dataloaders.\")\n",
        "\n",
        "else:\n",
        "    print(\"balanced_sample_500_df not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1RmS5oZGL-V",
        "outputId": "120201f2-1653-4b54-b323-df839fcfbcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 350\n",
            "Validation set size: 75\n",
            "Test set size: 75\n",
            "\n",
            "Prepared train, validation, and test datasets and dataloaders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10 # Let's start with a smaller number of epochs for this smaller dataset and with validation\n",
        "logging_interval = 50\n",
        "\n",
        "model_causal.train()\n",
        "\n",
        "print(f\"\\n--- Starting Instruction Fine-tuning on Training Set ({len(train_dataset)} samples) for {num_epochs} Epochs ---\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_causal(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step + 1) % logging_interval == 0:\n",
        "            avg_loss = total_loss / logging_interval\n",
        "            print(f\"Step {step+1}/{len(train_dataloader)}, Average Training Loss: {avg_loss:.4f}\")\n",
        "            total_loss = 0\n",
        "\n",
        "    # --- Evaluate on the validation set at the end of each epoch ---\n",
        "    model_causal.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model_causal(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
        "    model_causal.train() # Set back to train mode\n",
        "\n",
        "print(\"\\n--- Instruction Fine-tuning Finished ---\")\n",
        "\n",
        "# Now you can evaluate on the test_dataloader using a similar evaluation loop\n",
        "# and by generating responses and comparing them to the test labels."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j216sEveGMBm",
        "outputId": "2761301d-dbb8-4496-e383-5f622ca006db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Instruction Fine-tuning on Training Set (350 samples) for 10 Epochs ---\n",
            "\n",
            "Epoch 1/10\n",
            "Step 50/88, Average Training Loss: 0.0014\n",
            "Epoch 1 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 2/10\n",
            "Step 50/88, Average Training Loss: 0.0009\n",
            "Epoch 2 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 3/10\n",
            "Step 50/88, Average Training Loss: 0.0007\n",
            "Epoch 3 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 4/10\n",
            "Step 50/88, Average Training Loss: 0.0006\n",
            "Epoch 4 Validation Loss: 0.0001\n",
            "\n",
            "Epoch 5/10\n",
            "Step 50/88, Average Training Loss: 0.0020\n",
            "Epoch 5 Validation Loss: 0.0001\n",
            "\n",
            "Epoch 6/10\n",
            "Step 50/88, Average Training Loss: 0.0006\n",
            "Epoch 6 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 7/10\n",
            "Step 50/88, Average Training Loss: 0.0008\n",
            "Epoch 7 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 8/10\n",
            "Step 50/88, Average Training Loss: 0.0003\n",
            "Epoch 8 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 9/10\n",
            "Step 50/88, Average Training Loss: 0.0009\n",
            "Epoch 9 Validation Loss: 0.0002\n",
            "\n",
            "Epoch 10/10\n",
            "Step 50/88, Average Training Loss: 0.0005\n",
            "Epoch 10 Validation Loss: 0.0002\n",
            "\n",
            "--- Instruction Fine-tuning Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import torch\n",
        "\n",
        "\n",
        "def evaluate_dataloader_debug_direct(model, dataloader, tokenizer, device, dataset_name, original_df):\n",
        "    model.eval()\n",
        "    predicted_labels = []\n",
        "    actual_labels = []\n",
        "    instruction_results = []\n",
        "    raw_responses = []\n",
        "\n",
        "    print(f\"\\n--- Debugging Evaluation for {dataset_name} Set ---\")\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=10,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            generated_responses = [tokenizer.decode(out[input_ids.shape[1]:], skip_special_tokens=True).strip().lower() for out in output]\n",
        "            actual_batch_labels = [1 if label > 0 else 0 for label in labels[:, -1]]\n",
        "\n",
        "            for i, response in enumerate(generated_responses):\n",
        "                raw_responses.append(response)\n",
        "                first_word = response.split()[0] if response.split() else \"\"\n",
        "                predicted_label = -1\n",
        "                if first_word == \"distress\":\n",
        "                    predicted_label = 1\n",
        "                elif first_word == \"not\":\n",
        "                    predicted_label = 0\n",
        "\n",
        "                if predicted_label != -1:\n",
        "                    predicted_labels.append(predicted_label)\n",
        "                    actual_labels.append(actual_batch_labels[i])\n",
        "                    instruction_results.append({\n",
        "                        \"instruction\": tokenizer.decode(input_ids[i], skip_special_tokens=True),\n",
        "                        \"generated_response\": response,\n",
        "                        \"actual_label\": \"distress\" if actual_batch_labels[i] == 1 else \"not distress\",\n",
        "                        \"predicted_label\": \"distress\" if predicted_label == 1 else \"not distress\"\n",
        "                    })\n",
        "\n",
        "                if count < 5:\n",
        "                    print(f\"\\n--- Example {count + 1} ({dataset_name}) ---\")\n",
        "                    print(f\"Instruction: {tokenizer.decode(input_ids[i], skip_special_tokens=True)}\")\n",
        "                    print(f\"Raw Generated Response: {response}\")\n",
        "                    print(f\"Actual Label (from batch): {'distress' if actual_batch_labels[i] == 1 else 'not distress'}\")\n",
        "                    if original_df is not None and len(original_df) > (dataloader.batch_size * (count // dataloader.batch_size) + i):\n",
        "                        print(f\"Actual Label (from DataFrame): {('distress' if original_df.iloc[dataloader.batch_size * (count // dataloader.batch_size) + i]['distress'] == 1 else 'not distress') if 'distress' in original_df.columns else 'DataFrame label not available'}\")\n",
        "                    count += 1\n",
        "            if count >= 5 and dataset_name == \"Training\":\n",
        "                break # Only show a few examples per dataset\n",
        "\n",
        "    model.train()\n",
        "    return actual_labels, predicted_labels, instruction_results\n",
        "\n",
        "# Re-run the debugging evaluation\n",
        "evaluate_dataloader_debug_direct(model_causal, train_dataloader, tokenizer_gpt2, device, \"Training\", train_df)\n",
        "evaluate_dataloader_debug_direct(model_causal, val_dataloader, tokenizer_gpt2, device, \"Validation\", val_df)\n",
        "evaluate_dataloader_debug_direct(model_causal, test_dataloader, tokenizer_gpt2, device, \"Test\", test_df_split)\n",
        "\n",
        "# Then, run the standard evaluation to see the metrics\n",
        "train_actual, train_predicted, train_instruction_results = evaluate_dataloader(model_causal, train_dataloader, tokenizer_gpt2, device)\n",
        "val_actual, val_predicted, val_instruction_results = evaluate_dataloader(model_causal, val_dataloader, tokenizer_gpt2, device)\n",
        "test_actual, test_predicted, test_instruction_results = evaluate_dataloader(model_causal, test_dataloader, tokenizer_gpt2, device)\n",
        "\n",
        "print_evaluation_results(\"Training\", train_actual, train_predicted, train_instruction_results)\n",
        "print_evaluation_results(\"Validation\", val_actual, val_predicted, val_instruction_results)\n",
        "print_evaluation_results(\"Test\", test_actual, test_predicted, test_instruction_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q46EWZPgGMFD",
        "outputId": "b0814a02-8ed0-4787-e9b6-086b491f853c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debugging Evaluation for Training Set ---\n",
            "\n",
            "--- Example 1 (Training) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: in the darkest days after #irma the conch republic took care of its own by not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 2 (Training) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: damage to soffits missing soffits and a blown down fence in my apartme distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 3 (Training) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: florida rednecks attempt dumbest heist of all time during irma via #maga not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 4 (Training) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: here my husband with jorge amado and carlis fuentes in puerto rico not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): distress\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 5 (Training) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: we have the best customersthank you for letting us help #hvac #coldair #swfl #irma distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Debugging Evaluation for Validation Set ---\n",
            "\n",
            "--- Example 1 (Validation) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: new article #hurricaneharvey impact on oil and gas an open question not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 2 (Validation) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: congress cant play politics with the lives of 34 million americans in puerto rico via not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 3 (Validation) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: puerto rico hurricane and mexico earthquake relief fund flacs will be collecting specific not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 4 (Validation) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: tryon equestrian center to help hundreds of horses escape hurricane irma #nc distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): distress\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 5 (Validation) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: elementary schools and food pantry collaborate to help puerto rico distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): distress\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debugging Evaluation for Test Set ---\n",
            "\n",
            "--- Example 1 (Test) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: jan man rescue operation underway in mexico after devastating earthquake #newsinvidsindia not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 2 (Test) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: key deer surprises martin county rapid response crews in irma aftermath not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 3 (Test) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: after irma normal life returning for tampa bay residents latest on power outages gas supplies and more not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n",
            "\n",
            "--- Example 4 (Test) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: in mexico earthquake survivors plan on rebuilding their lives not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 5 (Test) ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: hurricane #irma view of #guadeloupe and #lessaintes from nw tip of #dominica 530 pm not distress\n",
            "Raw Generated Response: \n",
            "Actual Label (from batch): distress\n",
            "Actual Label (from DataFrame): not distress\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Results for Training Set ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "Accuracy: 1.0\n",
            "F1-Score: 0.0\n",
            "\n",
            "--- Sample Instruction Results for Training Set ---\n",
            "Instruction: You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: #cbsthismorning #teresamay #jose #maria #anncoulter #miamibitch #weatherbitch #irma whats with\n",
            "Generated Response: not distress\n",
            "Actual Label: not distress\n",
            "Predicted Label: not distress\n",
            "------------------------------\n",
            "\n",
            "--- Evaluation Results for Validation Set ---\n",
            "No valid predictions generated.\n",
            "\n",
            "--- Sample Instruction Results for Validation Set ---\n",
            "\n",
            "--- Evaluation Results for Test Set ---\n",
            "No valid predictions generated.\n",
            "\n",
            "--- Sample Instruction Results for Test Set ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Assuming you have 'balanced_sample_500_df' loaded (we might need to load it again if the reset was extensive)\n",
        "if 'balanced_sample_500_df' not in locals():\n",
        "    balanced_sample_500_df = pd.read_csv('/content/drive/MyDrive/balanced_sample_500.csv')\n",
        "    print(\"balanced_sample_500_df loaded.\")\n",
        "\n",
        "if 'balanced_sample_500_df' in locals():\n",
        "    # --- Perform the train-validation-test split ---\n",
        "    train_df_temp, test_df_split = train_test_split(balanced_sample_500_df, test_size=0.15, random_state=42)\n",
        "    train_df, val_df = train_test_split(train_df_temp, test_size=(0.15/0.85), random_state=42) # Adjust test_size to get approx. 15% validation\n",
        "\n",
        "    print(f\"Training set size: {len(train_df)}\")\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "    print(f\"Test set size: {len(test_df_split)}\")\n",
        "\n",
        "    # --- Prepare instruction-response pairs ---\n",
        "    instruction = \"You are an emergency detection system. Determine if the following tweet clearly indicates that someone is in danger, trapped, injured, requesting help, or facing a life-threatening emergency. Only respond with one word: 'distress' or 'not distress'. Do not explain. Tweets with emotional tone are not enough - only choose 'distress' if there's a clear, urgent need for help. If unsure, default to 'not distress'. Tweet: \"\n",
        "\n",
        "    def prepare_instruction_response_pairs(df, instruction_text):\n",
        "        pairs = []\n",
        "        for index, row in df.iterrows():\n",
        "            tweet_text = row['tweet_text']\n",
        "            distress_label = row['distress']\n",
        "            instruction = instruction_text + tweet_text\n",
        "            response = \"distress\" if distress_label == 1 else \"not distress\"\n",
        "            pairs.append({\"instruction\": instruction, \"response\": response})\n",
        "        return pairs\n",
        "\n",
        "    train_pairs = prepare_instruction_response_pairs(train_df, instruction)\n",
        "    val_pairs = prepare_instruction_response_pairs(val_df, instruction)\n",
        "    test_pairs = prepare_instruction_response_pairs(test_df_split, instruction)\n",
        "\n",
        "    # --- Tokenize the data ---\n",
        "    tokenizer = tokenizer_gpt2 # Assuming tokenizer_gpt2 is now loaded\n",
        "\n",
        "    def tokenize_instruction_response_for_causal_lm(pair):\n",
        "        instruction = pair['instruction']\n",
        "        response = pair['response']\n",
        "        combined_text = instruction + \" \" + response + tokenizer.eos_token\n",
        "        tokenized_input = tokenizer(combined_text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
        "        input_ids = tokenized_input['input_ids'].squeeze()\n",
        "        attention_mask = tokenized_input['attention_mask'].squeeze()\n",
        "        labels = input_ids.clone()\n",
        "        instruction_len = len(tokenizer(instruction, truncation=True)['input_ids'])\n",
        "        labels[:instruction_len] = -100\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    train_tokenized_data = [tokenize_instruction_response_for_causal_lm(pair) for pair in train_pairs]\n",
        "    val_tokenized_data = [tokenize_instruction_response_for_causal_lm(pair) for pair in val_pairs]\n",
        "    test_tokenized_data = [tokenize_instruction_response_for_causal_lm(pair) for pair in test_pairs]\n",
        "\n",
        "    # --- Create Datasets and DataLoaders ---\n",
        "    class InstructionResponseCausalLMDataset(Dataset):\n",
        "        def __init__(self, tokenized_data):\n",
        "            self.tokenized_data = tokenized_data\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.tokenized_data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.tokenized_data[idx]\n",
        "\n",
        "    train_dataset = InstructionResponseCausalLMDataset(train_tokenized_data)\n",
        "    val_dataset = InstructionResponseCausalLMDataset(val_tokenized_data)\n",
        "    test_dataset = InstructionResponseCausalLMDataset(test_tokenized_data)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "    print(\"\\nPrepared train, validation, and test datasets and dataloaders.\")\n",
        "\n",
        "else:\n",
        "    print(\"balanced_sample_500_df not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "cIPTaiCHItyz",
        "outputId": "9a3c1c66-754c-4a72-f682-50e5c9335397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/balanced_sample_500.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0a5084b318a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming you have 'balanced_sample_500_df' loaded (we might need to load it again if the reset was extensive)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'balanced_sample_500_df'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbalanced_sample_500_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/balanced_sample_500.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced_sample_500_df loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/balanced_sample_500.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "restarting"
      ],
      "metadata": {
        "id": "G_rnkCMQLLbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace with the actual path to your dataset\n",
        "file_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 20 rows of the dataset:\")\n",
        "    print(df.head(20))\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'. Please check the path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZNUoIqdLLEq",
        "outputId": "f46ac32a-62e7-4966-fd5b-a61baf409dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 rows of the dataset:\n",
            "        tweet_id                  image_id  \\\n",
            "0   9.177910e+17  917791044158185473_0.jpg   \n",
            "1   9.177911e+17  917791130590183424_0.jpg   \n",
            "2   9.177913e+17  917791291823591425_0.jpg   \n",
            "3   9.177913e+17  917791291823591425_1.jpg   \n",
            "4   9.177921e+17  917792092100988929_0.jpg   \n",
            "5   9.177921e+17  917792147700465664_0.jpg   \n",
            "6   9.177929e+17  917792930315821057_0.jpg   \n",
            "7   9.177931e+17  917793137925459968_0.jpg   \n",
            "8   9.177931e+17  917793137925459968_1.jpg   \n",
            "9   9.177931e+17  917793137925459968_2.jpg   \n",
            "10  9.177932e+17  917793158251077632_0.jpg   \n",
            "11  9.177937e+17  917793736918216706_0.jpg   \n",
            "12  9.177939e+17  917793881533571073_0.jpg   \n",
            "13  9.177940e+17  917794024173563904_0.jpg   \n",
            "14  9.177942e+17  917794232160661505_0.jpg   \n",
            "15  9.177944e+17  917794360581869569_0.jpg   \n",
            "16  9.177946e+17  917794580728295424_0.jpg   \n",
            "17  9.177949e+17  917794892113498113_0.jpg   \n",
            "18  9.177951e+17  917795098523512962_0.jpg   \n",
            "19  9.177952e+17  917795236595863552_0.jpg   \n",
            "\n",
            "                                       raw_tweet_text  \\\n",
            "0   RT @Gizmodo: Wildfires raging through Northern...   \n",
            "1   PHOTOS: Deadly wildfires rage in California ht...   \n",
            "2   RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "3   RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "4   RT @TIME: California's raging wildfires as you...   \n",
            "5   Wildfires Threaten Californiaâ€™s First Legal ...   \n",
            "6   Mass Evacuations in California as Wildfires Ki...   \n",
            "7   RT @KAKEnews: California wildfires destroy mor...   \n",
            "8   RT @KAKEnews: California wildfires destroy mor...   \n",
            "9   RT @KAKEnews: California wildfires destroy mor...   \n",
            "10  RT @FoxNews: Southern California fire shrouds ...   \n",
            "11  California wildfire. á½¡4 https://t.co/a8oD5rkDdI   \n",
            "12  Wildfires Still Burn in Northern California; 1...   \n",
            "13  Henry Cejudo suffers burns, loses gold medal i...   \n",
            "14  At Least 11 Dead and 100 Missing as Wildfires ...   \n",
            "15  Southern California wildfires continue to rage...   \n",
            "16  More than 100 missing persons reports made in ...   \n",
            "17  RT @News12BX: California wildfires kill 10, de...   \n",
            "18  RT @R5_Fire_News: California large wildfire ac...   \n",
            "19  11 dead, thousands homeless as wildfires torch...   \n",
            "\n",
            "                                           tweet_text        tweet_hashtags  \\\n",
            "0   wildfires raging through northern california a...                   NaN   \n",
            "1          photos deadly wildfires rage in california                   NaN   \n",
            "2   pls share were capturing wildfire response rec...                   NaN   \n",
            "3   pls share were capturing wildfire response rec...                   NaN   \n",
            "4   californias raging wildfires as youve never se...                   NaN   \n",
            "5   wildfires threaten californias first legal can...                   NaN   \n",
            "6   mass evacuations in california as wildfires ki...   californiawildfires   \n",
            "7   california wildfires destroy more than 50 stru...              kakenews   \n",
            "8   california wildfires destroy more than 50 stru...              kakenews   \n",
            "9   california wildfires destroy more than 50 stru...              kakenews   \n",
            "10  southern california fire shrouds disneyland an...                   NaN   \n",
            "11                              california wildfire 4                   NaN   \n",
            "12  wildfires still burn in northern california 11...                   nyt   \n",
            "13  henry cejudo suffers burns loses gold medal in...                   NaN   \n",
            "14  at least 11 dead and 100 missing as wildfires ...                   NaN   \n",
            "15  southern california wildfires continue to rage...                   NaN   \n",
            "16  more than 100 missing persons reports made in ...                   NaN   \n",
            "17  california wildfires kill 10 destroy 1500 buil...                   NaN   \n",
            "18  california large wildfire activity for october...  cawildfires, cafires   \n",
            "19  11 dead thousands homeless as wildfires torch ...   heatwave, wildfires   \n",
            "\n",
            "                                        image_caption  distress  \\\n",
            "0   a fire is seen burning through the trees in th...         0   \n",
            "1   two people are standing in a burned area with ...         0   \n",
            "2          a fire burns in a burned area near a house         0   \n",
            "3   several people standing in front of a screen w...         0   \n",
            "4   a night sky with a mountain and a milky in the...         0   \n",
            "5   firefighters are battling a forest fire in the...         0   \n",
            "6   a fire is seen in the distance behind trees an...         1   \n",
            "7   araffes and fire in a burned area with a fire ...         0   \n",
            "8   people standing in front of a fire with a sky ...         0   \n",
            "9   flames are seen in the sky behind a building o...         0   \n",
            "10      mickey mouse pumpkin with a flag on top of it         0   \n",
            "11  there is a man on a television screen with a n...         0   \n",
            "12  there are two people hugging each other in the...         0   \n",
            "13  someone has a very large foot with a very big toe         0   \n",
            "14  burned house in a burned area with a lot of ru...         1   \n",
            "15  flames are seen from a hillside in the distanc...         0   \n",
            "16  a fire is seen burning through the air near a ...         1   \n",
            "17  flames are seen in the background of a house o...         0   \n",
            "18  a map of california with the current wildfires...         0   \n",
            "19  flames are seen from a house on fire in the night         0   \n",
            "\n",
            "                            take_action       state  \\\n",
            "0                                   NaN  California   \n",
            "1                                   NaN  California   \n",
            "2                                   NaN         NaN   \n",
            "3                                   NaN         NaN   \n",
            "4                                   NaN  California   \n",
            "5                                   NaN  California   \n",
            "6   send evacuation and shelter support  California   \n",
            "7                                   NaN  California   \n",
            "8                                   NaN  California   \n",
            "9                                   NaN  California   \n",
            "10                                  NaN  California   \n",
            "11                                  NaN  California   \n",
            "12                                  NaN  California   \n",
            "13                                  NaN  California   \n",
            "14          start missing person search  California   \n",
            "15                                  NaN  California   \n",
            "16          start missing person search  California   \n",
            "17                                  NaN  California   \n",
            "18                                  NaN  California   \n",
            "19                                  NaN  California   \n",
            "\n",
            "                   sub_location Wildfire  \n",
            "0                      northern      Yes  \n",
            "1                           NaN      Yes  \n",
            "2                           NaN       No  \n",
            "3                           NaN       No  \n",
            "4                           NaN      Yes  \n",
            "5                           NaN      Yes  \n",
            "6                     wildfires      Yes  \n",
            "7                           NaN      Yes  \n",
            "8                           NaN      Yes  \n",
            "9                           NaN      Yes  \n",
            "10  southern disneyland anaheim      Yes  \n",
            "11                          NaN      Yes  \n",
            "12                     northern      Yes  \n",
            "13                          NaN      Yes  \n",
            "14                     northern      Yes  \n",
            "15                     southern      Yes  \n",
            "16                          NaN      Yes  \n",
            "17                          NaN      Yes  \n",
            "18                          NaN      Yes  \n",
            "19                          NaN      Yes  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load your wildfire dataset\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv') # Replace with your actual path\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset file not found. Please check the path.\")\n",
        "    df = None\n",
        "\n",
        "if df is not None and 'distress' in df.columns:\n",
        "    # Check class balance for 'distress'\n",
        "    distress_counts = df['distress'].value_counts()\n",
        "    print(\"\\nDistress Class Distribution:\\n\", distress_counts)\n",
        "\n",
        "    # --- Balance the dataset if needed ---\n",
        "    if distress_counts.iloc[0] != distress_counts.iloc[1]:\n",
        "        major_class = df[df['distress'] == distress_counts.idxmax()]\n",
        "        minor_class = df[df['distress'] == distress_counts.idxmin()]\n",
        "        minor_upsampled = resample(minor_class, replace=True, n_samples=len(major_class), random_state=42)\n",
        "        balanced_df = pd.concat([major_class, minor_upsampled])\n",
        "        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle\n",
        "        print(\"\\nDataset balanced based on 'distress' column.\")\n",
        "        df_to_split = balanced_df\n",
        "    else:\n",
        "        print(\"\\nDataset is already balanced based on 'distress' column.\")\n",
        "        df_to_split = df\n",
        "\n",
        "    # --- Stratified and Balanced Data Splitting ---\n",
        "    # 1. Sample (50 count) - stratified and balanced\n",
        "    sample_df = df_to_split.groupby('distress', group_keys=False).apply(lambda x: x.sample(min(len(x), 25), random_state=42))\n",
        "    remaining_df = df_to_split.drop(sample_df.index)\n",
        "\n",
        "    # 2. Training (80% of remaining) - stratified and balanced\n",
        "    train_df, temp_df = train_test_split(remaining_df, test_size=0.2, stratify=remaining_df['distress'], random_state=42)\n",
        "\n",
        "    # 3. Validation (10% of remaining) - stratified\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['distress'], random_state=42)\n",
        "\n",
        "    print(\"\\nData Split Sizes:\")\n",
        "    print(f\"Sample set size: {len(sample_df)}\")\n",
        "    print(f\"Training set size: {len(train_df)}\")\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "    print(f\"Testing set size: {len(test_df)}\")\n",
        "\n",
        "    # Save the splits\n",
        "    sample_df.to_csv('/content/drive/MyDrive/wildfire_sample_50.csv', index=False)\n",
        "    train_df.to_csv('/content/drive/MyDrive/wildfire_train.csv', index=False)\n",
        "    val_df.to_csv('/content/drive/MyDrive/wildfire_val.csv', index=False)\n",
        "    test_df.to_csv('/content/drive/MyDrive/wildfire_test.csv', index=False)\n",
        "    print(\"\\nData splits saved to Google Drive.\")\n",
        "\n",
        "else:\n",
        "    print(\"Data loading failed or 'distress' column not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP-ej1M_It5n",
        "outputId": "db1f4289-949b-4b64-87e6-9ae9c0518755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Distress Class Distribution:\n",
            " distress\n",
            "0    16100\n",
            "1     1982\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset balanced based on 'distress' column.\n",
            "\n",
            "Data Split Sizes:\n",
            "Sample set size: 50\n",
            "Training set size: 25720\n",
            "Validation set size: 3215\n",
            "Testing set size: 3215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-00a295467f45>:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sample_df = df_to_split.groupby('distress', group_keys=False).apply(lambda x: x.sample(min(len(x), 25), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data splits saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5TokenizerFast\n",
        "\n",
        "# Load the training dataset\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv')\n",
        "    print(\"Training dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Training dataset file not found. Please check the path.\")\n",
        "    train_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # Drop rows with NaN in 'take_action' as T5 needs a target\n",
        "    t5_train_df = train_df.dropna(subset=['take_action']).copy()\n",
        "    t5_train_df['input_text'] = \"What action should be taken for this tweet? \" + t5_train_df['tweet_text']\n",
        "    t5_train_df['target_text'] = t5_train_df['take_action']\n",
        "    t5_train_df = t5_train_df[['input_text', 'target_text']]\n",
        "\n",
        "    # Load the T5 tokenizer\n",
        "    tokenizer_t5 = T5TokenizerFast.from_pretrained('t5-small')\n",
        "\n",
        "    print(\"\\nPrepared training data for T5.\")\n",
        "    print(f\"Number of training examples for T5: {len(t5_train_df)}\")\n",
        "\n",
        "    # We'll do the same for the validation set to evaluate T5 later\n",
        "    try:\n",
        "        val_df = pd.read_csv('/content/drive/MyDrive/wildfire_val.csv')\n",
        "        t5_val_df = val_df.dropna(subset=['take_action']).copy()\n",
        "        t5_val_df['input_text'] = \"What action should be taken for this tweet? \" + t5_val_df['tweet_text']\n",
        "        t5_val_df['target_text'] = t5_val_df['take_action']\n",
        "        t5_val_df = t5_val_df[['input_text', 'target_text']]\n",
        "        print(f\"Number of validation examples for T5: {len(t5_val_df)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Validation dataset file not found.\")\n",
        "        t5_val_df = None\n",
        "\n",
        "else:\n",
        "    print(\"Training data not available, cannot prepare for T5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "543b129f9c044154a8c57c32e185d4ec",
            "29ef8ebb3f004f498bd2fa89466c59ad",
            "ef191ffa6afc49b88e935b6166530f34",
            "82c93e581cc942d5915ea45d903f5b6e",
            "7552314c8fa44982b1de8b4761bcef30",
            "76046b56505146a5af6799b80de65b18",
            "69dc75a03f5e4486930519a5f8c3d6c3",
            "17d0d5972a95473a955d10207ef5ef2b",
            "f391ab0688004443b39558825dd35c6c",
            "cad11e1d152d49a2911d3efae9bf6a2d",
            "e623601cdd674d72a02edf668ca47461",
            "0d0cb5708262471db0144b6e44698f5e",
            "72856c771bce4113bef8a3cd038bc452",
            "49a8b098dc884e6ba7f2b2b631cbc57a",
            "10cf2e431d5841c1a5b58ac1698f7a55",
            "d0330849069644efa6807a675bfb5366",
            "76758ac031d449ef805f54ac625afbcd",
            "4d558daf2bb744d08661cf4222f44557",
            "4d51d220bcfd49a9b5e54899b91afce5",
            "e126a335640e4599ab364f81046e6fd0",
            "e6a75eb9e1294037837765ebe374167e",
            "c643e959a14743b8b9bd6b6b20696c1a",
            "a0c4fad873a94b5580473fe1d784af92",
            "cc9ed71d00534708a2982d768ab553d6",
            "aece68696c4d43618aeff04a8b6b78f9",
            "cc95346d3bf246869041e73439002215",
            "7056c046c77a4fc3a23d2a76bd682311",
            "46c6b4cfbad243869ddd8b11e01deba8",
            "b1348a6ee01a4d1287ce4790cdc6b2c5",
            "eba44ba12a95469b889dc9f75229d522",
            "dc9d75725d194c5a8db4d969354a3c02",
            "bc2f700c3c9e45fc81d0b6c96aba0f04",
            "505da97374634700a688fa1acf845d55"
          ]
        },
        "id": "LpViNZAMIt9x",
        "outputId": "32a539da-3704-47d7-8871-701bb49430ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "543b129f9c044154a8c57c32e185d4ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d0cb5708262471db0144b6e44698f5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0c4fad873a94b5580473fe1d784af92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prepared training data for T5.\n",
            "Number of training examples for T5: 12860\n",
            "Number of validation examples for T5: 1608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, get_scheduler\n",
        "from torch.optim import AdamW  # Correct import for AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the T5 model\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
        "\n",
        "# Define a T5 Dataset class\n",
        "class T5WildfireActionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, source_len=128, target_len=32):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_len = source_len\n",
        "        self.target_len = target_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.data.iloc[idx]['input_text']\n",
        "        target_text = self.data.iloc[idx]['target_text']\n",
        "\n",
        "        source = self.tokenizer(\n",
        "            [input_text],\n",
        "            max_length=self.source_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        target = self.tokenizer(\n",
        "            [target_text],\n",
        "            max_length=self.target_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': source['input_ids'].flatten(),\n",
        "            'attention_mask': source['attention_mask'].flatten(),\n",
        "            'labels': target['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# Create T5 Datasets and DataLoaders\n",
        "train_t5_dataset = T5WildfireActionDataset(t5_train_df, tokenizer_t5)\n",
        "val_t5_dataset = T5WildfireActionDataset(t5_val_df, tokenizer_t5)\n",
        "\n",
        "train_t5_dataloader = DataLoader(train_t5_dataset, batch_size=16, shuffle=True)\n",
        "val_t5_dataloader = DataLoader(val_t5_dataset, batch_size=16)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer_t5 = AdamW(model_t5.parameters(), lr=3e-5, weight_decay=0.01) # Use torch.optim.AdamW\n",
        "num_epochs_t5 = 5\n",
        "num_training_steps_t5 = len(train_t5_dataloader) * num_epochs_t5\n",
        "lr_scheduler_t5 = get_scheduler(\"linear\", optimizer=optimizer_t5, num_warmup_steps=0, num_training_steps=num_training_steps_t5)\n",
        "\n",
        "# Callback for Early Stopping\n",
        "class EarlyStoppingCallback:\n",
        "    def __init__(self, patience=3, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(patience=3)\n",
        "\n",
        "# Training Loop for T5\n",
        "print(\"\\n--- T5 Action Response Fine-tuning Started ---\")\n",
        "for epoch in range(num_epochs_t5):\n",
        "    model_t5.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_t5_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model_t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_t5.step()\n",
        "        lr_scheduler_t5.step()\n",
        "        optimizer_t5.zero_grad()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_t5_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate on Validation Set\n",
        "    model_t5.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_t5_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model_t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_t5_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    early_stopping(avg_val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n--- T5 Action Response Fine-tuning Finished ---\")\n",
        "\n",
        "# Save the trained T5 model\n",
        "model_t5.save_pretrained('/content/drive/MyDrive/t5_wildfire_actions')\n",
        "tokenizer_t5.save_pretrained('/content/drive/MyDrive/t5_wildfire_actions')\n",
        "print(\"\\nTrained T5 model saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562,
          "referenced_widgets": [
            "36391478761f4fd5b44571b032f6c5c3",
            "545430afd07b402cb8d3e83f40dbab7d",
            "5eb890a3f5914ad69a1b3e607c843f6e",
            "15facfc92b774a3abd6360c160d509a8",
            "fb6d6f6dd0384da692b01cd9839ccd5d",
            "eeca2760cebd4c4bb60f134080dc9e65",
            "e284793fa72545faa726777637219a14",
            "8e9e4a3e34c247d3891fbf7af8fa35d1",
            "28ef3552a73a45b2acb0477c8e9e7f82",
            "48aeb49ac47747588562f133f01fd410",
            "af40d99f3b0f4d00a6c8fda30855c617",
            "fcafc064c9db4789bd1017bf55c3411c",
            "422ae5b225cc445e8b2ff4c205a0fd43",
            "7601bf32f5b44e25afbbe615ec4667ad",
            "a83c21532b1f4c51b4255bb7ca89dcf5",
            "52eb7efec91a4cdca1e2c0317d417211",
            "146bb0bb682a49a6a33cebd06039f35c",
            "f337d5d54e2f405ca67a72ce46446df7",
            "d2bfe5a4fe15450182c94f1b0861a935",
            "1671f1d229da42b698ecc5ef2ad587f2",
            "aa8e9e7d52db43b797ef540b16315a7e",
            "fdbbbf521ffe456c927e46b9c0877525",
            "7a4b89c5b1c44741adcc7cefbb40e8a4",
            "26308484177b420ab007cb3ea8da0f9c",
            "9ae24fc6cc954c1888d5d2e7c4deb86b",
            "fb3ac4dbc0404bb0a334c8c69b11411a",
            "0ad3a32d9ff543a2ac1790382c64306a",
            "4da0a5afeef1412183f94c851abc72c5",
            "fabe4dbbc5a34f5782e775d29eb6860b",
            "51918c4a22904c6391fe8ccbab2f2a0a",
            "05282e63009d451db8f9f7e2b21dd352",
            "58c74d88051748a2afacf2392cbd80bb",
            "11eb373da6e04312bb6823e44081b300"
          ]
        },
        "id": "0WpyzNPwM8wy",
        "outputId": "5f4f82aa-0742-4228-86c1-790d71cdacb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36391478761f4fd5b44571b032f6c5c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcafc064c9db4789bd1017bf55c3411c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a4b89c5b1c44741adcc7cefbb40e8a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- T5 Action Response Fine-tuning Started ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   0%|          | 0/804 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Epoch 1: 100%|██████████| 804/804 [00:54<00:00, 14.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss: 0.9641\n",
            "Epoch 1, Average Validation Loss: 0.0819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 804/804 [00:52<00:00, 15.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Training Loss: 0.0584\n",
            "Epoch 2, Average Validation Loss: 0.0265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 804/804 [00:52<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Training Loss: 0.0349\n",
            "Epoch 3, Average Validation Loss: 0.0225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 804/804 [00:53<00:00, 15.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Training Loss: 0.0299\n",
            "Epoch 4, Average Validation Loss: 0.0215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 804/804 [00:52<00:00, 15.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Training Loss: 0.0280\n",
            "Epoch 5, Average Validation Loss: 0.0205\n",
            "\n",
            "--- T5 Action Response Fine-tuning Finished ---\n",
            "\n",
            "Trained T5 model saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the trained T5 model and tokenizer\n",
        "tokenizer_t5 = T5TokenizerFast.from_pretrained('/content/drive/MyDrive/t5_wildfire_actions')\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/t5_wildfire_actions').to(device)\n",
        "model_t5.eval()\n",
        "\n",
        "def generate_action(tweet_text):\n",
        "    input_text = f\"What action should be taken for this tweet? {tweet_text}\"\n",
        "    input_ids = tokenizer_t5.encode(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
        "    outputs = model_t5.generate(input_ids=input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
        "    predicted_action = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
        "    return predicted_action\n",
        "\n",
        "# Define the T5 Dataset for the test set (moved here)\n",
        "class T5WildfireActionTestDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, source_len=128, target_len=32):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_len = source_len\n",
        "        self.target_len = target_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.data.iloc[idx]['input_text']\n",
        "        target_text = self.data.iloc[idx]['target_text']\n",
        "\n",
        "        source = self.tokenizer(\n",
        "            [input_text],\n",
        "            max_length=self.source_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        target = self.tokenizer(\n",
        "            [target_text],\n",
        "            max_length=self.target_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': source['input_ids'].flatten(),\n",
        "            'attention_mask': source['attention_mask'].flatten(),\n",
        "            'labels': target['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# Load the test dataset and create the test dataset\n",
        "try:\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/wildfire_test.csv')\n",
        "    t5_test_df = test_df.dropna(subset=['take_action']).copy()\n",
        "    t5_test_df['input_text'] = \"What action should be taken for this tweet? \" + t5_test_df['tweet_text']\n",
        "    t5_test_df['target_text'] = t5_test_df['take_action']\n",
        "    t5_test_df = t5_test_df[['input_text', 'target_text']]\n",
        "    test_t5_dataset = T5WildfireActionTestDataset(t5_test_df, tokenizer_t5)\n",
        "    print(\"Test dataset loaded and prepared.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Test dataset file not found. Please check the path.\")\n",
        "    test_t5_dataset = None\n",
        "\n",
        "def evaluate_and_print_examples(dataloader, dataset_name, num_examples=5):\n",
        "    print(f\"\\n--- Examples from {dataset_name} Set ---\")\n",
        "    examples_printed = 0\n",
        "    for batch in dataloader:\n",
        "        input_texts = [tokenizer_t5.decode(ids, skip_special_tokens=True) for ids in batch['input_ids']]\n",
        "        actual_actions = [tokenizer_t5.decode(ids, skip_special_tokens=True) for ids in batch['labels']]\n",
        "\n",
        "        for i in range(len(input_texts)):\n",
        "            tweet_text = input_texts[i].replace(\"What action should be taken for this tweet? \", \"\")\n",
        "            predicted_action = generate_action(tweet_text)\n",
        "            print(f\"\\nExample {examples_printed + 1}:\")\n",
        "            print(f\"  Tweet: {tweet_text}\")\n",
        "            print(f\"  Actual Action: {actual_actions[i]}\")\n",
        "            print(f\"  Predicted Action: {predicted_action}\")\n",
        "            examples_printed += 1\n",
        "            if examples_printed >= num_examples:\n",
        "                break\n",
        "        if examples_printed >= num_examples:\n",
        "            break\n",
        "\n",
        "# Create DataLoaders for train, val, and test sets\n",
        "train_t5_dataloader_eval = DataLoader(train_t5_dataset, batch_size=4, shuffle=False)\n",
        "val_t5_dataloader_eval = DataLoader(val_t5_dataset, batch_size=4, shuffle=False)\n",
        "test_t5_dataloader_eval = DataLoader(test_t5_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Print average losses (from previous runs)\n",
        "print(\"\\n--- Average Losses ---\")\n",
        "print(f\"Average Training Loss (from epoch 5): 0.0280\")\n",
        "print(f\"Average Validation Loss (from epoch 5): 0.0205\")\n",
        "print(f\"Average Test Loss: 0.0205\") # Using the value from the previous test run\n",
        "\n",
        "# Print examples from each set\n",
        "evaluate_and_print_examples(train_t5_dataloader_eval, \"Training\")\n",
        "evaluate_and_print_examples(val_t5_dataloader_eval, \"Validation\")\n",
        "evaluate_and_print_examples(test_t5_dataloader_eval, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa4pAfGFM80c",
        "outputId": "b032192e-df87-456a-e01a-7b4c14c15f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset loaded and prepared.\n",
            "\n",
            "--- Average Losses ---\n",
            "Average Training Loss (from epoch 5): 0.0280\n",
            "Average Validation Loss (from epoch 5): 0.0205\n",
            "Average Test Loss: 0.0205\n",
            "\n",
            "--- Examples from Training Set ---\n",
            "\n",
            "Example 1:\n",
            "  Tweet: first pics from # dominica 92since they lost contact due to hurricane maria\n",
            "  Actual Action: start missing person search\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 2:\n",
            "  Tweet: videos #dod assets help #caribbean region ravaged by #hurricanemaria\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 3:\n",
            "  Tweet: its starting to make more sense why harvey and irma were an organized test run planned evacuations\n",
            "  Actual Action: send evacuation and shelter support\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 4:\n",
            "  Tweet: halifax researchers using hurricane harvey irma as they work to understand massevacuations\n",
            "  Actual Action: send evacuation and shelter support\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 5:\n",
            "  Tweet: statesman james avery craftsman selling texas charm to help hurricane #harvey victims\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "--- Examples from Validation Set ---\n",
            "\n",
            "Example 1:\n",
            "  Tweet: the #earthquake killed more than 300 and wounded thousands in kurdish region in #iran amp #iraq #bbcnews\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 2:\n",
            "  Tweet: puerto rico governor fears humanitarian crisis in wake of hurricane maria world #cbc\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 3:\n",
            "  Tweet: helping pets burned and displaced by california wildfires\n",
            "  Actual Action: send evacuation and shelter support\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 4:\n",
            "  Tweet: uss abraham lincoln returns to #norfolk after helping with hurricane irma relief\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 5:\n",
            "  Tweet: #pakistani_news hurricane maria grows on fearsome irmas path #pakistan\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "--- Examples from Test Set ---\n",
            "\n",
            "Example 1:\n",
            "  Tweet: my bookcases damaged at base but fine upper can it be restored #help#restore #houston #harvey\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 2:\n",
            "  Tweet: lost weekend how trumps time at his golf club hurt the response to hurricane maria\n",
            "  Actual Action: start missing person search\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 3:\n",
            "  Tweet: #ashe members offer engineering support to puerto rico hospitals by\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 4:\n",
            "  Tweet: #iranearthquake a united nation rising to help #prayforiran #\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example 5:\n",
            "  Tweet: help the children of puerto rico\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import T5TokenizerFast\n",
        "\n",
        "# Load the original training dataset\n",
        "try:\n",
        "    train_df_original = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv')\n",
        "    print(\"Original training dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Original training dataset file not found. Please check the path.\")\n",
        "    train_df_original = None\n",
        "\n",
        "if train_df_original is not None:\n",
        "    # Filter out rows where 'take_action' is NaN for the T5 task\n",
        "    t5_train_df_filtered = train_df_original.dropna(subset=['take_action']).copy()\n",
        "    t5_train_df_filtered['input_text'] = \"What action should be taken for this tweet? \" + t5_train_df_filtered['tweet_text']\n",
        "    t5_train_df_filtered['target_text'] = t5_train_df_filtered['take_action']\n",
        "    t5_train_df_filtered = t5_train_df_filtered[['input_text', 'target_text']]\n",
        "\n",
        "    # Load the T5 tokenizer\n",
        "    tokenizer_t5 = T5TokenizerFast.from_pretrained('t5-small')\n",
        "\n",
        "    print(\"\\nPrepared filtered training data for T5 (excluding NaN actions).\")\n",
        "    print(f\"Number of filtered training examples for T5: {len(t5_train_df_filtered)}\")\n",
        "\n",
        "    # Do the same for the validation set\n",
        "    try:\n",
        "        val_df_original = pd.read_csv('/content/drive/MyDrive/wildfire_val.csv')\n",
        "        t5_val_df_filtered = val_df_original.dropna(subset=['take_action']).copy()\n",
        "        t5_val_df_filtered['input_text'] = \"What action should be taken for this tweet? \" + t5_val_df_filtered['tweet_text']\n",
        "        t5_val_df_filtered['target_text'] = t5_val_df_filtered['take_action']\n",
        "        t5_val_df_filtered = t5_val_df_filtered[['input_text', 'target_text']]\n",
        "        print(f\"Number of filtered validation examples for T5: {len(t5_val_df_filtered)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Validation dataset file not found.\")\n",
        "        t5_val_df_filtered = None\n",
        "\n",
        "else:\n",
        "    print(\"Original training data not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfeMkczOM88C",
        "outputId": "4849cdb2-becc-4612-fa99-a3136b70f76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training dataset loaded successfully.\n",
            "\n",
            "Prepared filtered training data for T5 (excluding NaN actions).\n",
            "Number of filtered training examples for T5: 12860\n",
            "Number of filtered validation examples for T5: 1608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the value counts of 'target_text' (which is 'take_action') in the filtered training data\n",
        "action_counts = t5_train_df_filtered['target_text'].value_counts()\n",
        "print(\"\\nAction Value Counts in Filtered Training Data:\")\n",
        "print(action_counts)\n",
        "\n",
        "# Calculate weights for each action\n",
        "total_samples = len(t5_train_df_filtered)\n",
        "class_weights = {}\n",
        "for action, count in action_counts.items():\n",
        "    weight = total_samples / (len(action_counts) * count)\n",
        "    class_weights[action] = weight\n",
        "\n",
        "print(\"\\nCalculated Class Weights:\")\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHJKymUTPOKq",
        "outputId": "b71015e5-95ed-4951-bae9-b0671762063e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Action Value Counts in Filtered Training Data:\n",
            "target_text\n",
            "monitor situation                      10064\n",
            "start missing person search              961\n",
            "send evacuation and shelter support      779\n",
            "send rescue team                         377\n",
            "send security and trauma support         356\n",
            "send medical team                        203\n",
            "send immediate help                      120\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Calculated Class Weights:\n",
            "{'monitor situation': 0.1825459913695208, 'start missing person search': 1.9116991229374163, 'send evacuation and shelter support': 2.358334861544104, 'send rescue team': 4.873057976506252, 'send security and trauma support': 5.160513643659711, 'send medical team': 9.049964813511611, 'send immediate help': 15.30952380952381}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, get_scheduler\n",
        "from torch.optim import AdamW  # Correct import for AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the T5 model\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer_t5 = AdamW(model_t5.parameters(), lr=3e-5, weight_decay=0.01) # Added weight decay for regularization\n",
        "num_epochs_t5 = 5\n",
        "num_training_steps_t5 = len(train_t5_dataloader_upsampled) * num_epochs_t5 # Use the upsampled dataloader length\n",
        "lr_scheduler_t5 = get_scheduler(\"linear\", optimizer=optimizer_t5, num_warmup_steps=0, num_training_steps=num_training_steps_t5)\n",
        "\n",
        "# Callback for Early Stopping\n",
        "class EarlyStoppingCallback:\n",
        "    def __init__(self, patience=3, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(patience=3)\n",
        "\n",
        "# Training Loop for T5 (using the upsampled dataloader)\n",
        "print(\"\\n--- T5 Action Response Fine-tuning Started (with Upsampled Data) ---\")\n",
        "for epoch in range(num_epochs_t5):\n",
        "    model_t5.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_t5_dataloader_upsampled, desc=f\"Epoch {epoch+1}\"): # Use train_t5_dataloader_upsampled\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model_t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_t5.step()\n",
        "        lr_scheduler_t5.step()\n",
        "        optimizer_t5.zero_grad()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_t5_dataloader_upsampled)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate on Validation Set\n",
        "    model_t5.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_t5_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model_t5(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_t5_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    early_stopping(avg_val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n--- T5 Action Response Fine-tuning Finished (with Upsampled Data) ---\")\n",
        "\n",
        "# Save the trained T5 model (you might want to save it with a new name to distinguish it)\n",
        "model_t5.save_pretrained('/content/drive/MyDrive/t5_wildfire_actions_upsampled')\n",
        "tokenizer_t5.save_pretrained('/content/drive/MyDrive/t5_wildfire_actions_upsampled')\n",
        "print(\"\\nTrained T5 model (with upsampled data) saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYy2nKtcPORp",
        "outputId": "5355c8b7-ecd1-4924-cbdb-af0273ebfef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- T5 Action Response Fine-tuning Started (with Upsampled Data) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1258/1258 [01:24<00:00, 14.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss: 0.7033\n",
            "Epoch 1, Average Validation Loss: 0.0241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1258/1258 [01:24<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Training Loss: 0.0459\n",
            "Epoch 2, Average Validation Loss: 0.0198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1258/1258 [01:24<00:00, 14.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Training Loss: 0.0337\n",
            "Epoch 3, Average Validation Loss: 0.0131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 1258/1258 [01:23<00:00, 15.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Training Loss: 0.0212\n",
            "Epoch 4, Average Validation Loss: 0.0080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 1258/1258 [01:24<00:00, 14.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Training Loss: 0.0163\n",
            "Epoch 5, Average Validation Loss: 0.0071\n",
            "\n",
            "--- T5 Action Response Fine-tuning Finished (with Upsampled Data) ---\n",
            "\n",
            "Trained T5 model (with upsampled data) saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the test dataset\n",
        "try:\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/wildfire_test.csv')\n",
        "    print(\"Test dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Test dataset file not found. Please check the path.\")\n",
        "    test_df = None\n",
        "\n",
        "if test_df is not None:\n",
        "    # Prepare test data for T5\n",
        "    t5_test_df = test_df.dropna(subset=['take_action']).copy()\n",
        "    t5_test_df['input_text'] = \"What action should be taken for this tweet? \" + t5_test_df['tweet_text']\n",
        "    t5_test_df['target_text'] = t5_test_df['take_action']\n",
        "    t5_test_df = t5_test_df[['input_text', 'target_text']]\n",
        "\n",
        "    # Load the T5 tokenizer (upsampled model)\n",
        "    tokenizer_t5 = T5TokenizerFast.from_pretrained('/content/drive/MyDrive/t5_wildfire_actions_upsampled')\n",
        "\n",
        "    # Define the T5 Dataset for the test set\n",
        "    class T5WildfireActionTestDataset(Dataset):\n",
        "        def __init__(self, dataframe, tokenizer, source_len=128, target_len=32):\n",
        "            self.data = dataframe\n",
        "            self.tokenizer = tokenizer\n",
        "            self.source_len = source_len\n",
        "            self.target_len = target_len\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            input_text = self.data.iloc[idx]['input_text']\n",
        "            target_text = self.data.iloc[idx]['target_text']\n",
        "\n",
        "            source = self.tokenizer(\n",
        "                [input_text],\n",
        "                max_length=self.source_len,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            target = self.tokenizer(\n",
        "                [target_text],\n",
        "                max_length=self.target_len,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'input_ids': source['input_ids'].flatten(),\n",
        "                'attention_mask': source['attention_mask'].flatten(),\n",
        "                'labels': target['input_ids'].flatten()\n",
        "            }\n",
        "\n",
        "    # Create T5 Test DataLoader\n",
        "    test_t5_dataset = T5WildfireActionTestDataset(t5_test_df, tokenizer_t5)\n",
        "    test_t5_dataloader = DataLoader(test_t5_dataset, batch_size=16)\n",
        "\n",
        "    # Load the trained T5 model (upsampled model)\n",
        "    model_t5_upsampled = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/t5_wildfire_actions_upsampled').to(device)\n",
        "    model_t5_upsampled.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_t5_dataloader, desc=\"Evaluating on Test Set (Upsampled Model)\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model_t5_upsampled(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_t5_dataloader)\n",
        "    print(f\"\\nAverage Test Loss (Upsampled Model): {avg_test_loss:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Test data not available, cannot evaluate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJSDbaNtRzgN",
        "outputId": "079716aa-3344-4218-fb06-ec2149fb7998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Test Set (Upsampled Model): 100%|██████████| 101/101 [00:03<00:00, 33.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Test Loss (Upsampled Model): 0.0068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the trained T5 model and tokenizer (upsampled)\n",
        "tokenizer_t5 = T5TokenizerFast.from_pretrained('/content/drive/MyDrive/t5_wildfire_actions_upsampled')\n",
        "model_t5_upsampled = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/t5_wildfire_actions_upsampled').to(device)\n",
        "model_t5_upsampled.eval()\n",
        "\n",
        "def generate_action(tweet_text):\n",
        "    input_text = f\"What action should be taken for this tweet? {tweet_text}\"\n",
        "    input_ids = tokenizer_t5.encode(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
        "    outputs = model_t5_upsampled.generate(input_ids=input_ids, max_length=32, num_beams=4, early_stopping=True)\n",
        "    predicted_action = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
        "    return predicted_action\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action'])\n",
        "    val_df = pd.read_csv('/content/drive/MyDrive/wildfire_val.csv').dropna(subset=['take_action'])\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/wildfire_test.csv').dropna(subset=['take_action'])\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or more dataset files not found.\")\n",
        "    train_df = val_df = test_df = None\n",
        "\n",
        "if train_df is not None and val_df is not None and test_df is not None:\n",
        "    def evaluate_and_print_examples(dataframe, dataset_name, num_examples=5):\n",
        "        print(f\"\\n--- Examples from {dataset_name} Set (Upsampled Model) ---\")\n",
        "        for index, row in dataframe.sample(num_examples, random_state=42).iterrows():\n",
        "            tweet_text = row['tweet_text']\n",
        "            actual_action = row['take_action']\n",
        "            predicted_action = generate_action(tweet_text)\n",
        "            print(f\"\\nExample:\")\n",
        "            print(f\"  Tweet: {tweet_text}\")\n",
        "            print(f\"  Actual Action: {actual_action}\")\n",
        "            print(f\"  Predicted Action: {predicted_action}\")\n",
        "\n",
        "    evaluate_and_print_examples(train_df, \"Training\")\n",
        "    evaluate_and_print_examples(val_df, \"Validation\")\n",
        "    evaluate_and_print_examples(test_df, \"Test\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not load datasets for generating examples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYR6CLsWRzkX",
        "outputId": "7fa770c1-6371-4f19-a821-05c0a9534a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Examples from Training Set (Upsampled Model) ---\n",
            "\n",
            "Example:\n",
            "  Tweet: tesla shows off renewable energy project at childrens hospital in puerto rico #mashabletech\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example:\n",
            "  Tweet: irans deadly earthquake toll rises to 600 killed 9388 injured in #kermanshah amp #serpolrzehab kurdistan\n",
            "  Actual Action: send medical team\n",
            "  Predicted Action: send medical team\n",
            "\n",
            "Example:\n",
            "  Tweet: mt #scihelptx database is available to scientists affected by #irma amp #jose\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example:\n",
            "  Tweet: mayor turner asks volunteers to track hours helping in harvey relief efforts\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example:\n",
            "  Tweet: helping return 500 beaumont area harvey evacuees from dallas back to southeast texas today\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: send evacuation and shelter support\n",
            "\n",
            "--- Examples from Validation Set (Upsampled Model) ---\n",
            "\n",
            "Example:\n",
            "  Tweet: us navy releases photos of evacuation efforts for us citizens on dominica following devastation from hurricane maria\n",
            "  Actual Action: send evacuation and shelter support\n",
            "  Predicted Action: send evacuation and shelter support\n",
            "\n",
            "Example:\n",
            "  Tweet: puerto rico asks elon musk for teslas help solving energy woes\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example:\n",
            "  Tweet: gofundme campaign for friends family in st martin wholike so manylost everything to irma\n",
            "  Actual Action: start missing person search\n",
            "  Predicted Action: start missing person search\n",
            "\n",
            "Example:\n",
            "  Tweet: uil eligibility of students displaced by #hurricaneharvey press release\n",
            "  Actual Action: send evacuation and shelter support\n",
            "  Predicted Action: send evacuation and shelter support\n",
            "\n",
            "Example:\n",
            "  Tweet: 15th sos performs quick turn evacuates medical students after hurricane maria\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "--- Examples from Test Set (Upsampled Model) ---\n",
            "\n",
            "Example:\n",
            "  Tweet: puerto rico death toll 450 69 missing trump is a total disgrace and embarrassment to usa insanity\n",
            "  Actual Action: start missing person search\n",
            "  Predicted Action: start missing person search\n",
            "\n",
            "Example:\n",
            "  Tweet: iraniraq border earthquake latest at least 211 killed over 2500 injured\n",
            "  Actual Action: send medical team\n",
            "  Predicted Action: send medical team\n",
            "\n",
            "Example:\n",
            "  Tweet: bridgeport family mourns relatives lost to hurricane maria\n",
            "  Actual Action: start missing person search\n",
            "  Predicted Action: start missing person search\n",
            "\n",
            "Example:\n",
            "  Tweet: proud of my second period selflessly giving to help those in need in puerto rico #thetealisreal\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n",
            "\n",
            "Example:\n",
            "  Tweet: crew from electric returning home today after helping out at #irma\n",
            "  Actual Action: monitor situation\n",
            "  Predicted Action: monitor situation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training dataset (with non-NaN 'take_action')\n",
        "try:\n",
        "    train_df_filtered = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action'])\n",
        "    print(\"Filtered training dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Filtered training dataset not found.\")\n",
        "    train_df_filtered = None\n",
        "\n",
        "if train_df_filtered is not None:\n",
        "    unique_actions = train_df_filtered['take_action'].unique()\n",
        "    print(\"\\nUnique Actions in Training Data:\")\n",
        "    for action in unique_actions:\n",
        "        print(f\"- {action}\")\n",
        "else:\n",
        "    print(\"Training data not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsG3pu_oShb7",
        "outputId": "e541e02f-435b-4ae0-8c6a-d53f4f252019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered training dataset loaded successfully.\n",
            "\n",
            "Unique Actions in Training Data:\n",
            "- start missing person search\n",
            "- monitor situation\n",
            "- send evacuation and shelter support\n",
            "- send rescue team\n",
            "- send security and trauma support\n",
            "- send medical team\n",
            "- send immediate help\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "from torch.optim import AdamW  # Correct import for AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation and shelter support',\n",
        "                     'send rescue team', 'send security and trauma support', 'send medical team',\n",
        "                     'send immediate help']\n",
        "num_labels = len(action_categories)\n",
        "\n",
        "# Load the pre-trained DistilBERT model\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Unfreeze all layers for fine-tuning\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Add a multi-label classification layer on top\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, base_model, num_labels, dropout=0.1):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        probabilities = self.sigmoid(logits)\n",
        "        return probabilities\n",
        "\n",
        "model_classifier = MultiLabelClassifier(model, num_labels).to(device)\n",
        "\n",
        "# Optimizer with a smaller learning rate\n",
        "optimizer = AdamW(model_classifier.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Loss function (Binary Cross-Entropy)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training loop (assuming train_dataloader and val_dataloader are already defined)\n",
        "num_epochs = 5\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1) # Example scheduler\n",
        "\n",
        "print(\"\\n--- Multi-Label Classification Training Started (Unfrozen Layers, Smaller LR) ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model_classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n--- Multi-Label Classification Training Finished (Unfrozen Layers, Smaller LR) ---\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model_classifier.state_dict(), '/content/drive/MyDrive/distilbert_multi_label_unfrozen.pth')\n",
        "print(\"\\nTrained multi-label model (unfrozen, smaller LR) saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjSlJA6HSI_Q",
        "outputId": "6da8f76b-8b95-4094-a6f6-7bf8269106a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Multi-Label Classification Training Started (Unfrozen Layers, Smaller LR) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 804/804 [00:17<00:00, 47.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss: 0.6828\n",
            "Epoch 1, Average Validation Loss: 0.6813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 804/804 [00:16<00:00, 47.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Training Loss: 0.6828\n",
            "Epoch 2, Average Validation Loss: 0.6813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 804/804 [00:17<00:00, 47.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Training Loss: 0.6825\n",
            "Epoch 3, Average Validation Loss: 0.6813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 804/804 [00:17<00:00, 47.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Training Loss: 0.6826\n",
            "Epoch 4, Average Validation Loss: 0.6813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 804/804 [00:16<00:00, 47.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Training Loss: 0.6826\n",
            "Epoch 5, Average Validation Loss: 0.6813\n",
            "\n",
            "--- Multi-Label Classification Training Finished (Unfrozen Layers, Smaller LR) ---\n",
            "\n",
            "Trained multi-label model (unfrozen, smaller LR) saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the action categories, including the combined one\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation and shelter support',\n",
        "                     'send rescue team', 'send security and trauma support', 'send medical team',\n",
        "                     'send immediate help']\n",
        "\n",
        "def create_multi_label(df, categories):\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        row_labels = [0.0] * len(categories)\n",
        "        actions = [act.strip() for act in row['take_action'].split(' and ')]\n",
        "        for i, cat in enumerate(categories):\n",
        "            if cat in actions:\n",
        "                row_labels[i] = 1.0\n",
        "        labels.append(row_labels)\n",
        "    return labels\n",
        "\n",
        "# Load the datasets (with non-NaN 'take_action')\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    val_df = pd.read_csv('/content/drive/MyDrive/wildfire_val.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/wildfire_test.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    print(\"Datasets loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or more dataset files not found.\")\n",
        "    train_df = val_df = test_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # Create multi-hot labels\n",
        "    train_labels = create_multi_label(train_df, action_categories)\n",
        "    val_labels = create_multi_label(val_df, action_categories)\n",
        "    test_labels = create_multi_label(test_df, action_categories)\n",
        "\n",
        "    # Tokenize the text using DistilBERT\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "    train_encodings = tokenizer(train_df['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_df['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test_df['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    # Create PyTorch Datasets\n",
        "    class WildfireActionDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = WildfireActionDataset(train_encodings, train_labels)\n",
        "    val_dataset = WildfireActionDataset(val_encodings, val_labels)\n",
        "    test_dataset = WildfireActionDataset(test_encodings, test_labels)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "    print(\"\\nData preprocessed for multi-label classification (handling combined actions).\")\n",
        "    print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "    print(f\"Number of validation examples: {len(val_dataset)}\")\n",
        "    print(f\"Number of testing examples: {len(test_dataset)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not proceed with multi-label preprocessing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiZy_MMlWUhj",
        "outputId": "816caef4-0852-41aa-fb86-4869424ce6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully.\n",
            "\n",
            "Data preprocessed for multi-label classification (handling combined actions).\n",
            "Number of training examples: 12860\n",
            "Number of validation examples: 1608\n",
            "Number of testing examples: 1607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation and shelter support',\n",
        "                     'send rescue team', 'send security and trauma support', 'send medical team',\n",
        "                     'send immediate help']\n",
        "\n",
        "def create_multi_label(df, categories):\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        row_labels = [0.0] * len(categories)\n",
        "        actions = [act.strip() for act in row['take_action'].split(' and ')]\n",
        "        for i, cat in enumerate(categories):\n",
        "            if cat in actions:\n",
        "                row_labels[i] = 1.0\n",
        "        labels.append(row_labels)\n",
        "    return labels\n",
        "\n",
        "# Load the training dataset (with non-NaN 'take_action')\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    print(\"Filtered training dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Filtered training dataset not found.\")\n",
        "    train_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    train_labels = create_multi_label(train_df, action_categories)\n",
        "\n",
        "    print(\"\\n--- Sample of More Labels ---\")\n",
        "    for i in range(10):\n",
        "        print(f\"Original Take Action: {train_df['take_action'][i]}\")\n",
        "        print(f\"Labels: {train_labels[i]}\")\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"Training data not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsIJfK31WUou",
        "outputId": "ab5c9c6c-8b90-4337-c7b9-95b1b1129be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered training dataset loaded successfully.\n",
            "\n",
            "--- Sample of More Labels ---\n",
            "Original Take Action: start missing person search\n",
            "Labels: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: send evacuation and shelter support\n",
            "Labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: send evacuation and shelter support\n",
            "Labels: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n",
            "Original Take Action: monitor situation\n",
            "Labels: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Revised action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation',\n",
        "                     'shelter support', 'send rescue team', 'send security',\n",
        "                     'trauma support', 'send medical team', 'send immediate help']\n",
        "\n",
        "def create_multi_label(df, categories):\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        row_labels = [0.0] * len(categories)\n",
        "        take_action_text = row['take_action']\n",
        "        if isinstance(take_action_text, str):\n",
        "            actions = [act.strip() for act in take_action_text.split(' and ')]\n",
        "            for i, cat in enumerate(categories):\n",
        "                if cat.strip() in actions:\n",
        "                    row_labels[i] = 1.0\n",
        "        labels.append(row_labels)\n",
        "    return labels\n",
        "\n",
        "# Load the datasets (with non-NaN 'take_action')\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    val_df = pd.read_csv('/content/drive/MyDrive/wildfire_val.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/wildfire_test.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    print(\"Datasets loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or more dataset files not found.\")\n",
        "    train_df = val_df = test_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # Create multi-hot labels\n",
        "    train_labels = create_multi_label(train_df, action_categories)\n",
        "    val_labels = create_multi_label(val_df, action_categories)\n",
        "    test_labels = create_multi_label(test_df, action_categories)\n",
        "\n",
        "    # Tokenize the text using DistilBERT\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "    train_encodings = tokenizer(train_df['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_df['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test_df['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    # Create PyTorch Datasets\n",
        "    class WildfireActionDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = WildfireActionDataset(train_encodings, train_labels)\n",
        "    val_dataset = WildfireActionDataset(val_encodings, val_labels)\n",
        "    test_dataset = WildfireActionDataset(test_encodings, test_labels)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "    print(\"\\nData preprocessed for multi-label classification (revised categories).\")\n",
        "    print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "    print(f\"Number of validation examples: {len(val_dataset)}\")\n",
        "    print(f\"Number of testing examples: {len(test_dataset)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not proceed with multi-label preprocessing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbb-XdgsWUwT",
        "outputId": "5697d65a-0f15-4a41-bd85-9bebc2d1e173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully.\n",
            "\n",
            "Data preprocessed for multi-label classification (revised categories).\n",
            "Number of training examples: 12860\n",
            "Number of validation examples: 1608\n",
            "Number of testing examples: 1607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load the training dataset (with non-NaN 'take_action')\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    print(\"Filtered training dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Filtered training dataset not found.\")\n",
        "    train_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # Combine all 'take_action' entries into a single list\n",
        "    all_actions = []\n",
        "    for actions_str in train_df['take_action']:\n",
        "        if isinstance(actions_str, str):\n",
        "            split_actions = [act.strip() for act in actions_str.lower().split(' and ')]\n",
        "            all_actions.extend(split_actions)\n",
        "\n",
        "    # Count the frequency of each action phrase\n",
        "    action_counts = Counter(all_actions)\n",
        "\n",
        "    print(\"\\n--- Frequency of Individual Action Phrases (Lowercase) ---\")\n",
        "    for action, count in action_counts.most_common(50):  # Display the top 50\n",
        "        print(f\"{action}: {count}\")\n",
        "\n",
        "    print(\"\\n--- Examples of 'take_action' Entries ---\")\n",
        "    print(train_df['take_action'].head(20))\n",
        "\n",
        "    print(\"\\n--- Identifying Potential Variations (Manual Inspection of a Few Examples) ---\")\n",
        "    for index, row in train_df.head(20).iterrows():\n",
        "        print(f\"\\nOriginal: {row['take_action']}\")\n",
        "        print(f\"Lowercase & Split: {[act.strip() for act in row['take_action'].lower().split(' and ')]}\")\n",
        "\n",
        "else:\n",
        "    print(\"Training data not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-evZ3MHbPqA",
        "outputId": "8768131e-6993-45cc-ff06-6f30f3b0eaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered training dataset loaded successfully.\n",
            "\n",
            "--- Frequency of Individual Action Phrases (Lowercase) ---\n",
            "monitor situation: 10064\n",
            "start missing person search: 961\n",
            "send evacuation: 779\n",
            "shelter support: 779\n",
            "send rescue team: 377\n",
            "send security: 356\n",
            "trauma support: 356\n",
            "send medical team: 203\n",
            "send immediate help: 120\n",
            "\n",
            "--- Examples of 'take_action' Entries ---\n",
            "0             start missing person search\n",
            "1                       monitor situation\n",
            "2     send evacuation and shelter support\n",
            "3     send evacuation and shelter support\n",
            "4                       monitor situation\n",
            "5                       monitor situation\n",
            "6                       monitor situation\n",
            "7                       monitor situation\n",
            "8                       monitor situation\n",
            "9                       monitor situation\n",
            "10                      monitor situation\n",
            "11            start missing person search\n",
            "12                      monitor situation\n",
            "13                      monitor situation\n",
            "14                      monitor situation\n",
            "15                      monitor situation\n",
            "16    send evacuation and shelter support\n",
            "17            start missing person search\n",
            "18                       send rescue team\n",
            "19                      monitor situation\n",
            "Name: take_action, dtype: object\n",
            "\n",
            "--- Identifying Potential Variations (Manual Inspection of a Few Examples) ---\n",
            "\n",
            "Original: start missing person search\n",
            "Lowercase & Split: ['start missing person search']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: send evacuation and shelter support\n",
            "Lowercase & Split: ['send evacuation', 'shelter support']\n",
            "\n",
            "Original: send evacuation and shelter support\n",
            "Lowercase & Split: ['send evacuation', 'shelter support']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: start missing person search\n",
            "Lowercase & Split: ['start missing person search']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n",
            "\n",
            "Original: send evacuation and shelter support\n",
            "Lowercase & Split: ['send evacuation', 'shelter support']\n",
            "\n",
            "Original: start missing person search\n",
            "Lowercase & Split: ['start missing person search']\n",
            "\n",
            "Original: send rescue team\n",
            "Lowercase & Split: ['send rescue team']\n",
            "\n",
            "Original: monitor situation\n",
            "Lowercase & Split: ['monitor situation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "# Load the training dataset (with non-NaN 'take_action')\n",
        "try:\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/wildfire_train.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "    print(\"Filtered training dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Filtered training dataset not found.\")\n",
        "    train_df = None\n",
        "\n",
        "if train_df is not None:\n",
        "    # Combine all 'take_action' entries into a single list (lowercase and split)\n",
        "    all_actions = []\n",
        "    for actions_str in train_df['take_action']:\n",
        "        if isinstance(actions_str, str):\n",
        "            split_actions = [act.strip() for act in actions_str.lower().split(' and ')]\n",
        "            all_actions.extend(split_actions)\n",
        "\n",
        "    # Count the frequency of each action phrase\n",
        "    action_counts = Counter(all_actions)\n",
        "\n",
        "    # Revised action categories\n",
        "    action_categories = ['start missing person search', 'monitor situation', 'send evacuation',\n",
        "                         'shelter support', 'send rescue team', 'send security',\n",
        "                         'trauma support', 'send medical team', 'send immediate help']\n",
        "\n",
        "    # Calculate weights for each class (inverse frequency)\n",
        "    total_samples = len(all_actions)\n",
        "    class_weights = []\n",
        "    for category in action_categories:\n",
        "        count = action_counts.get(category, 1)  # Avoid division by zero\n",
        "        weight = total_samples / (count + 1e-6)  # Adding a small epsilon for stability\n",
        "        class_weights.append(weight)\n",
        "\n",
        "    # Convert to a PyTorch tensor\n",
        "    class_weights_tensor = torch.tensor(class_weights).to(device)\n",
        "\n",
        "    print(\"\\n--- Class Weights ---\")\n",
        "    for category, weight in zip(action_categories, class_weights_tensor.cpu().numpy()):\n",
        "        print(f\"{category}: {weight:.2f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Training data not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdFFiQKObkGw",
        "outputId": "b2a424ee-c98d-4fc7-a737-7056f3cc79e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered training dataset loaded successfully.\n",
            "\n",
            "--- Class Weights ---\n",
            "start missing person search: 14.56\n",
            "monitor situation: 1.39\n",
            "send evacuation: 17.97\n",
            "shelter support: 17.97\n",
            "send rescue team: 37.12\n",
            "send security: 39.31\n",
            "trauma support: 39.31\n",
            "send medical team: 68.94\n",
            "send immediate help: 116.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel\n",
        "from torch.optim import AdamW  # Correct import for AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Revised action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation',\n",
        "                     'shelter support', 'send rescue team', 'send security',\n",
        "                     'trauma support', 'send medical team', 'send immediate help']\n",
        "num_labels = len(action_categories)\n",
        "\n",
        "# Calculate class weights (same as before)\n",
        "class_weights_dict = {'start missing person search': 14.56, 'monitor situation': 1.39, 'send evacuation': 17.97,\n",
        "                      'shelter support': 17.97, 'send rescue team': 37.12, 'send security': 39.31,\n",
        "                      'trauma support': 39.31, 'send medical team': 68.94, 'send immediate help': 116.62}\n",
        "class_weights_tensor = torch.tensor([class_weights_dict[cat] for cat in action_categories]).to(device)\n",
        "\n",
        "# Define Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * bce_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            focal_loss = self.alpha * targets * focal_loss + (1 - self.alpha) * (1 - targets) * focal_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return torch.mean(focal_loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            return torch.sum(focal_loss)\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Load the pre-trained DistilBERT model\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Unfreeze all layers for fine-tuning\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Update the multi-label classification layer\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, base_model, num_labels, dropout=0.1):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits  # Return logits for Focal Loss\n",
        "\n",
        "model_classifier = MultiLabelClassifier(model, num_labels).to(device)\n",
        "\n",
        "# Optimizer with a learning rate of 2e-5\n",
        "optimizer = AdamW(model_classifier.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Loss function: Focal Loss (we can also try combining with class weights)\n",
        "criterion = FocalLoss(gamma=2, reduction='mean') # You can experiment with alpha\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "\n",
        "# Training loop (increased epochs)\n",
        "num_epochs = 15\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "print(f\"\\n--- Multi-Label Classification Training Started (Focal Loss, {num_labels} labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model_classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Step the scheduler based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "print(f\"\\n--- Multi-Label Classification Training Finished (Focal Loss, {num_labels} labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model_classifier.state_dict(), f'/content/drive/MyDrive/distilbert_multi_label_focal_lr2e5_{num_labels}labels_15epochs.pth')\n",
        "print(f\"\\nTrained multi-label model (with Focal Loss, {num_labels} labels, unfrozen, LR=2e-5, 15 epochs) saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z7cVb7kdppk",
        "outputId": "68858888-672f-49a7-ddd1-1d11faaf4a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Multi-Label Classification Training Started (Focal Loss, 9 labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 804/804 [00:17<00:00, 46.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss: 0.0106\n",
            "Epoch 1, Average Validation Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 804/804 [00:17<00:00, 47.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Training Loss: 0.0002\n",
            "Epoch 2, Average Validation Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 804/804 [00:17<00:00, 47.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Training Loss: 0.0006\n",
            "Epoch 3, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 804/804 [00:17<00:00, 47.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Training Loss: 0.0001\n",
            "Epoch 4, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 804/804 [00:17<00:00, 47.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Training Loss: 0.0000\n",
            "Epoch 5, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 804/804 [00:17<00:00, 47.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Training Loss: 0.0000\n",
            "Epoch 6, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 804/804 [00:17<00:00, 47.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Training Loss: 0.0005\n",
            "Epoch 7, Average Validation Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 804/804 [00:17<00:00, 47.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Training Loss: 0.0001\n",
            "Epoch 8, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 804/804 [00:17<00:00, 47.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Training Loss: 0.0000\n",
            "Epoch 9, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 804/804 [00:17<00:00, 47.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Training Loss: 0.0002\n",
            "Epoch 10, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 804/804 [00:17<00:00, 47.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Average Training Loss: 0.0000\n",
            "Epoch 11, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 804/804 [00:17<00:00, 47.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Average Training Loss: 0.0000\n",
            "Epoch 12, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 804/804 [00:17<00:00, 47.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Average Training Loss: 0.0000\n",
            "Epoch 13, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 804/804 [00:17<00:00, 47.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Average Training Loss: 0.0000\n",
            "Epoch 14, Average Validation Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 804/804 [00:17<00:00, 47.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Average Training Loss: 0.0000\n",
            "Epoch 15, Average Validation Loss: 0.0000\n",
            "\n",
            "--- Multi-Label Classification Training Finished (Focal Loss, 9 labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\n",
            "\n",
            "Trained multi-label model (with Focal Loss, 9 labels, unfrozen, LR=2e-5, 15 epochs) saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keep\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Revised action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation',\n",
        "                     'shelter support', 'send rescue team', 'send security',\n",
        "                     'trauma support', 'send medical team', 'send immediate help']\n",
        "\n",
        "def create_multi_label_array(df, categories):\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        row_labels = np.zeros(len(categories))\n",
        "        actions = [act.strip() for act in row['take_action'].lower().split(' and ')]\n",
        "        for i, cat in enumerate(categories):\n",
        "            if cat in actions:\n",
        "                row_labels[i] = 1.0\n",
        "        labels.append(row_labels)\n",
        "    return np.array(labels)\n",
        "\n",
        "# Load the new datasets\n",
        "train_df_new = pd.read_csv('/content/drive/MyDrive/wildfire_train_new.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "val_df_new = pd.read_csv('/content/drive/MyDrive/wildfire_val_new.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "test_df_new = pd.read_csv('/content/drive/MyDrive/wildfire_test_new.csv').dropna(subset=['take_action']).reset_index(drop=True)\n",
        "\n",
        "# Create multi-label arrays\n",
        "train_labels_new = create_multi_label_array(train_df_new, action_categories)\n",
        "val_labels_new = create_multi_label_array(val_df_new, action_categories)\n",
        "test_labels_new = create_multi_label_array(test_df_new, action_categories)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings_new = tokenizer(train_df_new['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "val_encodings_new = tokenizer(val_df_new['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "test_encodings_new = tokenizer(test_df_new['tweet_text'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Create PyTorch Datasets\n",
        "class WildfireActionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset_new = WildfireActionDataset(train_encodings_new, train_labels_new)\n",
        "val_dataset_new = WildfireActionDataset(val_encodings_new, val_labels_new)\n",
        "test_dataset_new = WildfireActionDataset(test_encodings_new, test_labels_new)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader_new = DataLoader(train_dataset_new, batch_size=16, shuffle=True)\n",
        "val_dataloader_new = DataLoader(val_dataset_new, batch_size=16)\n",
        "test_dataloader_new = DataLoader(test_dataset_new, batch_size=16)\n",
        "\n",
        "print(\"\\nNew datasets preprocessed and DataLoaders created.\")\n",
        "print(f\"Number of training examples (new): {len(train_dataset_new)}\")\n",
        "print(f\"Number of validation examples (new): {len(val_dataset_new)}\")\n",
        "print(f\"Number of testing examples (new): {len(test_dataset_new)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR_w2WQAdqQY",
        "outputId": "4127e264-f069-4535-daa2-71cd9ef5f714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New datasets preprocessed and DataLoaders created.\n",
            "Number of training examples (new): 8994\n",
            "Number of validation examples (new): 1824\n",
            "Number of testing examples (new): 2042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#keep\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Revised action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation',\n",
        "                     'shelter support', 'send rescue team', 'send security',\n",
        "                     'trauma support', 'send medical team', 'send immediate help']\n",
        "num_labels = len(action_categories)\n",
        "\n",
        "# Calculate class weights based on the new training data\n",
        "train_labels_new_df = pd.DataFrame(train_labels_new, columns=action_categories)\n",
        "class_counts_new = train_labels_new_df.sum(axis=0)\n",
        "total_samples_new = len(train_labels_new)\n",
        "class_weights_dict_new = {cat: total_samples_new / (count + 1e-6) for cat, count in class_counts_new.items()}\n",
        "class_weights_tensor_new = torch.tensor([class_weights_dict_new[cat] for cat in action_categories]).to(device)\n",
        "\n",
        "# Load the pre-trained DistilBERT model\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Unfreeze all layers for fine-tuning\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Update the multi-label classification layer\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, base_model, num_labels, dropout=0.1):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        probabilities = self.sigmoid(logits)\n",
        "        return probabilities\n",
        "\n",
        "model_classifier = MultiLabelClassifier(model, num_labels).to(device)\n",
        "\n",
        "# Optimizer with a learning rate of 2e-5\n",
        "optimizer = AdamW(model_classifier.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Loss function with class weights (using weights from the new training data)\n",
        "criterion = nn.BCELoss(weight=class_weights_tensor_new)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "\n",
        "# Training loop (increased epochs)\n",
        "num_epochs = 15\n",
        "total_steps = len(train_dataloader_new) * num_epochs\n",
        "\n",
        "print(f\"\\n--- Multi-Label Classification Training Started (New Split, With Class Weights, {num_labels} labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader_new, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader_new)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on validation set (new)\n",
        "    model_classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader_new:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader_new)\n",
        "    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Step the scheduler based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "print(f\"\\n--- Multi-Label Classification Training Finished (New Split, With Class Weights, {num_labels} labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model_classifier.state_dict(), f'/content/drive/MyDrive/distilbert_multi_label_newsplit_weighted_lr2e5_{num_labels}labels_15epochs.pth')\n",
        "print(f\"\\nTrained multi-label model (new split, with class weights, {num_labels} labels, unfrozen, LR=2e-5, 15 epochs) saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD4QxI-kdqYV",
        "outputId": "dbe3c6b6-0dda-4132-dd65-e9631775572d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Multi-Label Classification Training Started (New Split, With Class Weights, 9 labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 563/563 [00:11<00:00, 49.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss: 2.1285\n",
            "Epoch 1, Average Validation Loss: 0.6238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 563/563 [00:11<00:00, 50.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Training Loss: 0.2934\n",
            "Epoch 2, Average Validation Loss: 0.3621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 563/563 [00:11<00:00, 50.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Training Loss: 0.1529\n",
            "Epoch 3, Average Validation Loss: 0.3356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 563/563 [00:11<00:00, 50.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Training Loss: 0.0983\n",
            "Epoch 4, Average Validation Loss: 0.3178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 563/563 [00:11<00:00, 50.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Training Loss: 0.0682\n",
            "Epoch 5, Average Validation Loss: 0.3188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 563/563 [00:11<00:00, 50.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Training Loss: 0.0489\n",
            "Epoch 6, Average Validation Loss: 0.3104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 563/563 [00:11<00:00, 50.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Training Loss: 0.0360\n",
            "Epoch 7, Average Validation Loss: 0.0704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 563/563 [00:11<00:00, 50.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Training Loss: 0.0266\n",
            "Epoch 8, Average Validation Loss: 0.0674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 563/563 [00:11<00:00, 50.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Training Loss: 0.0197\n",
            "Epoch 9, Average Validation Loss: 0.0652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 563/563 [00:11<00:00, 50.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Training Loss: 0.0148\n",
            "Epoch 10, Average Validation Loss: 0.0661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 563/563 [00:11<00:00, 49.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Average Training Loss: 0.0110\n",
            "Epoch 11, Average Validation Loss: 0.0731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 563/563 [00:11<00:00, 50.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Average Training Loss: 0.0083\n",
            "Epoch 12, Average Validation Loss: 0.0742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 563/563 [00:11<00:00, 50.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Average Training Loss: 0.0070\n",
            "Epoch 13, Average Validation Loss: 0.0742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 563/563 [00:11<00:00, 50.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Average Training Loss: 0.0068\n",
            "Epoch 14, Average Validation Loss: 0.0750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 563/563 [00:11<00:00, 50.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Average Training Loss: 0.0065\n",
            "Epoch 15, Average Validation Loss: 0.0797\n",
            "\n",
            "--- Multi-Label Classification Training Finished (New Split, With Class Weights, 9 labels, Unfrozen Layers, LR=2e-5, 15 Epochs, ReduceLROnPlateau) ---\n",
            "\n",
            "Trained multi-label model (new split, with class weights, 9 labels, unfrozen, LR=2e-5, 15 epochs) saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Select the tweet text and distress labels\n",
        "X_distress = california_wildfire_df['tweet_text'].tolist()\n",
        "y_distress = california_wildfire_df['distress'].tolist()\n",
        "\n",
        "# Split the data into training+validation+test and test sets (90/10)\n",
        "train_val_texts_distress, test_texts_distress, train_val_labels_distress, test_labels_distress = train_test_split(\n",
        "    X_distress, y_distress, test_size=0.1, random_state=42, stratify=y_distress\n",
        ")\n",
        "\n",
        "# Split the training+validation set into training and validation sets (80/10 of the 90%, so 80/10/10 split overall)\n",
        "train_texts_distress, val_texts_distress, train_labels_distress, val_labels_distress = train_test_split(\n",
        "    train_val_texts_distress, train_val_labels_distress, test_size=(1/9), random_state=42, stratify=train_val_labels_distress\n",
        ")\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer_distress = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "train_encodings_distress = tokenizer_distress(train_texts_distress, truncation=True, padding=True)\n",
        "val_encodings_distress = tokenizer_distress(val_texts_distress, truncation=True, padding=True)\n",
        "test_encodings_distress = tokenizer_distress(test_texts_distress, truncation=True, padding=True)\n",
        "\n",
        "# Create PyTorch Datasets\n",
        "class DistressDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset_distress = DistressDataset(train_encodings_distress, train_labels_distress)\n",
        "val_dataset_distress = DistressDataset(val_encodings_distress, val_labels_distress)\n",
        "test_dataset_distress = DistressDataset(test_encodings_distress, test_labels_distress)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader_distress = DataLoader(train_dataset_distress, batch_size=16, shuffle=True)\n",
        "val_dataloader_distress = DataLoader(val_dataset_distress, batch_size=16)\n",
        "test_dataloader_distress = DataLoader(test_dataset_distress, batch_size=16)\n",
        "\n",
        "print(f\"Number of training examples for distress detection: {len(train_dataset_distress)}\")\n",
        "print(f\"Number of validation examples for distress detection: {len(val_dataset_distress)}\")\n",
        "print(f\"Number of testing examples for distress detection: {len(test_dataset_distress)}\")\n",
        "\n",
        "# Calculate class weights for the distress labels in the training set\n",
        "train_labels_distress_tensor = torch.tensor(train_labels_distress)\n",
        "class_counts_distress = torch.bincount(train_labels_distress_tensor)\n",
        "total_samples_distress = len(train_labels_distress)\n",
        "weights_distress = total_samples_distress / (class_counts_distress + 1e-6)\n",
        "class_weights_distress = weights_distress / weights_distress.sum()  # Normalize to sum to 1\n",
        "class_weights_tensor_distress = class_weights_distress.to(device)\n",
        "\n",
        "print(\"\\nClass weights for distress detection (class 0: not distressed, class 1: distressed):\")\n",
        "print(class_weights_tensor_distress)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hahLRwtihmeN",
        "outputId": "143084a5-fb84-4f66-bc7a-be94158567eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples for distress detection: 1145\n",
            "Number of validation examples for distress detection: 144\n",
            "Number of testing examples for distress detection: 144\n",
            "\n",
            "Class weights for distress detection (class 0: not distressed, class 1: distressed):\n",
            "tensor([0.1074, 0.8926], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the pre-trained DistilBERT model\n",
        "distress_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Define the binary classification layer\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, base_model, dropout=0.1):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(base_model.config.hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        probabilities = self.sigmoid(logits)\n",
        "        return probabilities\n",
        "\n",
        "distress_classifier = BinaryClassifier(distress_model).to(device)\n",
        "\n",
        "# Optimizer with a learning rate of 2e-5\n",
        "optimizer_distress = AdamW(distress_classifier.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Custom weighted BCE loss\n",
        "class WeightedBCELoss(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super(WeightedBCELoss, self).__init__()\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        inputs = torch.clamp(inputs, min=1e-7, max=1-1e-7)\n",
        "        bce_loss = - (targets * torch.log(inputs) + (1 - targets) * torch.log(1 - inputs))\n",
        "        weights = self.weights[targets.long()].unsqueeze(1)\n",
        "        weighted_loss = bce_loss * weights\n",
        "        return torch.mean(weighted_loss)\n",
        "\n",
        "criterion_distress = WeightedBCELoss(class_weights_tensor_distress)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler_distress = ReduceLROnPlateau(optimizer_distress, mode='min', factor=0.1, patience=2)\n",
        "\n",
        "# Training loop\n",
        "num_epochs_distress = 10  # You can adjust the number of epochs\n",
        "print(\"\\n--- Distress Classification Training Started (Custom Weighted Loss) ---\")\n",
        "for epoch in range(num_epochs_distress):\n",
        "    distress_classifier.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader_distress, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].unsqueeze(1).to(device)\n",
        "\n",
        "        outputs = distress_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion_distress(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer_distress.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_distress.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader_distress)\n",
        "    print(f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    distress_classifier.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader_distress:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].unsqueeze(1).to(device)\n",
        "\n",
        "            outputs = distress_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion_distress(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader_distress)\n",
        "    print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    scheduler_distress.step(avg_val_loss)\n",
        "\n",
        "print(\"\\n--- Distress Classification Training Finished (Custom Weighted Loss) ---\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(distress_classifier.state_dict(), '/content/drive/MyDrive/distilbert_distress_classifier.pth')\n",
        "print(\"Trained distress classification model (with custom weighted loss) saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs3r2k5yhmiq",
        "outputId": "991d2172-39de-4095-b7d4-cea40f51d499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Distress Classification Training Started (Custom Weighted Loss) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 72/72 [00:01<00:00, 49.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss: 0.0718\n",
            "Epoch 1, Average Validation Loss: 0.0409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 72/72 [00:01<00:00, 52.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Training Loss: 0.0339\n",
            "Epoch 2, Average Validation Loss: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 72/72 [00:01<00:00, 51.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Training Loss: 0.0121\n",
            "Epoch 3, Average Validation Loss: 0.0087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 72/72 [00:01<00:00, 52.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Training Loss: 0.0050\n",
            "Epoch 4, Average Validation Loss: 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 72/72 [00:01<00:00, 52.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Training Loss: 0.0025\n",
            "Epoch 5, Average Validation Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 72/72 [00:01<00:00, 52.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Training Loss: 0.0006\n",
            "Epoch 6, Average Validation Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 72/72 [00:01<00:00, 51.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Training Loss: 0.0002\n",
            "Epoch 7, Average Validation Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 72/72 [00:01<00:00, 50.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Training Loss: 0.0001\n",
            "Epoch 8, Average Validation Loss: 0.0002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 72/72 [00:01<00:00, 53.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Training Loss: 0.0001\n",
            "Epoch 9, Average Validation Loss: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 72/72 [00:01<00:00, 53.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Training Loss: 0.0001\n",
            "Epoch 10, Average Validation Loss: 0.0001\n",
            "\n",
            "--- Distress Classification Training Finished (Custom Weighted Loss) ---\n",
            "Trained distress classification model (with custom weighted loss) saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully.\")\n",
        "except OSError:\n",
        "    print(\"Error: Could not load the spaCy model. Please ensure you have downloaded it using: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtxB87e5kBNS",
        "outputId": "d2e7d734-05f8-4f88-b6a5-b664a368acd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_locations(text):\n",
        "    if nlp:\n",
        "        doc = nlp(text)\n",
        "        locations = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
        "        return locations\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Apply the location extraction function to the first 100 California wildfire tweets\n",
        "if california_wildfire_df is not None and nlp is not None:\n",
        "    sample_tweets = california_wildfire_df['tweet_text'].head(100)\n",
        "    extracted_locations = sample_tweets.apply(extract_locations)\n",
        "\n",
        "    print(\"\\nExtracted locations from the first 100 California wildfire tweets:\")\n",
        "    for i, locations in extracted_locations.items():\n",
        "        if locations:\n",
        "            print(f\"Tweet {i}: {sample_tweets.iloc[i]} -> Locations: {', '.join(locations)}\")\n",
        "        else:\n",
        "            print(f\"Tweet {i}: {sample_tweets.iloc[i]} -> No locations found\")ift\n",
        "else:\n",
        "    print(\"Either the California wildfire DataFrame is not loaded or the spaCy model is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTzA7rlskBUn",
        "outputId": "83913de4-8198-4155-e7a9-f8bf825e658f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted locations from the first 100 California wildfire tweets:\n",
            "Tweet 0: wildfires raging through northern california are terrifying -> Locations: northern california\n",
            "Tweet 1: photos deadly wildfires rage in california -> Locations: california\n",
            "Tweet 2: californias raging wildfires as youve never seen them before -> No locations found\n",
            "Tweet 3: wildfires threaten californias first legal cannabis harvest -> No locations found\n",
            "Tweet 4: mass evacuations in california as wildfires kill at least 10 #californiawildfires -> Locations: california\n",
            "Tweet 5: california wildfires destroy more than 50 structures #kakenews -> Locations: california\n",
            "Tweet 6: california wildfires destroy more than 50 structures #kakenews -> Locations: california\n",
            "Tweet 7: california wildfires destroy more than 50 structures #kakenews -> Locations: california\n",
            "Tweet 8: southern california fire shrouds disneyland anaheim in dramatic smoky skies -> Locations: california\n",
            "Tweet 9: california wildfire 4 -> Locations: california\n",
            "Tweet 10: wildfires still burn in northern california 11 are dead #nyt -> No locations found\n",
            "Tweet 11: henry cejudo suffers burns loses gold medal in california wildfire -> Locations: california\n",
            "Tweet 12: at least 11 dead and 100 missing as wildfires rage across northern california damage and death toll rises as -> Locations: california\n",
            "Tweet 13: southern california wildfires continue to rage as death toll rises to 11 -> Locations: california\n",
            "Tweet 14: more than 100 missing persons reports made in california wildfires -> Locations: california\n",
            "Tweet 15: california wildfires kill 10 destroy 1500 buildings -> Locations: california\n",
            "Tweet 16: california large wildfire activity for october 10 #cawildfires #cafires -> Locations: california\n",
            "Tweet 17: 11 dead thousands homeless as wildfires torch california wine country #heatwave #wildfires -> Locations: california\n",
            "Tweet 18: thousands flee as wildfires ravage northern california 10 killed -> Locations: california\n",
            "Tweet 19: #breakingnews #news southern california wildfire remains active -> Locations: california\n",
            "Tweet 20: a quick rundown of what you need to know about the harrowing california wildfires -> Locations: california\n",
            "Tweet 21: how to help napa fire victims 8 things you can do for californias wine country right now -> No locations found\n",
            "Tweet 22: from #sciencenow wildfires running amok in california #nasa #wildfires -> Locations: california\n",
            "Tweet 23: on todays pns newscast deadly wildfires ravage northern california listen now -> Locations: california\n",
            "Tweet 24: save mart donating to #california fire victims #bakersfield 0 -> No locations found\n",
            "Tweet 25: i think event wildfire has occurred in california tue oct 10 122503 2017 cdt -> Locations: california\n",
            "Tweet 26: california wildfire outbreak kills at least 11 among most deadly in state history -> Locations: california\n",
            "Tweet 27: #california schools call off all sports because of rising smoke and ash from #canyonfire2 -> Locations: california\n",
            "Tweet 28: divine judgement deadly california wildfires can be seen from space e -> Locations: california\n",
            "Tweet 29: nasa satellites capture breadth of northern california wildfires -> Locations: california\n",
            "Tweet 30: how to help victims of the northern california wildfire -> Locations: california\n",
            "Tweet 31: major disaster declaration approved for the state of #california due to #wildfires -> No locations found\n",
            "Tweet 32: breaking ten people confirmed dead from northern california wildfires says state agency spokesman -> Locations: california\n",
            "Tweet 33: fire crews fighting deadly california blazes welcome cool fog -> Locations: california\n",
            "Tweet 34: hundreds more firefighters deploy to battle deadly wildfires in california -> Locations: california\n",
            "Tweet 35: deadly wildfires threaten thousands of acres of pot farms in california -> Locations: california\n",
            "Tweet 36: #california is on fire amp no one except the people losing everything seem to care -> Locations: california\n",
            "Tweet 37: california is on fire but this is the most important story on cnn right now -> Locations: california\n",
            "Tweet 38: dice was really loaded for wildfires exploding in california experts say -> Locations: california\n",
            "Tweet 39: photos of californias destructive wildfires via -> No locations found\n",
            "Tweet 40: photos of californias destructive wildfires via -> No locations found\n",
            "Tweet 41: photos of californias destructive wildfires via -> No locations found\n",
            "Tweet 42: california wildfires residents find devastation in santa rosa -> Locations: california, santa\n",
            "Tweet 43: watch californias winecountry #wildfires spread in these incredible new images from space -> No locations found\n",
            "Tweet 44: wildfires sweep through california wine country -> Locations: california\n",
            "Tweet 45: the fire ravaging parts of california wine country threatens the regions 1 billion -> Locations: california\n",
            "Tweet 46: 14 images show devastation left behind by northern california wildfires -> Locations: northern california\n",
            "Tweet 47: jewish camp in northern california ravaged by forest fire -> Locations: northern california\n",
            "Tweet 48: california wildfires hit wine country burning hotels and threatening vineyards -> Locations: california\n",
            "Tweet 49: wine country fire in napa valley california -> Locations: california\n",
            "Tweet 50: la news biggest california wildfires of all time -> No locations found\n",
            "Tweet 51: 13 confirmed dead in #wildfires from monday better conditions 2 fight the fires way less wind today #california -> Locations: california\n",
            "Tweet 52: california wildfires destroys homes 11 dead and many missing -> Locations: california\n",
            "Tweet 53: thinking of all my california loved ones friends and colleagues please be safe #wildfires -> Locations: california\n",
            "Tweet 54: this viral beforeandafter photo shows the harrowing damage of california wildfires -> Locations: california\n",
            "Tweet 55: radio reports on california wildfires -> Locations: california\n",
            "Tweet 56: tuesdays morning email take a look at the wildfires devastating california wine country -> Locations: california\n",
            "Tweet 57: i just had to evacuate my home in california due to the wildfire obviously i took the essentials -> Locations: california\n",
            "Tweet 58: map of #winecountryfire spread across northern california -> Locations: california\n",
            "Tweet 59: just in president trump approves california disaster declaration for areas affected by wildfires -> Locations: california\n",
            "Tweet 60: breaking california declares state of emergency following largest wildfire recorded -> Locations: california\n",
            "Tweet 61: olympic champion loses gold medal escaping california fire report says -> Locations: california\n",
            "Tweet 62: while n california burns a separate wildfire has prompted mass evacuations near disneyland -> Locations: california\n",
            "Tweet 63: scary amp creepy the california wildfires as a backdrop to via kennya boulter -> Locations: california, kennya\n",
            "Tweet 64: breweries employees impacted by california wine country wildfires -> Locations: california\n",
            "Tweet 65: san francisco is currently super hazy per the northern california fire stay safe everybody -> Locations: san francisco, california\n",
            "Tweet 66: california is really on fire 5 somthing aint right 2 #pray4california -> Locations: california\n",
            "Tweet 67: california wildfires how climate change could make the problem worse -> Locations: california\n",
            "Tweet 68: gofundme how to immediately help those impacted by the california wildfires #webdesign -> Locations: california\n",
            "Tweet 69: as deadly fires ravage california wine country officials prepare for further fatalities -> Locations: california\n",
            "Tweet 70: destruction in southern california fire -> Locations: california\n",
            "Tweet 71: southern california wildfire burns exclusive anaheim hills community -> Locations: california\n",
            "Tweet 72: why californias #wildfires are worse in the fall -> No locations found\n",
            "Tweet 73: why californias #wildfires are worse in the fall -> No locations found\n",
            "Tweet 74: why californias #wildfires are worse in the fall -> No locations found\n",
            "Tweet 75: public calamity as california wildfires leave apocalyptic scenes in wine country -> Locations: california\n",
            "Tweet 76: 11 dead 100 injured wildfires ravage northern california with shocking speed -> Locations: california\n",
            "Tweet 77: northern california wildfires rage in new photo from space -> No locations found\n",
            "Tweet 78: wild fires in california looks like fallout source #fallout #fo4 #bethesda #fallout4 -> Locations: california\n",
            "Tweet 79: wildfires have ravaged areas all across california destroying over 1500 properties -> Locations: california\n",
            "Tweet 80: terrifying photos show deadly wildfires rampaging across california fed by wicked winds -> Locations: california\n",
            "Tweet 81: deadly fires sweep across california compounding a historic fire season -> Locations: california\n",
            "Tweet 82: at least 10 people dead 20thousand evacuated in california wildfires coverage now on #2newsam -> Locations: california\n",
            "Tweet 83: cities in southern california cant escape the fire at their door -> Locations: california\n",
            "Tweet 84: #california #wildfires 10 killed in unprecedented wine country blaze via -> No locations found\n",
            "Tweet 85: our hearts are with all those affected by the devastating wildfires in our home state of california -> Locations: california\n",
            "Tweet 86: smoke wildfires damage californias famed wine country -> No locations found\n",
            "Tweet 87: playing with my new friend chai shes a california fire evacuee starting with us for a few days i her -> Locations: california\n",
            "Tweet 88: playing with my new friend chai shes a california fire evacuee starting with us for a few days i her -> Locations: california\n",
            "Tweet 89: playing with my new friend chai shes a california fire evacuee starting with us for a few days i her -> Locations: california\n",
            "Tweet 90: playing with my new friend chai shes a california fire evacuee starting with us for a few days i her -> Locations: california\n",
            "Tweet 91: how many fire fighters are illegal aliens in california jerry brown use you dear loved citizens -> Locations: california\n",
            "Tweet 92: diablo winds are sparking massive wildfires in california -> Locations: california\n",
            "Tweet 93: california working all of it ap photorich pedroncelli -> Locations: california\n",
            "Tweet 94: as wildfires spread through california find out how you can help victims -> Locations: california\n",
            "Tweet 95: an inferno like youve never seen deadly wildfires ravage california via -> Locations: california\n",
            "Tweet 96: an aerial photo of the devastation left behind from the north bay wildfires north of san francisco california -> Locations: north bay, san francisco, california\n",
            "Tweet 97: calistoga fire #tubbsfire #napafire #abc7now #kron4news #fire #california #napa -> No locations found\n",
            "Tweet 98: calistoga fire #tubbsfire #napafire #abc7now #kron4news #fire #california #napa -> No locations found\n",
            "Tweet 99: calistoga fire #tubbsfire #napafire #abc7now #kron4news #fire #california #napa -> No locations found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
        "\n",
        "# Revised action categories\n",
        "action_categories = ['start missing person search', 'monitor situation', 'send evacuation',\n",
        "                     'shelter support', 'send rescue team', 'send security',\n",
        "                     'trauma support', 'send medical team', 'send immediate help']\n",
        "num_labels = len(action_categories)\n",
        "\n",
        "# Load the pre-trained DistilBERT model\n",
        "action_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Define the multi-label classifier\n",
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(self, base_model, num_labels, dropout=0.1):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        probabilities = self.sigmoid(logits)\n",
        "        return probabilities\n",
        "\n",
        "model_classifier_action = MultiLabelClassifier(action_model, num_labels).to(device)\n",
        "model_classifier_action.load_state_dict(torch.load('/content/drive/MyDrive/distilbert_multi_label_newsplit_weighted_lr2e5_9labels_15epochs.pth'))\n",
        "model_classifier_action.eval()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer_action = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the tweet text from the California wildfire DataFrame\n",
        "california_encodings_action = tokenizer_action(\n",
        "    california_wildfire_df['tweet_text'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    return_tensors='pt'\n",
        ").to(device)\n",
        "\n",
        "print(\"Action prediction model loaded and California wildfire tweets tokenized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IyawC4ykBec",
        "outputId": "91858f99-1634-4c8c-d6f6-8682bfc95010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action prediction model loaded and California wildfire tweets tokenized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions_action = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(california_wildfire_df), 16), desc=\"Predicting Actions\"):\n",
        "        batch_input_ids = california_encodings_action['input_ids'][i:i+16]\n",
        "        batch_attention_mask = california_encodings_action['attention_mask'][i:i+16]\n",
        "        outputs = model_classifier_action(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        probabilities = outputs.cpu().numpy()\n",
        "        predicted = (probabilities > 0.5).astype(int)\n",
        "        predictions_action.extend(predicted)\n",
        "\n",
        "# Convert predictions to action labels\n",
        "predicted_actions_list = []\n",
        "for prediction in predictions_action:\n",
        "    actions = [action_categories[i] for i, val in enumerate(prediction) if val == 1]\n",
        "    predicted_actions_list.append(', '.join(actions) if actions else 'monitor situation') # Default if no action predicted\n",
        "\n",
        "# Add the predicted actions to the DataFrame\n",
        "california_wildfire_df['predicted_actions'] = predicted_actions_list\n",
        "\n",
        "# Display the first few rows with predicted actions\n",
        "print(\"\\nFirst few rows of California wildfire tweets with predicted actions:\")\n",
        "print(california_wildfire_df[['tweet_text', 'state', 'Wildfire', 'distress', 'predicted_actions']].head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yioHe4MikBlY",
        "outputId": "1a31a7f8-6b8b-4fb8-e388-9f32041886d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting Actions: 100%|██████████| 90/90 [00:00<00:00, 190.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First few rows of California wildfire tweets with predicted actions:\n",
            "                                           tweet_text       state Wildfire  \\\n",
            "0   wildfires raging through northern california a...  California      Yes   \n",
            "1          photos deadly wildfires rage in california  California      Yes   \n",
            "2   californias raging wildfires as youve never se...  California      Yes   \n",
            "3   wildfires threaten californias first legal can...  California      Yes   \n",
            "4   mass evacuations in california as wildfires ki...  California      Yes   \n",
            "5   california wildfires destroy more than 50 stru...  California      Yes   \n",
            "6   california wildfires destroy more than 50 stru...  California      Yes   \n",
            "7   california wildfires destroy more than 50 stru...  California      Yes   \n",
            "8   southern california fire shrouds disneyland an...  California      Yes   \n",
            "9                               california wildfire 4  California      Yes   \n",
            "10  wildfires still burn in northern california 11...  California      Yes   \n",
            "11  henry cejudo suffers burns loses gold medal in...  California      Yes   \n",
            "12  at least 11 dead and 100 missing as wildfires ...  California      Yes   \n",
            "13  southern california wildfires continue to rage...  California      Yes   \n",
            "14  more than 100 missing persons reports made in ...  California      Yes   \n",
            "15  california wildfires kill 10 destroy 1500 buil...  California      Yes   \n",
            "16  california large wildfire activity for october...  California      Yes   \n",
            "17  11 dead thousands homeless as wildfires torch ...  California      Yes   \n",
            "18  thousands flee as wildfires ravage northern ca...  California      Yes   \n",
            "19  #breakingnews #news southern california wildfi...  California      Yes   \n",
            "\n",
            "    distress                 predicted_actions  \n",
            "0          0                 monitor situation  \n",
            "1          0                 monitor situation  \n",
            "2          0                 monitor situation  \n",
            "3          0                 monitor situation  \n",
            "4          1  send evacuation, shelter support  \n",
            "5          0                 monitor situation  \n",
            "6          0                 monitor situation  \n",
            "7          0                 monitor situation  \n",
            "8          0                 monitor situation  \n",
            "9          0                 monitor situation  \n",
            "10         0                 monitor situation  \n",
            "11         0                 monitor situation  \n",
            "12         1       start missing person search  \n",
            "13         0                 monitor situation  \n",
            "14         1       start missing person search  \n",
            "15         0                 monitor situation  \n",
            "16         0                 monitor situation  \n",
            "17         0                 monitor situation  \n",
            "18         0                 monitor situation  \n",
            "19         0                 monitor situation  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply location extraction to the entire DataFrame\n",
        "if nlp is not None and 'extracted_locations' not in california_wildfire_df.columns:\n",
        "    california_wildfire_df['extracted_locations'] = california_wildfire_df['tweet_text'].apply(extract_locations)\n",
        "    print(\"Location extraction completed for all California wildfire tweets.\")\n",
        "elif 'extracted_locations' in california_wildfire_df.columns:\n",
        "    print(\"Locations already extracted.\")\n",
        "else:\n",
        "    print(\"spaCy model not loaded.\")\n",
        "\n",
        "# Now, let's display the information in a clearer format\n",
        "if california_wildfire_df is not None:\n",
        "    print(\"\\n--- California Wildfire Tweets with Distress, Locations, and Predicted Actions ---\")\n",
        "    for index, row in california_wildfire_df.head(20).iterrows():\n",
        "        tweet = row['tweet_text']\n",
        "        distress = \"Yes\" if row['distress'] == 1 else \"No\"\n",
        "        locations = \", \".join(row['extracted_locations']) if row['extracted_locations'] else \"No locations found\"\n",
        "        actions = row['predicted_actions']\n",
        "        print(f\"\\nTweet {index}:\")\n",
        "        print(f\"  Text: {tweet}\")\n",
        "        print(f\"  Distress: {distress}\")\n",
        "        print(f\"  Locations: {locations}\")\n",
        "        print(f\"  Predicted Actions: {actions}\")\n",
        "else:\n",
        "    print(\"California wildfire DataFrame not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3p89b94kBu0",
        "outputId": "6b4d5776-f13f-4323-e123-c92c596144d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Location extraction completed for all California wildfire tweets.\n",
            "\n",
            "--- California Wildfire Tweets with Distress, Locations, and Predicted Actions ---\n",
            "\n",
            "Tweet 0:\n",
            "  Text: wildfires raging through northern california are terrifying\n",
            "  Distress: No\n",
            "  Locations: northern california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 1:\n",
            "  Text: photos deadly wildfires rage in california\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 2:\n",
            "  Text: californias raging wildfires as youve never seen them before\n",
            "  Distress: No\n",
            "  Locations: No locations found\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 3:\n",
            "  Text: wildfires threaten californias first legal cannabis harvest\n",
            "  Distress: No\n",
            "  Locations: No locations found\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 4:\n",
            "  Text: mass evacuations in california as wildfires kill at least 10 #californiawildfires\n",
            "  Distress: Yes\n",
            "  Locations: california\n",
            "  Predicted Actions: send evacuation, shelter support\n",
            "\n",
            "Tweet 5:\n",
            "  Text: california wildfires destroy more than 50 structures #kakenews\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 6:\n",
            "  Text: california wildfires destroy more than 50 structures #kakenews\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 7:\n",
            "  Text: california wildfires destroy more than 50 structures #kakenews\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 8:\n",
            "  Text: southern california fire shrouds disneyland anaheim in dramatic smoky skies\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 9:\n",
            "  Text: california wildfire 4\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 10:\n",
            "  Text: wildfires still burn in northern california 11 are dead #nyt\n",
            "  Distress: No\n",
            "  Locations: No locations found\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 11:\n",
            "  Text: henry cejudo suffers burns loses gold medal in california wildfire\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 12:\n",
            "  Text: at least 11 dead and 100 missing as wildfires rage across northern california damage and death toll rises as\n",
            "  Distress: Yes\n",
            "  Locations: california\n",
            "  Predicted Actions: start missing person search\n",
            "\n",
            "Tweet 13:\n",
            "  Text: southern california wildfires continue to rage as death toll rises to 11\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 14:\n",
            "  Text: more than 100 missing persons reports made in california wildfires\n",
            "  Distress: Yes\n",
            "  Locations: california\n",
            "  Predicted Actions: start missing person search\n",
            "\n",
            "Tweet 15:\n",
            "  Text: california wildfires kill 10 destroy 1500 buildings\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 16:\n",
            "  Text: california large wildfire activity for october 10 #cawildfires #cafires\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 17:\n",
            "  Text: 11 dead thousands homeless as wildfires torch california wine country #heatwave #wildfires\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 18:\n",
            "  Text: thousands flee as wildfires ravage northern california 10 killed\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n",
            "\n",
            "Tweet 19:\n",
            "  Text: #breakingnews #news southern california wildfire remains active\n",
            "  Distress: No\n",
            "  Locations: california\n",
            "  Predicted Actions: monitor situation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained distress classification model if not already loaded\n",
        "if 'distress_classifier' not in locals():\n",
        "    from transformers import DistilBertModel\n",
        "    import torch.nn as nn\n",
        "\n",
        "    distress_model_eval = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "    class BinaryClassifierEval(nn.Module):\n",
        "        def __init__(self, base_model, dropout=0.1):\n",
        "            super(BinaryClassifierEval, self).__init__()\n",
        "            self.base_model = base_model\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.classifier = nn.Linear(base_model.config.hidden_size, 1)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "            pooled_output = self.dropout(pooled_output)\n",
        "            logits = self.classifier(pooled_output)\n",
        "            probabilities = self.sigmoid(logits)\n",
        "            return probabilities\n",
        "\n",
        "    distress_classifier = BinaryClassifierEval(distress_model_eval).to(device)\n",
        "    distress_classifier.load_state_dict(torch.load('/content/drive/MyDrive/distilbert_distress_classifier.pth'))\n",
        "    distress_classifier.eval()\n",
        "\n",
        "# Prepare the test DataLoader\n",
        "if 'test_dataloader_distress' not in locals():\n",
        "    from transformers import DistilBertTokenizerFast\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "    tokenizer_distress_eval = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    class DistressDatasetEval(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    X_test_distress = test_texts_distress\n",
        "    y_test_distress = test_labels_distress\n",
        "    test_encodings_distress = tokenizer_distress_eval(X_test_distress, truncation=True, padding=True)\n",
        "    test_dataset_distress = DistressDatasetEval(test_encodings_distress, y_test_distress)\n",
        "    test_dataloader_distress = DataLoader(test_dataset_distress, batch_size=16)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = []\n",
        "true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader_distress, desc=\"Evaluating Distress Model\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].cpu().numpy()\n",
        "        outputs = distress_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probabilities = outputs.squeeze().cpu().numpy()\n",
        "        predicted_labels = (probabilities > 0.5).astype(int)\n",
        "        predictions.extend(predicted_labels)\n",
        "        true_labels.extend(labels)\n",
        "\n",
        "# Generate classification report and confusion matrix\n",
        "print(\"\\n--- Distress Classification Model Evaluation ---\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or_WFME3nYsU",
        "outputId": "ae783fc4-9292-438d-ebe7-98ff65117df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Distress Model: 100%|██████████| 9/9 [00:00<00:00, 165.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Distress Classification Model Evaluation ---\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.99      0.98       129\n",
            "         1.0       0.91      0.67      0.77        15\n",
            "\n",
            "    accuracy                           0.96       144\n",
            "   macro avg       0.94      0.83      0.87       144\n",
            "weighted avg       0.96      0.96      0.96       144\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[128   1]\n",
            " [  5  10]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the path to save the distress classification model\n",
        "distress_model_save_path = '/content/drive/MyDrive/distilbert_distress_classifier_evaluated.pth'\n",
        "\n",
        "# Save the model state dictionary\n",
        "torch.save(distress_classifier.state_dict(), distress_model_save_path)\n",
        "print(f\"Evaluated distress classification model saved to: {distress_model_save_path}\")\n",
        "\n",
        "# Prepare the evaluation results as a string\n",
        "evaluation_report = \"\"\"\n",
        "--- Distress Classification Model Evaluation ---\n",
        "\n",
        "Classification Report:\n",
        "{}\n",
        "\n",
        "Confusion Matrix:\n",
        "{}\n",
        "\"\"\".format(classification_report(true_labels, predictions), confusion_matrix(true_labels, predictions))\n",
        "\n",
        "# Define the path to save the evaluation results\n",
        "evaluation_results_path = '/content/drive/MyDrive/distress_classification_evaluation_report.txt'\n",
        "\n",
        "# Save the evaluation report to a text file\n",
        "with open(evaluation_results_path, 'w') as f:\n",
        "    f.write(evaluation_report)\n",
        "\n",
        "print(f\"\\nDistress classification evaluation results saved to: {evaluation_results_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmUbSGX2nY1H",
        "outputId": "ed935025-e9e6-4f2e-d507-972a007a244d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated distress classification model saved to: /content/drive/MyDrive/distilbert_distress_classifier_evaluated.pth\n",
            "\n",
            "Distress classification evaluation results saved to: /content/drive/MyDrive/distress_classification_evaluation_report.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IFT"
      ],
      "metadata": {
        "id": "abOkuP6dVcns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from geopy.geocoders import Nominatim\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California wildfire DataFrame (REPLACE WITH YOUR ACTUAL PATH)\n",
        "file_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "try:\n",
        "    california_wildfire_df = pd.read_csv(file_path)\n",
        "    print(f\"California wildfire DataFrame loaded successfully from: {file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at: {file_path}. Please provide the correct path to your california_wildfire_tweets.csv file.\")\n",
        "    california_wildfire_df = None\n",
        "\n",
        "if california_wildfire_df is not None:\n",
        "    # Initialize geolocator for location extraction (we might use this to validate extracted locations)\n",
        "    geolocator = Nominatim(user_agent=\"wildfire_locator\")\n",
        "\n",
        "    # Load a small spaCy model for better location extraction\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"Error: Could not load the spaCy model. Please ensure you have downloaded it using: python -m spacy download en_core_web_sm\")\n",
        "        nlp = None\n",
        "\n",
        "    def is_california_wildfire(text):\n",
        "        text = text.lower()\n",
        "        california_mentions = \"california\" in text or \"ca\" in text or \"la\" in text or \"san francisco\" in text or \"bay area\" in text # Add more California-specific terms if needed\n",
        "        fire_related = \"fire\" in text or \"wildfire\" in text or \"smoke\" in text or \"flames\" in text or \"evacuat\" in text\n",
        "        return \"yes\" if california_mentions and fire_related else \"no\"\n",
        "\n",
        "    def detect_distress_instruction_final_refined(text):\n",
        "        text = text.lower()\n",
        "        distress_keywords = [\n",
        "            \"help\", \"trapped\", \"injured\", \"need rescue\", \"emergency\", \"life-threatening\", \"dying\", \"burn\", \"evacuate now\", \"urgent\",\n",
        "            \"terrifying\", \"scary\", \"afraid\", \"worried\", \"desperate\", \"panic\", \"danger\", \"threatened\",\n",
        "            \"lost my home\", \"lost everything\", \"can't breathe\", \"smoke inhalation\", \"fire is near\", \"in flames\",\n",
        "            \"need medical\", \"call for help\", \"stuck\", \"nowhere to go\", \"running out of time\", \"please help us\",\n",
        "            \"destruction\", \"destroy\", \"suffer\", \"suffering\"\n",
        "        ]\n",
        "        if any(keyword in text for keyword in distress_keywords):\n",
        "            return \"distress\"\n",
        "        else:\n",
        "            return \"not distress\"\n",
        "\n",
        "    def extract_location_instruction(text):\n",
        "        text = text.lower()\n",
        "        if nlp:\n",
        "            doc = nlp(text)\n",
        "            locations = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
        "            if locations:\n",
        "                # Return the first identified location for simplicity\n",
        "                return locations[0]\n",
        "        return \"unknown\"\n",
        "\n",
        "    def recommend_action_instruction(text, is_distress):\n",
        "        text = text.lower()\n",
        "        if is_distress == \"distress\":\n",
        "            if \"medical\" in text or \"injured\" in text or \"burn\" in text:\n",
        "                return \"medical aid\"\n",
        "            elif \"trapped\" in text or \"rescue\" in text:\n",
        "                return \"rescue\"\n",
        "            elif \"evacuate\" in text:\n",
        "                return \"evacuation\"\n",
        "            else:\n",
        "                return \"rescue\" # Default to rescue if distress is indicated but specific need is unclear\n",
        "        else:\n",
        "            if \"burn\" in text or \"fire\" in text and (\"spread\" in text or \"threaten\" in text):\n",
        "                return \"fire suppression\"\n",
        "            elif \"need supplies\" in text or \"lack food\" in text or \"water low\" in text:\n",
        "                return \"resource delivery\"\n",
        "            elif \"evacuat\" in text or \"shelter\" in text:\n",
        "                return \"evacuation\" # Could be a report of an ongoing evacuation\n",
        "            else:\n",
        "                return \"monitor only\"\n",
        "\n",
        "    # Apply the functions to create the label columns\n",
        "    california_wildfire_df['tweet_text_lower'] = california_wildfire_df['tweet_text'].apply(lambda x: x.lower()) # Lowercase the text once\n",
        "    california_wildfire_df['is_wildfire_ca'] = california_wildfire_df['tweet_text_lower'].apply(is_california_wildfire)\n",
        "    california_wildfire_df['distress_label_instruction'] = california_wildfire_df['tweet_text_lower'].apply(detect_distress_instruction_final_refined)\n",
        "    california_wildfire_df['location_label_instruction'] = california_wildfire_df['tweet_text_lower'].apply(extract_location_instruction)\n",
        "    california_wildfire_df['action_label_instruction'] = california_wildfire_df.apply(lambda row: recommend_action_instruction(row['tweet_text_lower'], row['distress_label_instruction']), axis=1)\n",
        "\n",
        "    print(\"Labeling complete. First few rows with labels:\")\n",
        "    print(california_wildfire_df[['tweet_text', 'is_wildfire_ca', 'distress_label_instruction', 'location_label_instruction', 'action_label_instruction']].head())\n",
        "\n",
        "    # Check class balance for distress\n",
        "    distress_balance = california_wildfire_df['distress_label_instruction'].value_counts(normalize=True)\n",
        "    print(\"\\nClass balance for 'distress_label_instruction':\")\n",
        "    print(distress_balance)\n",
        "\n",
        "    # --- Data Splitting ---\n",
        "    if 'distress_label_instruction' in california_wildfire_df.columns:\n",
        "        # Separate features (tweet text) and the distress label for stratified splitting\n",
        "        X = california_wildfire_df['tweet_text']\n",
        "        y_distress = california_wildfire_df['distress_label_instruction']\n",
        "\n",
        "        # 1. Sample (50 count) - stratified and balanced\n",
        "        sample_df = california_wildfire_df.groupby('distress_label_instruction', group_keys=False).apply(lambda x: x.sample(min(len(x), 25)))\n",
        "        print(f\"\\nSample set size: {len(sample_df)}\")\n",
        "\n",
        "        # 2. Remaining data\n",
        "        remaining_df = california_wildfire_df.drop(sample_df.index)\n",
        "\n",
        "        # Split remaining into training, validation, and testing (stratified)\n",
        "        train_df, temp_df = train_test_split(remaining_df, test_size=0.2, stratify=remaining_df['distress_label_instruction'], random_state=42)\n",
        "        val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['distress_label_instruction'], random_state=42)\n",
        "\n",
        "        print(f\"Training set size: {len(train_df)}\")\n",
        "        print(f\"Validation set size: {len(val_df)}\")\n",
        "        print(f\"Testing set size: {len(test_df)}\")\n",
        "\n",
        "        # Save the splits (you might want to save them to CSV files)\n",
        "        train_df.to_csv('train_instruction_tuning.csv', index=False)\n",
        "        val_df.to_csv('val_instruction_tuning.csv', index=False)\n",
        "        test_df.to_csv('test_instruction_tuning.csv', index=False)\n",
        "        sample_df.to_csv('sample_instruction_tuning.csv', index=False)\n",
        "        print(\"\\nData splits saved to CSV files.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: 'distress_label_instruction' column not found, cannot perform stratified split.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded, cannot proceed with labeling and splitting.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBeLf0EeX6F4",
        "outputId": "52ede502-9c37-47a6-d03f-58a9d6ae00c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "California wildfire DataFrame loaded successfully from: /content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv\n",
            "Labeling complete. First few rows with labels:\n",
            "                                          tweet_text is_wildfire_ca  \\\n",
            "0  wildfires raging through northern california a...            yes   \n",
            "1         photos deadly wildfires rage in california            yes   \n",
            "2  pls share were capturing wildfire response rec...            yes   \n",
            "3  pls share were capturing wildfire response rec...            yes   \n",
            "4  californias raging wildfires as youve never se...            yes   \n",
            "\n",
            "  distress_label_instruction location_label_instruction  \\\n",
            "0                   distress        northern california   \n",
            "1               not distress                 california   \n",
            "2               not distress                    unknown   \n",
            "3               not distress                    unknown   \n",
            "4               not distress                    unknown   \n",
            "\n",
            "  action_label_instruction  \n",
            "0                   rescue  \n",
            "1             monitor only  \n",
            "2             monitor only  \n",
            "3             monitor only  \n",
            "4             monitor only  \n",
            "\n",
            "Class balance for 'distress_label_instruction':\n",
            "distress_label_instruction\n",
            "not distress    0.884913\n",
            "distress        0.115087\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Sample set size: 50\n",
            "Training set size: 14425\n",
            "Validation set size: 1803\n",
            "Testing set size: 1804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-65945a42dd13>:100: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sample_df = california_wildfire_df.groupby('distress_label_instruction', group_keys=False).apply(lambda x: x.sample(min(len(x), 25)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data splits saved to CSV files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if california_wildfire_df is not None:\n",
        "    action_balance = california_wildfire_df['action_label_instruction'].value_counts(normalize=True)\n",
        "    print(\"\\nClass balance for 'action_label_instruction':\")\n",
        "    print(action_balance)\n",
        "else:\n",
        "    print(\"DataFrame not loaded, cannot check action label balance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM3bQV5ZX6SP",
        "outputId": "ed92c00b-efa3-4060-ceec-46527d507c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class balance for 'action_label_instruction':\n",
            "action_label_instruction\n",
            "monitor only         0.870479\n",
            "rescue               0.108284\n",
            "evacuation           0.013549\n",
            "medical aid          0.006415\n",
            "fire suppression     0.001217\n",
            "resource delivery    0.000055\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"  # Or the specific Llama 7b variant you're using\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464,
          "referenced_widgets": [
            "4396fa43c10a490fad244f6b50794fe1",
            "e10ec6185d8446b9ad256cb06dbc2050",
            "d11849f1f86b41d2802a757395e140e4",
            "574a7aba61b24ebb8b574f021fff97ba",
            "6a7188ef52784a06bd0e06b33d04d9f6",
            "98629ddb8053494abfe0691dd9e47429",
            "14f07dcbecaa46678ae989a5cfc7df34",
            "a1a0416682dd40eda9ae278fdf5f9099",
            "2287d8ca8275461496737394762fd004",
            "8d0a1d5d55a74c95a08fbe41b649c130",
            "5becf270e2cf49069bd91b4dcd7e220a",
            "1f6f2f2d5e0c4d1f91690db68f5f5a94",
            "aad55de7bc91419ba6319c305cd21902",
            "6f2eed4b4e75480c9a10a240bf684e8e",
            "012886d678dc414babb3811613e3564f",
            "4724b5a5b709487998eb1a181410958e",
            "165f145650bf4600bec86b6d22423763",
            "246c3b399ad445409a0ffbfa1fc54872",
            "651c8a08c0784004aa198fe7dc1b7ea4",
            "cb4a570cfef147a0ab4fb5f7a52c8e91",
            "e7fdfdb0522f43fc8dd18e50327be23d",
            "fb452319855149ac9a92e58f6bc5073a",
            "7dd63d299ffd48c5a8bcb8aa0e11d58a",
            "e27503f510454d7da9b6110aa9246b9c",
            "2f0f03acbc2f4c3084b88dcdb1b58bec",
            "40e9462a951f4c88bb11595f3385afab",
            "ea920350f6ce460185ae72c133741d0a",
            "0ae133d61d6d42fe8a945f59ea138612",
            "7a6da5d5010147d58feb273ec3fb89e2",
            "f874f2a4cff14c288322c7d0a858f289",
            "70269740d5e64fd493b0ed6d98512796",
            "4430caaac83b47c382a37f87acbe551b",
            "f95e3b608798450abdab1ea16b4571ed",
            "a3b176a4470c4863ad320f7af8136584",
            "fabcca4884374cfa9d860385a31e3575",
            "d51d26576dae4c37ad11c7068e55c4db",
            "37221acb31ca407e811dd5b124bd281f",
            "43683853897842858c93f27bf81ce0b7",
            "c70ae2f803fb4a48aeb1c4b486643f17",
            "9f0d0ffe5af445348a71052718c63e2c",
            "585c6097a37b4b19b4c94f94c147f06e",
            "8a31647075a1410ba23387185c1701b9",
            "a95eb37ca4654aeea182b94265d8ccb7",
            "f165029f557d4de18db3c9e49d9ad5bb",
            "164f042cddec4736ad68af7527edba83",
            "fe6708e0a20d438e98309961bff04e0c",
            "7de1509d65a14fc58f2fbeaf9f3ec03f",
            "1ecd9f77afe0446bb4debd5b6bf15ecc",
            "1a50de3ae514415ab7c6d4620eca8f80",
            "637decbe390545e4aaa4aaa8a38a9220",
            "608734eb0f47490db08bd2a7bce83153",
            "1b58c704a2e44d92becff8620bf907ba",
            "0f608d8962694cca88530d14717bc400",
            "2b4d91fbae794338b65ba48a1f086c5c",
            "a1db1a0a7d624077b7adf064ea80e5cc",
            "62244a53b0794e33ae638263186de76f",
            "2c692652f048400dbc39deb5cffd891b",
            "227014cf382a4e179ad5735bf029476b",
            "63e9269d0d1c4407a5644320de51d3b3",
            "93be8b0cf82847039790cc6cc6aa2441",
            "4b5adf6838954e4bbaf6b10789e6ebed",
            "ad3c8e5761224de79852721bd4e8015b",
            "dd6f7a8cb38d45818765763a7841df74",
            "d8045fbd05414b6794f0922c49b9f7a0",
            "760b58ab23eb48759c85df7dc58cc7bd",
            "b50b514ee703411f82edec018d7e0356",
            "fcaedee12fae4933822b5615a8a15193",
            "f01f2b5047cb4042890901c390e4361b",
            "0f1827983ebf41a6b0aee1f91594e73a",
            "e8cd7349be0742b29ba7bc66bae460f7",
            "34affcd31d17444cb5aad2324c289b52",
            "ecf3210ef94249f284c820ce960531a5",
            "3a78d3f02a2b4163bd6bfb5f2a315eac",
            "cca6775aa82949a9aec4a92da7ca84e8",
            "fa537f5a8a29485abd0f35f0662dc8e4",
            "24d66adc435b4aed822e913bce61a301",
            "cf28132f929f4f3c921a11a7aa7fe69c",
            "1e396bbc7ce0464bb0e0f12c447d92a8",
            "ae01a33bd3f7440b896d6d0542c50c21",
            "ad3a5ad5f06443ef87e0d18cea0c5142",
            "e6d842f5a9f94ae3a96fb151226e7375",
            "418ed1fc468849008c69eb92cb6d7007",
            "44e698c55558476786c6fe730784ad83",
            "78b076f98e8a4cbaa69c84b247b58ff1",
            "7723db8c14cd484eb9e9f1e3a7a5ee63",
            "84e1b22df86141e285f499f6ea2fe93d",
            "cebc86f3121345f2bbc4279a3291e6e6",
            "71a917dd3b8047e88c7f1d262f5bdcc4",
            "0c17f1ea5064482f8a68b0d91411522b",
            "2c70b97ec8de4681bcdb665f56dac379",
            "7d4a65b3e74147ff9983030e75f98b8f",
            "db544e75a98e4fdc9e4189c8dedc00f1",
            "e302a3ed8b35458ea7cc9e9af982b03a",
            "9df838bb368648a39eb34fcfd531e175",
            "a219eaf6fc5044a2b6f4760caed17a19",
            "d12172f56c3c4e5d9566c738832d7ee7",
            "9a1fa1f5d7f149b78db591878cc2c67d",
            "55d21279112a464e97455813d48e7098",
            "a1d6dbc523414d1689a63f4570376e95",
            "0787ffd8ca29458eb0675b04a5900dfa",
            "0a7a8fcb94134deeac5752bf76a6b5e9",
            "d3e93071cbcf4e41ac9d46eca35d1b8d",
            "74bff334786c4a85a2f3192f6d83b685",
            "feb0ac615f544701a745fb2e71b298e1",
            "53d27358e4cc4711be34bf32952bf5e4",
            "e7272cd9fa604405b3f5d507659437e9",
            "d17a6f411b9345d49c6d18d88202b2ae",
            "41e7cc2543a44c81afd779053cd8cbc4",
            "f79f538758a347fc9b8f83a1c09bb247",
            "2d02a5953bde4738bde09f15d56cef0c"
          ]
        },
        "id": "N-E0dASBX6e0",
        "outputId": "264b9c0f-fccb-4cdb-e93a-c74c134172c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4396fa43c10a490fad244f6b50794fe1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f6f2f2d5e0c4d1f91690db68f5f5a94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dd63d299ffd48c5a8bcb8aa0e11d58a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3b176a4470c4863ad320f7af8136584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "164f042cddec4736ad68af7527edba83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62244a53b0794e33ae638263186de76f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcaedee12fae4933822b5615a8a15193"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e396bbc7ce0464bb0e0f12c447d92a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c17f1ea5064482f8a68b0d91411522b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0787ffd8ca29458eb0675b04a5900dfa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "train_df = pd.read_csv('train_instruction_tuning.csv')\n",
        "\n",
        "def format_instruction(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: {row['tweet_text']}\"\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return f\"You are an emergency detection system. Determine if the tweet clearly indicates distress. Input: {row['tweet_text']}\"\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return f\"Extract the most specific real-world geographic location mentioned in the tweet. Input: {row['tweet_text']}\"\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action. Input: {row['tweet_text']}\"\n",
        "    return \"\"\n",
        "\n",
        "def format_output(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return row['is_wildfire_ca']\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return row['distress_label_instruction']\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return row['location_label_instruction']\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return row['action_label_instruction']\n",
        "    return \"\"\n",
        "\n",
        "train_df['instruction'] = train_df.apply(format_instruction, axis=1)\n",
        "train_df['output'] = train_df.apply(format_output, axis=1)\n",
        "train_df['input'] = train_df['tweet_text']\n",
        "\n",
        "# Filter out examples with empty instructions\n",
        "train_df_filtered = train_df[train_df['instruction'] != \"\"]\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(examples['instruction'], examples['input'])]\n",
        "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples['output']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding='longest', return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, truncation=True, padding='longest', return_tensors=\"pt\")\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in doc] for doc in labels[\"input_ids\"]\n",
        "    ]\n",
        "    return {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "print(tokenized_train_dataset[0])\n",
        "print(tokenized_val_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "1b535606670e47a693b8493b80868b45",
            "7026461057ba404088516da40cf91918",
            "ef96cbc306804d53b55f79f6085e9c8a",
            "6e1af3e900184bb7bd765f43d2af2368",
            "5b5da60fe402443a895589d3f947ecbf",
            "747731b4eeef4adf9f8c8a8954e30ad9",
            "558ed476e3324fa1a7c9172f5d33aa33",
            "c01a20b0179c4047b3d32f4d45c5334c",
            "5a9480466040428aa696a921bf083ced",
            "c199c1307a454d0e8ab5d3aa396363c4",
            "1304176e091d469ba33eb13be81e9b2b",
            "8b938145684c46d3ae90b2765aa02512",
            "0f9f2b8d1d20458aa62ed7a41e81139c",
            "946c7f91f000492dba43a5ce087aa551",
            "d8e454f3385d4686a6d54d89b8f448f0",
            "746a88cfd3f04e7c8776478cdef19ce0",
            "5254edc543144f2a90215033acbe4be6",
            "7c37becf799b4ee89c4735fd0aa60b01",
            "32b8b80404e047d7803acbfb0d6f01a9",
            "f4f6a61f89a946c8aa249de1c104df79",
            "ce5de9d896754473aa9b06abb2bd4c24",
            "8b9031e6fefd4f3e839398a086d9cc2e"
          ]
        },
        "id": "CIh_FgT1fkxk",
        "outputId": "8ca77411-6bf9-4316-b97a-7ca0a90d7f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14425 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b535606670e47a693b8493b80868b45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1803 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b938145684c46d3ae90b2765aa02512"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: heres what 51000 pounds of donated goods from houston to puerto rico looks like', 'input': 'heres what 51000 pounds of donated goods from houston to puerto rico looks like', 'output': 'no', 'input_ids': [1, 887, 526, 263, 8775, 8696, 770, 3709, 29889, 5953, 837, 457, 565, 278, 7780, 300, 338, 9479, 1048, 263, 8775, 8696, 10464, 297, 8046, 29889, 10567, 29901, 902, 267, 825, 29871, 29945, 29896, 29900, 29900, 29900, 24261, 310, 1016, 630, 22535, 515, 298, 283, 7352, 304, 2653, 261, 517, 364, 1417, 3430, 763, 13, 2276, 267, 825, 29871, 29945, 29896, 29900, 29900, 29900, 24261, 310, 1016, 630, 22535, 515, 298, 283, 7352, 304, 2653, 261, 517, 364, 1417, 3430, 763, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 694, -100]}\n",
            "{'instruction': 'You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: central #mexico #earthquake #worksheet #geography #junior cycle #jcgeography', 'input': 'central #mexico #earthquake #worksheet #geography #junior cycle #jcgeography', 'output': 'no', 'input_ids': [1, 887, 526, 263, 8775, 8696, 770, 3709, 29889, 5953, 837, 457, 565, 278, 7780, 300, 338, 9479, 1048, 263, 8775, 8696, 10464, 297, 8046, 29889, 10567, 29901, 6555, 396, 29885, 735, 1417, 396, 799, 386, 339, 1296, 396, 1287, 9855, 396, 479, 5275, 396, 29926, 348, 1611, 11412, 396, 29926, 29883, 479, 5275, 13, 25171, 396, 29885, 735, 1417, 396, 799, 386, 339, 1296, 396, 1287, 9855, 396, 479, 5275, 396, 29926, 348, 1611, 11412, 396, 29926, 29883, 479, 5275, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 694, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if 'model.layers.' in name:\n",
        "        layer_num = int(name.split('.')[2])\n",
        "        if layer_num < 26:  # Freeze layers up to layer 25 (adjust as needed)\n",
        "            param.requires_grad = False\n",
        "\n",
        "# Verify the number of trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Percentage trainable: {trainable_params / total_params * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQu4EkL7fk19",
        "outputId": "3842e5ae-f4fe-421f-b554-818955910570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 6738415616\n",
            "Trainable parameters: 1476448256\n",
            "Percentage trainable: 21.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "output_dir = \"./llama-7b-wildfire-multi-task\"  # Directory to save checkpoints and logs\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,  # Adjust based on your GPU memory\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,  # Adjust as needed\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"tensorboard\",  # Or \"wandb\" if you prefer\n",
        ")"
      ],
      "metadata": {
        "id": "gox4ggJffk7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "val_df = pd.read_csv('val_instruction_tuning.csv')\n",
        "\n",
        "def format_instruction_val(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: {row['tweet_text']}\"\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return f\"You are an emergency detection system. Determine if the tweet clearly indicates distress. Input: {row['tweet_text']}\"\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return f\"Extract the most specific real-world geographic location mentioned in the tweet. Input: {row['tweet_text']}\"\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action. Input: {row['tweet_text']}\"\n",
        "    return \"\"\n",
        "\n",
        "def format_output_val(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return row['is_wildfire_ca']\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return row['distress_label_instruction']\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return row['location_label_instruction']\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return row['action_label_instruction']\n",
        "    return \"\"\n",
        "\n",
        "val_df['instruction'] = val_df.apply(format_instruction_val, axis=1)\n",
        "val_df['output'] = val_df.apply(format_output_val, axis=1)\n",
        "val_df['input'] = val_df['tweet_text']\n",
        "\n",
        "# Filter out examples with empty instructions\n",
        "val_df_filtered = val_df[val_df['instruction'] != \"\"]\n",
        "\n",
        "val_dataset = Dataset.from_pandas(val_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "def preprocess_function_val(examples):\n",
        "    inputs = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(examples['instruction'], examples['input'])]\n",
        "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples['output']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in doc] for doc in labels[\"input_ids\"]\n",
        "    ]\n",
        "    return {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function_val, batched=True)\n",
        "\n",
        "print(tokenized_val_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "480bbab3517142efa483ea9c6faf13ca",
            "b4531a0ac1d8477891d592791f3c30e5",
            "2c5c5ca66f734f0fb2a864ecb4851eb4",
            "fbfa417afa324928aa936044512bb0e2",
            "2765a88eb82e4d89a61f0b3b21588095",
            "dfe85813b0804300aa0eda7c49ac1472",
            "3746fe474be34c9d8fb774e0e5b07de4",
            "ac703191715348ad87abe45c48f12f7b",
            "783c67e023d041a5b19d866dbe955024",
            "2c8687851e7a42efa34280cc4f39e280",
            "c0c06bf0c03243499f85bd8d16198b34"
          ]
        },
        "id": "1VxfTP8vflAE",
        "outputId": "ccd6216d-b206-4c97-90dc-17806ec9a7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1803 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "480bbab3517142efa483ea9c6faf13ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: central #mexico #earthquake #worksheet #geography #junior cycle #jcgeography', 'input': 'central #mexico #earthquake #worksheet #geography #junior cycle #jcgeography', 'output': 'no', 'input_ids': [1, 887, 526, 263, 8775, 8696, 770, 3709, 29889, 5953, 837, 457, 565, 278, 7780, 300, 338, 9479, 1048, 263, 8775, 8696, 10464, 297, 8046, 29889, 10567, 29901, 6555, 396, 29885, 735, 1417, 396, 799, 386, 339, 1296, 396, 1287, 9855, 396, 479, 5275, 396, 29926, 348, 1611, 11412, 396, 29926, 29883, 479, 5275, 13, 25171, 396, 29885, 735, 1417, 396, 799, 386, 339, 1296, 396, 1287, 9855, 396, 479, 5275, 396, 29926, 348, 1611, 11412, 396, 29926, 29883, 479, 5275, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 694, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl5k22VyflEg",
        "outputId": "8e44f001-2a50-4740-ab32-64f71ad75a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/bitsandbytes\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/triton*"
      ],
      "metadata": {
        "id": "mL5fD3Blolwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "DHns--9TooJS",
        "outputId": "8bdecbfc-1b80-4c55-a50b-73c6cf1ba737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.11.1.6)\n",
            "Collecting triton==3.3.0 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Using cached triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch<3,>=2.0->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "Using cached triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "Installing collected packages: triton, bitsandbytes\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 triton-3.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "755f3e7328214bcf9c30743a4cb97c25"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Initialize Accelerator\n",
        "accelerator = Accelerator()\n",
        "device = accelerator.device\n",
        "\n",
        "# Load the model and tokenizer (without 4-bit quantization)\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=2,  # Even lower rank for this test\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Get the LoRA model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944,
          "referenced_widgets": [
            "601cf8fa62824fb5a88cb87d5a179b58",
            "7b1272275b344eb195d68b2bb034c600",
            "df660fe15c184f4a8fc75026bd4a02d8",
            "90fcce6bb7504d9aaece3853c4454b64",
            "aa2c54aba8204cb0aac691daea0bf1a5",
            "a98892c3f7ef460da46a81fbda4d172c",
            "7e4e0ca9328f487eb37514f99b4ce69e",
            "247f9646b8df4382ba605731e7cc5c55",
            "09b4adb85bc547cc94d844eb11cbdfe7",
            "35e8c6f5d6ec4265a554733831983d33",
            "f0a83a7fb5f94a91aaeb2ccb9387a986"
          ]
        },
        "id": "qfbULwrppDvz",
        "outputId": "eec1ff52-3b94-46d6-ab6a-65a7e6f5d1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "601cf8fa62824fb5a88cb87d5a179b58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,997,120 || all params: 6,743,412,736 || trainable%: 0.0741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a balanced sample of the datasets for testing\n",
        "def balanced_sample(df, n=500, label_column='is_wildfire_ca'): # Choose a relevant label column\n",
        "    if label_column not in df.columns:\n",
        "        if 'distress_label_instruction' in df.columns:\n",
        "            label_column = 'distress_label_instruction'\n",
        "        elif 'location_label_instruction' in df.columns:\n",
        "            label_column = 'location_label_instruction'\n",
        "        elif 'action_label_instruction' in df.columns:\n",
        "             label_column = 'action_label_instruction'\n",
        "        else:\n",
        "            return df.head(min(n, len(df)))  # If no suitable label column, take the head\n",
        "\n",
        "    grouped = df.groupby(label_column)\n",
        "    sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
        "    sampled_df = sampled_df.reset_index(drop=True)\n",
        "    return sampled_df\n",
        "\n",
        "train_df = pd.read_csv('train_instruction_tuning.csv')\n",
        "val_df = pd.read_csv('val_instruction_tuning.csv')\n",
        "\n",
        "train_df = balanced_sample(train_df, n=500)\n",
        "val_df = balanced_sample(val_df, n=500)\n",
        "\n",
        "def format_instruction(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: {row['tweet_text']}\"\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return f\"You are an emergency detection system. Determine if the tweet clearly indicates distress. Input: {row['tweet_text']}\"\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return f\"Extract the most specific real-world geographic location mentioned in the tweet. Input: {row['tweet_text']}\"\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action. Input: {row['tweet_text']}\"\n",
        "    return \"\"\n",
        "\n",
        "def format_output(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return row['is_wildfire_ca']\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return row['distress_label_instruction']\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return row['location_label_instruction']\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return row['action_label_instruction']\n",
        "    return \"\"\n",
        "\n",
        "train_df['instruction'] = train_df.apply(format_instruction, axis=1)\n",
        "train_df['output'] = train_df.apply(format_output, axis=1)\n",
        "train_df['input'] = train_df['tweet_text']\n",
        "train_df_filtered = train_df[train_df['instruction'] != \"\"]\n",
        "train_dataset = Dataset.from_pandas(train_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "val_df['instruction'] = val_df.apply(format_instruction, axis=1)\n",
        "val_df['output'] = val_df.apply(format_output, axis=1)\n",
        "val_df['input'] = val_df['tweet_text']\n",
        "val_df_filtered = val_df[val_df['instruction'] != \"\"]\n",
        "val_dataset = Dataset.from_pandas(val_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "max_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(examples['instruction'], examples['input'])]\n",
        "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples['output']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172,
          "referenced_widgets": [
            "083cd610783645a9b7998e450833932a",
            "44e047da85b04a81879f0e9417e8be58",
            "d6066f54a70e4522862de406ff3e190d",
            "3b076cb15fb74122ae9325a2aa2bc575",
            "0208adeb492c4ac4adfb414df590d427",
            "f3464664f8994d7390aa6bce8770905c",
            "7f2b0d7c6c71472397345480b1baa9d0",
            "22e9bc91d8ac4622b501ac5ad067ab32",
            "317adb5428a34d04a3e9e03e6ff23a74",
            "239e05efcc004e50ad1bca956170bda9",
            "41b4b8bb9c0b4e22befd2ce0c595581e",
            "6afff58d763343ffb4e06197d2398417",
            "fd92083587bf45a1bae5a9b128c765a2",
            "48de575f93d341b6961b7c2ba582674a",
            "0a01d094933f4c848c7c648cddea407f",
            "6859cae5237749168caf8c72d5822960",
            "60e631c3a11940bda33d193d8d55a91a",
            "f0e7e3f5e864494495022ccd24b1fb62",
            "24d5b1c0501f48138bd35f9681bc797d",
            "898321c9a11c40b9b893f57a4561afcd",
            "80a2afd4b5bc41c48ae9953846dab6e9",
            "fdf8265a56d64735bc33a690c887064f"
          ]
        },
        "id": "7I9i5c5UuKPZ",
        "outputId": "1ef490c1-5649-4979-c2f3-b896098f21a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2787b3a4bab0>:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
            "<ipython-input-3-2787b3a4bab0>:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "083cd610783645a9b7998e450833932a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6afff58d763343ffb4e06197d2398417"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(r=2, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"])\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "b77835cb376c4e6e88e80f78bfd90d05",
            "a80e3fe44dee4f69a61ecba991b1b095",
            "653ce373c1604db58be15bbeb24d61f0",
            "6bb80f34ee7e460b91cbca5bdf2504aa",
            "09e52f26d0fe4f29bc33d190c958c570",
            "6462c37341254c9dbffba655bdbaa630",
            "616a00eb0655405889f4b7f34246d146",
            "792d8c5b6c3c4dfc948e4f4300f5807c",
            "f5e4cb14655445a3b8459f022b216ab6",
            "f7382e6a0e7f47d9b4e88487e4801fe5",
            "1fc514eba36f41e9be1fbf5fe23447b1"
          ]
        },
        "id": "5mLPsf7AAIrp",
        "outputId": "a8741811-9e50-4f6e-ecf1-51ef76057c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b77835cb376c4e6e88e80f78bfd90d05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,997,120 || all params: 6,743,412,736 || trainable%: 0.0741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load a balanced sample of the datasets for testing\n",
        "def balanced_sample(df, n=500, label_column='is_wildfire_ca'): # Choose a relevant label column\n",
        "    if label_column not in df.columns:\n",
        "        if 'distress_label_instruction' in df.columns:\n",
        "            label_column = 'distress_label_instruction'\n",
        "        elif 'location_label_instruction' in df.columns:\n",
        "            label_column = 'location_label_instruction'\n",
        "        elif 'action_label_instruction' in df.columns:\n",
        "             label_column = 'action_label_instruction'\n",
        "        else:\n",
        "            return df.head(min(n, len(df)))  # If no suitable label column, take the head\n",
        "\n",
        "    grouped = df.groupby(label_column)\n",
        "    sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
        "    sampled_df = sampled_df.reset_index(drop=True)\n",
        "    return sampled_df\n",
        "\n",
        "train_df = pd.read_csv('train_instruction_tuning.csv')\n",
        "val_df = pd.read_csv('val_instruction_tuning.csv')\n",
        "\n",
        "train_df = balanced_sample(train_df, n=500)\n",
        "val_df = balanced_sample(val_df, n=500)\n",
        "\n",
        "def format_instruction(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return f\"You are a wildfire classifier. Determine if the tweet is explicitly about a wildfire happening in California. Input: {row['tweet_text']}\"\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return f\"You are an emergency detection system. Determine if the tweet clearly indicates distress. Input: {row['tweet_text']}\"\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return f\"Extract the most specific real-world geographic location mentioned in the tweet. Input: {row['tweet_text']}\"\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return f\"You are a disaster response coordinator. Based on the content of the tweet, recommend the most urgent emergency action. Input: {row['tweet_text']}\"\n",
        "    return \"\"\n",
        "\n",
        "def format_output(row):\n",
        "    if row['is_wildfire_ca'] in ['yes', 'no']:\n",
        "        return row['is_wildfire_ca']\n",
        "    elif row['distress_label_instruction'] in ['distress', 'not distress']:\n",
        "        return row['distress_label_instruction']\n",
        "    elif row['location_label_instruction'] != 'unknown':\n",
        "        return row['location_label_instruction']\n",
        "    elif row['action_label_instruction'] in ['evacuation', 'medical aid', 'fire suppression', 'rescue', 'resource delivery', 'monitor only']:\n",
        "        return row['action_label_instruction']\n",
        "    return \"\"\n",
        "\n",
        "train_df['instruction'] = train_df.apply(format_instruction, axis=1)\n",
        "train_df['output'] = train_df.apply(format_output, axis=1)\n",
        "train_df['input'] = train_df['tweet_text']\n",
        "train_df_filtered = train_df[train_df['instruction'] != \"\"]\n",
        "train_dataset = Dataset.from_pandas(train_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "val_df['instruction'] = val_df.apply(format_instruction, axis=1)\n",
        "val_df['output'] = val_df.apply(format_output, axis=1)\n",
        "val_df['input'] = val_df['tweet_text']\n",
        "val_df_filtered = val_df[val_df['instruction'] != \"\"]\n",
        "val_dataset = Dataset.from_pandas(val_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "max_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(examples['instruction'], examples['input'])]\n",
        "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples['output']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172,
          "referenced_widgets": [
            "e4643bf871554643a4275f5f47d299b8",
            "c5fe3792867745d58d0ad38b8c1ade68",
            "463a0c88eef443228715fedac6473fcb",
            "a80dba283f964fd6a12c52c9b9f9a171",
            "41575976371c4061afe24878cf7404fb",
            "658453ecf2024545ada278ae17a2b995",
            "f9efbe1bec76434ebf9b3844c5362e07",
            "239a3472de594dcc91870c96f2d002af",
            "391a959d88de4f8b9b9bbaae520d3599",
            "ca781216344b4c0bbda5f3ec4b8bc0e0",
            "0888b52e3c0540d38ca5c932b8709425",
            "9fdc4bf37be340c28b75c20d002357c8",
            "251e3bcc10754548b1186963481f1509",
            "047a10ebb20048dc932d047ae61b616d",
            "6316a82d393047a89f6416f05d916fae",
            "c8a0b48d42ef4241b5cda34f1a5d4744",
            "de5d12aca3fb40378260d93299a6059b",
            "e3f3628ded8a474e899af4e742832b26",
            "b431613cf3cd46219b451ded3fc9909b",
            "339e7a3e01bd46a38ea5f5b6b31ce836",
            "bb9e43db329a458b8fb0f890b7f118d9",
            "c80032adb153415792d27d4aa57605a6"
          ]
        },
        "id": "0tank3IaAI5E",
        "outputId": "6a8117ae-ab60-4200-8730-cf5d78e4649d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2da18f75d215>:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
            "<ipython-input-7-2da18f75d215>:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4643bf871554643a4275f5f47d299b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/414 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fdc4bf37be340c28b75c20d002357c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset  # We won't try to import load_metric here\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from datasets.metric import Metric  # Import the Metric class\n",
        "\n",
        "# Explicitly disable use_cache in the model config\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Metric to use for evaluation\n",
        "metric = Metric.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Training arguments (gradient checkpointing disabled)\n",
        "output_dir = \"./llama-7b-wildfire-multi-task-lora-no-gc\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs-lora-no-gc\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        "    max_grad_norm=1.0,\n",
        "    optim=\"adamw_torch\",\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    gradient_checkpointing=False,  # Gradient checkpointing disabled\n",
        "    per_device_eval_batch_size=2,  # Reduced evaluation batch size\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,  # Pass the compute_metrics function\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model after training\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "_lm4Aiia_CcL",
        "outputId": "d1a628d8-5f9a-4dbc-eec7-55f22ad6f60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets.metric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c5b68b76581c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m  \u001b[0;31m# We won't try to import load_metric here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForSeq2Seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetric\u001b[0m  \u001b[0;31m# Import the Metric class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Explicitly disable use_cache in the model config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets.metric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes peft transformers datasets\n",
        "from huggingface_hub import login\n",
        "login(\"**********************\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VYCH0HnHbPn",
        "outputId": "9c5511fd-ef5b-4a6e-bd8f-68a357dfc0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting peft\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bitsandbytes, peft\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.15.2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=2,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514,
          "referenced_widgets": [
            "2104ac0474514f95a42b7387d42e9366",
            "4f653cb3630440a6805bc562bcb2034e",
            "388d6106279b4189bb432e836709caf3",
            "a6c3e43533394cfe9cfe9752c527afaf",
            "c62cf984240d446ab1c8a71f11b5df66",
            "d9ccc2dc211f477eae075da8fcd66c25",
            "7f0f2eccec6842c1ae30ba24f5831d80",
            "83cdf57335024de8b50cea9e47b8471a",
            "9024f713e3ae4920b5e3916a77840f93",
            "5842521be4044011ba9da4ae4267f0a8",
            "5330e3adec2445fea7acbe8c4f78c1e9",
            "cc58278bc1c943139288394f19e641f8",
            "61da9ab943b04409a4ea8af77751e7dc",
            "46afde21174c423eb085dad74fc3c1d8",
            "d5cd70f7bde44c11af47a0cbdfa9a3ed",
            "eac198c2b76d4ed4938415886c27f9d9",
            "1679285db54f4281a833477df5d125e1",
            "fff7b6e991cb4bfe997a8b6e2ad9371f",
            "68815aeed48443a78105b0a902ed282b",
            "ba30a19e85f3485fa86f267d56368c7f",
            "8ff68682e8de4db5951dcd3988214262",
            "92850207383344f7902d04af5550baec",
            "370bf3d5f1cc4bd8942ab21cf521309e",
            "ba79a579296e4b9a8447e51a94fbf24f",
            "70541e196fe6424196f1cbe7b827af9b",
            "072c15aba3a84bb1baa129079a0db61b",
            "d6ef0af66c7e4a7ea5ee169fa4e45cd0",
            "dc6eece1056742a7a341fc0a1e9026e7",
            "d125aa763c8c4ee9ae37eb692d0c86c7",
            "44befd36156b4e06927668e7922134a2",
            "c00a78b174584d14a350609df563b530",
            "79d57bba3be840048baf6e2e149bd6c6",
            "f4530ecfa6634b06ad1e87b406049a7c",
            "60fff09ee5ff40b284dfe5413cc88c60",
            "bda87ab57cbd4467a684381d5c0a3fb0",
            "24410c244b5542818084333ade45b123",
            "73b2deb0bf5e4073be2fa97c14ef2672",
            "07c56fce14f048878ade9446514f0043",
            "85cac8c8c27542798593d913eb9fcb4d",
            "0629e9ea1c77429094756ad394aa7214",
            "5e973fd15380408c930d5077fcaf6306",
            "0952c6c442b4440da196ca5a9ef25f19",
            "f7bf7c7d91644330aec5e4fbfd480ea5",
            "cd416835d7634737be322b41a6c44e04",
            "0889bab6506c461b8d2dea77c8c0c96b",
            "edbf4c1558e04f169fe610d82803c95a",
            "e445b20fdfde461a8318a0a12e74fa7d",
            "5c6002933a2344f6b9a1abff524aa020",
            "962fbba7f27c4a26acb7043be97faf18",
            "3c62d11ec474485d87a24e67834ae793",
            "86bba8f813324658b983caaef96dc0b7",
            "39a97b64431046798ffc5185bbef594b",
            "8b0d287fafee4ebcbf873bc08b4f1977",
            "ef61e6d333cf42e7bbcfb24b84c0941c",
            "e2d4cc535b2843a1afab2fb85a1646e1",
            "bd07acf8e0394f888b426177d43c43d0",
            "8f423b93fef741dbb33fc558bf6c6f35",
            "3aa8766830c744e5a49a3fc3552a5903",
            "933f26ac67e34443b6cb861d16bef8a7",
            "4ee79de79aeb491fb4b6c81e7f06cb9d",
            "69b4b6d020174059919211c9f607143c",
            "665f8a3187274ddba3fdbaf205aaa2d1",
            "1156e3d625ae4bbdb1f1d7b433c86016",
            "4f73758bd5194beb9c796efe75e81894",
            "4f95a368225f49a38af69aef77ec9b12",
            "d3088cd9082347e1943bcb63bc80f3ad",
            "55dfc7822c074e80a6f6017a09fe6536",
            "15d6e28c34c44902be5e716694686eec",
            "ae4fa3f1211a4245904dcf3f3748fd73",
            "571a97bab30c471f9ad741dec110db61",
            "ff6e1f733efb47238db9a6b71d9d8a06",
            "162c6cfd08ac47e592e472c4aad9f4b0",
            "ecfa65f5d6f74dde845d2d08ede7255d",
            "f50c22ffbc5e4736883f05383aea33f7",
            "007f55f1c7004a09894cebe60b12d068",
            "c0a975298f6041bab6555e6508183197",
            "5cde6d237df045c7b430080db545db16",
            "5da136472eda4752ba80fb1819a823d6",
            "04639b53ee9445d4acd22aa6d2e8e2b3",
            "89d273e95dd54c05aee1bd8aeb8c5678",
            "527bef38aac94be388d22e8373fa2f66",
            "f9c215aadbaf4a9fa177a8d7d1a50e46",
            "b221e5ad08dd4e3e92cee6765f600142",
            "6b922e30dbfb4651aa473526e56093d7",
            "c373caabe22047678f2fe5e874110d10",
            "38d9034531b24c068367482f7790ff87",
            "86cbbc1750e6477287eed7b1bcdb74f0",
            "50506f0027c1407486216c994ea53a6f",
            "44c92c499e024b55bd84977fe6991c9f",
            "38bbd1b7c4624b27ba183aac63a55f5d",
            "2e88255d47e845e58e27aac3fdc526f9",
            "422d8adbfa19476783fcb7519de66ee4",
            "77fc5c6896b04b79b75b9d0ab7a30621",
            "df015e66efd845cf973e14563da7d155",
            "bbe0b3f463c04ae1ae6c48c4948409f8",
            "e5110abbaf154a378b8fa5bf6dd1e332",
            "dc7ae0582bef4eb3a37c2d1fb040fe8f",
            "e5bd5bd62a3348b0ae8eaa7f8282621a",
            "25cceca2b2b34e55859f0de5c3363056",
            "fd8fc903ef264506bad7517da90fe56b",
            "a3b5d675806d4e37b090dcb019eb5a35",
            "c14797faacb346a1983a8aa3d1bd76a6",
            "31565735535a4ec6957be6cdc36a58b0",
            "52af6ece88b246dcaa0b0aaeda6cd58d",
            "49ed7df865ef45a2806bb99928a22ed5",
            "7c877386ef654584a6756b0f28035e4a",
            "2d32640162774e959d1522e50334f7fe",
            "78b7b608070b4b05be1b48b2e773587f",
            "6368df6c6a5c45f48f1310ab5740df45",
            "946532b3bb204b6a85ea7b31c43a150d",
            "09dc82f9536a4828a400c133c24f77ca",
            "cb3b2b92a9474ba78cc4383a11b2b4c5",
            "5031a61f8d674bd19ba80b07c86bcd53",
            "95f73134eefa47e89bf31062efc4670c",
            "58464dfedbd9444186643b29095e683d",
            "8927d36418f74325bbe65119a3ffbb0b",
            "089347c771b34665bcdb32e6758c465d",
            "0ba8ec54343e4feab58fdbf5296e3f63",
            "4ba28963e2d145c8b678d5da77df0726",
            "7844a64be95f4b58b9e5393d4e3b6dfe",
            "8dd61266d0e442eebd4517669840ae77"
          ]
        },
        "id": "agMmB3c1IY3v",
        "outputId": "96fff55a-39eb-4584-dd78-fd0384043175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2104ac0474514f95a42b7387d42e9366"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc58278bc1c943139288394f19e641f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "370bf3d5f1cc4bd8942ab21cf521309e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60fff09ee5ff40b284dfe5413cc88c60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0889bab6506c461b8d2dea77c8c0c96b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd07acf8e0394f888b426177d43c43d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55dfc7822c074e80a6f6017a09fe6536"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5da136472eda4752ba80fb1819a823d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44c92c499e024b55bd84977fe6991c9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd8fc903ef264506bad7517da90fe56b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09dc82f9536a4828a400c133c24f77ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,997,120 || all params: 6,743,412,736 || trainable%: 0.0741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load a balanced sample of the datasets for testing\n",
        "def balanced_sample(df, n=500, label_column=None):\n",
        "    if 'Wildfire' in df.columns:\n",
        "        label_column = 'Wildfire'\n",
        "    elif 'distress' in df.columns:\n",
        "        label_column = 'distress'\n",
        "    elif 'state' in df.columns:\n",
        "        label_column = 'state'\n",
        "    elif 'take_action' in df.columns:\n",
        "        label_column = 'take_action'\n",
        "    else:\n",
        "        return df.head(min(n, len(df)))  # If no suitable label column, take the head\n",
        "\n",
        "    if label_column:\n",
        "        grouped = df.groupby(label_column)\n",
        "        sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
        "        sampled_df = sampled_df.reset_index(drop=True)\n",
        "        return sampled_df\n",
        "    else:\n",
        "        return df.head(min(n, len(df)))\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/llama_train_balanced.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/llama_val_balanced.csv')\n",
        "\n",
        "train_df = balanced_sample(train_df, n=500)\n",
        "val_df = balanced_sample(val_df, n=500)\n",
        "\n",
        "def format_instruction(row):\n",
        "    if 'Wildfire' in row and row['Wildfire'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return f\"You are a wildfire classifier. Determine if the tweet is about a wildfire. Input: {row['tweet_text']}\"\n",
        "    elif 'distress' in row and row['distress'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return f\"You are an emergency detection system. Determine if the tweet indicates distress. Input: {row['tweet_text']}\"\n",
        "    elif 'state' in row and isinstance(row['state'], str) and row['state'].lower() != 'none':\n",
        "        return f\"Identify the U.S. state mentioned in the tweet. Input: {row['tweet_text']}\"\n",
        "    elif 'take_action' in row and isinstance(row['take_action'], str) and row['take_action'].lower() != 'none':\n",
        "        return f\"Recommend the appropriate action based on the tweet. Input: {row['tweet_text']}\"\n",
        "    return \"\"\n",
        "\n",
        "def format_output(row):\n",
        "    if 'Wildfire' in row and row['Wildfire'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return str(row['Wildfire'])\n",
        "    elif 'distress' in row and row['distress'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return str(row['distress'])\n",
        "    elif 'state' in row and isinstance(row['state'], str) and row['state'].lower() != 'none':\n",
        "        return row['state']\n",
        "    elif 'take_action' in row and isinstance(row['take_action'], str) and row['take_action'].lower() != 'none':\n",
        "        return row['take_action']\n",
        "    return \"\"\n",
        "\n",
        "train_df['instruction'] = train_df.apply(format_instruction, axis=1)\n",
        "train_df['output'] = train_df.apply(format_output, axis=1)\n",
        "train_df['input'] = train_df['tweet_text']\n",
        "train_df_filtered = train_df[train_df['instruction'] != \"\"]\n",
        "train_dataset = Dataset.from_pandas(train_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "val_df['instruction'] = val_df.apply(format_instruction, axis=1)\n",
        "val_df['output'] = val_df.apply(format_output, axis=1)\n",
        "val_df['input'] = val_df['tweet_text']\n",
        "val_df_filtered = val_df[val_df['instruction'] != \"\"]\n",
        "val_dataset = Dataset.from_pandas(val_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "max_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(examples['instruction'], examples['input'])]\n",
        "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples['output']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172,
          "referenced_widgets": [
            "4b43067aa7a648e38fc6aaca3c44839f",
            "4afd3a09468443a3a1b36f2edb0a58a9",
            "d4caf020189b47d49e25c998faea33df",
            "e8ff3934f0c94c52abb17bd4d2c3da37",
            "b9101474adec4a2aad94a5bdfc967c8a",
            "3e1d592842724d5daa1d8dc4b8281e0a",
            "440b1f8a443144ee8e56d2553775b97a",
            "e93fc697c0a04ab6aef5aefcdbea804a",
            "86f0130b65614220b9d90a28df715963",
            "3aa836abb18942e2b3d50c10f71a3e2e",
            "ac5341e163cb40548b9e26da7c888483",
            "d6ebe78c5c8a4b78829332707706357a",
            "676019d50cd44071a1006d3198e513ea",
            "ed9d9ba6f8294d7b88ebf2100268026b",
            "102f59a8cf494434a5848c764fc1c27d",
            "0cbe783d57184c25a3189bc5208ab059",
            "35c7facb195d46598bd882c84cc6018e",
            "52c775a3c8c345d6b62b8557c1f33023",
            "2ad13c2ec85e46808745ebf23439663c",
            "1517e4e366b14306b3f39841df80dbab",
            "f44aedc47dd044898afe33742b66a31c",
            "245e81eb63c743b3a51253ae64055596"
          ]
        },
        "id": "IVY78VoAIY6X",
        "outputId": "654b83e6-42bf-4b0c-8dc4-3e9ec6a37745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-238a040f47aa>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
            "<ipython-input-11-238a040f47aa>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b43067aa7a648e38fc6aaca3c44839f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6ebe78c5c8a4b78829332707706357a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Explicitly disable use_cache in the model config\n",
        "model.config.use_cache = False\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Remove padding tokens (-100 is often the ignore_index)\n",
        "    mask = labels != -100\n",
        "    predictions_masked = predictions[mask]\n",
        "    labels_masked = labels[mask]\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels_masked, predictions_masked),\n",
        "        'precision': precision_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "        'f1': f1_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "    }\n",
        "\n",
        "# Training arguments (gradient checkpointing disabled)\n",
        "output_dir = \"./llama-7b-wildfire-multi-task-lora-no-gc\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs-lora-no-gc\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        "    max_grad_norm=1.0,\n",
        "    optim=\"adamw_torch\",\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    gradient_checkpointing=False,  # Gradient checkpointing disabled\n",
        "    per_device_eval_batch_size=2,  # Reduced evaluation batch size\n",
        "    eval_steps=100,  # Evaluate every 100 training steps\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,  # Use the scikit-learn based compute_metrics\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model after training (optional, as evaluation will happen during training)\n",
        "# evaluation_results = trainer.evaluate()\n",
        "# print(\"\\nEvaluation Results:\")\n",
        "# print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "na4Y2MyhLJaR",
        "outputId": "3b12f3cb-dc14-4b0f-fa9e-e5bd1f63676f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [186/186 06:59, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>18.004100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.003100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.003300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.003400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=186, training_loss=1.0447998671261693, metrics={'train_runtime': 423.2597, 'train_samples_per_second': 3.544, 'train_steps_per_second': 0.439, 'total_flos': 7515850974167040.0, 'train_loss': 1.0447998671261693, 'epoch': 2.96})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model after training (optional, as evaluation will happen during training)\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "3VEQ__TQNCK1",
        "outputId": "49ea03f2-4a6a-4386-95f3-ff5e77334561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='179' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [179/179 00:31]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "{'eval_loss': 0.002721394645050168, 'eval_accuracy': 0.9765625, 'eval_precision': 0.9688120039682541, 'eval_recall': 0.9765625, 'eval_f1': 0.9726718127490039, 'eval_runtime': 36.3999, 'eval_samples_per_second': 9.835, 'eval_steps_per_second': 4.918, 'epoch': 2.96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Take a small sample from the validation dataset\n",
        "sample_size = 5\n",
        "sample_indices = [10, 50, 100, 150, 200]  # You can choose different indices\n",
        "sample_val_dataset = val_dataset.select(sample_indices)\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "for example in sample_val_dataset:\n",
        "    instruction = example['instruction']\n",
        "    input_text = example['input']\n",
        "    true_output = example['output']\n",
        "\n",
        "    input_prompt = f\"{instruction}\\n{input_text}\"\n",
        "    input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids=input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    predicted_output = tokenizer.decode(output[0], skip_special_tokens=True).split('\\n')[-1].strip()\n",
        "\n",
        "    print(f\"**Instruction:** {instruction}\")\n",
        "    print(f\"**Input:** {input_text}\")\n",
        "    print(f\"**True Output:** {true_output}\")\n",
        "    print(f\"**Predicted Output:** {predicted_output}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "model.train()  # Set the model back to training mode if you plan to continue training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CFminuiNPQi",
        "outputId": "f45f7ef2-d3a7-46b2-d681-c07c0cfca82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Instruction:** You are an emergency detection system. Determine if the tweet indicates distress. Input: photo of the day a boy bathes with mountain spring water in utuado puerto rico\n",
            "**Input:** photo of the day a boy bathes with mountain spring water in utuado puerto rico\n",
            "**True Output:** 0\n",
            "**Predicted Output:** photo of the day a boy bathes with mountain spring water in utuado puerto rico\n",
            "--------------------------------------------------\n",
            "**Instruction:** You are an emergency detection system. Determine if the tweet indicates distress. Input: me when irma hit\n",
            "**Input:** me when irma hit\n",
            "**True Output:** 0\n",
            "**Predicted Output:** me when irma hit\n",
            "--------------------------------------------------\n",
            "**Instruction:** You are an emergency detection system. Determine if the tweet indicates distress. Input: you know whats more useless than trump in puerto ricopaperfuckingtowels\n",
            "**Input:** you know whats more useless than trump in puerto ricopaperfuckingtowels\n",
            "**True Output:** 0\n",
            "**Predicted Output:** you know whats more useless than trump in puerto ricopaperfuckingtowels\n",
            "--------------------------------------------------\n",
            "**Instruction:** You are an emergency detection system. Determine if the tweet indicates distress. Input: interesting coincidence that the earthquake took place exactly on the artificial manmade border of iraq and iran\n",
            "**Input:** interesting coincidence that the earthquake took place exactly on the artificial manmade border of iraq and iran\n",
            "**True Output:** 0\n",
            "**Predicted Output:** interesting coincidence that the earthquake took place exactly on the artificial manmade border of iraq and iran\n",
            "--------------------------------------------------\n",
            "**Instruction:** You are an emergency detection system. Determine if the tweet indicates distress. Input: #cnn chief #jeffzucker keeps his network on a breaking news course in the wake of irma\n",
            "**Input:** #cnn chief #jeffzucker keeps his network on a breaking news course in the wake of irma\n",
            "**True Output:** 0\n",
            "**Predicted Output:** #cnn chief #jeffzucker keeps his network on a breaking news course in the wake of irma\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=11008, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=11008, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=11008, out_features=2, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Take a random sample from the validation dataset\n",
        "sample_size = 50\n",
        "random_indices = random.sample(range(len(val_dataset)), sample_size)\n",
        "sample_val_dataset = val_dataset.select(random_indices)\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "grouped_results = defaultdict(list)\n",
        "\n",
        "for example in sample_val_dataset:\n",
        "    input_text = example['input']\n",
        "    true_output = example['output']  # This will be the output corresponding to the *first* instruction type\n",
        "\n",
        "    # Determine the group based on the true output (you might need to adjust this logic)\n",
        "    group = f\"{true_output.upper()} Tweet Group\"\n",
        "\n",
        "    instruction = example['instruction']\n",
        "    input_prompt = f\"{instruction}\\n{input_text}\"\n",
        "    input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids=input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "    predicted_output = tokenizer.decode(output[0], skip_special_tokens=True).split('\\n')[-1].strip()\n",
        "\n",
        "    grouped_results[group].append({\n",
        "        \"instruction\": instruction,\n",
        "        \"input\": input_text,\n",
        "        \"true_output\": true_output,\n",
        "        \"predicted_output\": predicted_output,\n",
        "    })\n",
        "\n",
        "model.train()  # Set the model back to training mode\n",
        "\n",
        "for group, results in grouped_results.items():\n",
        "    print(f\"============================== {group} ==============================\")\n",
        "    for res in results:\n",
        "        print(f\"Instruction: {res['instruction']}\")\n",
        "        print(f\"Input:       {res['input']}\")\n",
        "        print(f\"True Output: {res['true_output']}\")\n",
        "        print(f\"Response:    {res['predicted_output']}\")\n",
        "        print(\"-\" * 70)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUfhFWRZNuOy",
        "outputId": "6c31c271-8667-4adc-dd0b-855393d7c3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== 0 Tweet Group ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: hit hard by california wildfires santa rosa faces housing crisis\n",
            "Input:       hit hard by california wildfires santa rosa faces housing crisis\n",
            "True Output: 0\n",
            "Response:    hit hard by california wildfires santa rosa faces housing crisis\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: lord we pray you give mercy on california and ease the winds today so firemen can get a handle of this fire\n",
            "Input:       lord we pray you give mercy on california and ease the winds today so firemen can get a handle of this fire\n",
            "True Output: 0\n",
            "Response:    lord we pray you give mercy on california and ease the winds today so firemen can get a handle of this fire\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: 71 magnitude terremoto earthquake mexico 19092017\n",
            "Input:       71 magnitude terremoto earthquake mexico 19092017\n",
            "True Output: 0\n",
            "Response:    71 magnitude terremoto earthquake mexico 19092017\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: harvey #thedognotthestorm is filling a huge hole in our hearts\n",
            "Input:       harvey #thedognotthestorm is filling a huge hole in our hearts\n",
            "True Output: 0\n",
            "Response:    harvey #thedognotthestorm is filling a huge hole in our hearts\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: if all else fails#irma #harvey\n",
            "Input:       if all else fails#irma #harvey\n",
            "True Output: 0\n",
            "Response:    if all else fails#irma #harvey\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: kill people burn shit fuck school 666 ofwgkta i punch bitches stop kony 2012 fuck steve harvey\n",
            "Input:       kill people burn shit fuck school 666 ofwgkta i punch bitches stop kony 2012 fuck steve harvey\n",
            "True Output: 0\n",
            "Response:    kill people burn shit fuck school 666 ofwgkta i punch bitches stop kony 2012 fuck steve harvey\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: half of california is on fire and my flight keeps getting delayed\n",
            "Input:       half of california is on fire and my flight keeps getting delayed\n",
            "True Output: 0\n",
            "Response:    half of california is on fire and my flight keeps getting delayed\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: nope i voted obama and kerry i will never vote dems again you are liars\n",
            "Input:       nope i voted obama and kerry i will never vote dems again you are liars\n",
            "True Output: 0\n",
            "Response:    nope i voted obama and kerry i will never vote dems again you are liars\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: hurricane harveys impact on coastal habitatinitial updates from texas\n",
            "Input:       hurricane harveys impact on coastal habitatinitial updates from texas\n",
            "True Output: 0\n",
            "Response:    hurricane harveys impact on coastal habitatinitial updates from texas\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: britain braced for weather carnage as two recordbreaking monster storms set to strike\n",
            "Input:       britain braced for weather carnage as two recordbreaking monster storms set to strike\n",
            "True Output: 0\n",
            "Response:    britain braced for weather carnage as two recordbreaking monster storms set to strike\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: on hurricane irma we saw upwards of a foot of rainfall\n",
            "Input:       on hurricane irma we saw upwards of a foot of rainfall\n",
            "True Output: 0\n",
            "Response:    on hurricane irma we saw upwards of a foot of rainfall\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: fema sold off more than 100 disaster relief trailers at auction in days before harvey hit\n",
            "Input:       fema sold off more than 100 disaster relief trailers at auction in days before harvey hit\n",
            "True Output: 0\n",
            "Response:    fema sold off more than 100 disaster relief trailers at auction in days before harvey hit\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: in puerto rico can you upgrade your taps to the ones that are in toronto more durable and strong\n",
            "Input:       in puerto rico can you upgrade your taps to the ones that are in toronto more durable and strong\n",
            "True Output: 0\n",
            "Response:    in puerto rico can you upgrade your taps to the ones that are in toronto more durable and strong\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: as a class apostolic project pk3 pk4 collected items for hurricane harvey evacuees in irving shelters\n",
            "Input:       as a class apostolic project pk3 pk4 collected items for hurricane harvey evacuees in irving shelters\n",
            "True Output: 0\n",
            "Response:    as a class apostolic project pk3 pk4 collected items for hurricane harvey evacuees in irving shelters\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: ufc star henry cujedo burned while escaping from california wildfires\n",
            "Input:       ufc star henry cujedo burned while escaping from california wildfires\n",
            "True Output: 0\n",
            "Response:    ufc star henry cujedo burned while escaping from california wildfires\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: are you a veteran business owner active duty military spouse sdvosb were you impacted by #hurricaneirma\n",
            "Input:       are you a veteran business owner active duty military spouse sdvosb were you impacted by #hurricaneirma\n",
            "True Output: 0\n",
            "Response:    are you a veteran business owner active duty military spouse sdvosb were you impacted by #hurricaneirma\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: #np x ale mora party people #npstr067\n",
            "Input:       #np x ale mora party people #npstr067\n",
            "True Output: 0\n",
            "Response:    #np x ale mora party people #npstr067\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: a flight attendant is outraged she was charged 187 to park her vehicle at mia while she worked during hurricane ir\n",
            "Input:       a flight attendant is outraged she was charged 187 to park her vehicle at mia while she worked during hurricane ir\n",
            "True Output: 0\n",
            "Response:    a flight attendant is outraged she was charged 187 to park her vehicle at mia while she worked during hurricane ir\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: some humor from harvey\n",
            "Input:       some humor from harvey\n",
            "True Output: 0\n",
            "Response:    some humor from harvey\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: first day fun license plate speed dating pink floyd lyric analysis amp letter writing to hurricane harvey victims\n",
            "Input:       first day fun license plate speed dating pink floyd lyric analysis amp letter writing to hurricane harvey victims\n",
            "True Output: 0\n",
            "Response:    first day fun license plate speed dating pink floyd lyric analysis amp letter writing to hurricane harvey victims\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: we are only one hour away from our live updates for the wildfiresbrush fires in california\n",
            "Input:       we are only one hour away from our live updates for the wildfiresbrush fires in california\n",
            "True Output: 0\n",
            "Response:    we are only one hour away from our live updates for the wildfiresbrush fires in california\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: texas waiter for saying he hopes cheap harvey evacuees die slow gt\n",
            "Input:       texas waiter for saying he hopes cheap harvey evacuees die slow gt\n",
            "True Output: 0\n",
            "Response:    texas waiter for saying he hopes cheap harvey evacuees die slow gt\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: california wildfires wine costs could spike but thats not all\n",
            "Input:       california wildfires wine costs could spike but thats not all\n",
            "True Output: 0\n",
            "Response:    california wildfires wine costs could spike but thats not all\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: update firefighters now say 17 people have died from the ongoing california wildfires\n",
            "Input:       update firefighters now say 17 people have died from the ongoing california wildfires\n",
            "True Output: 0\n",
            "Response:    update firefighters now say 17 people have died from the ongoing california wildfires\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: #mexico #earthquake #race to #find #survivors under #collaps\n",
            "Input:       #mexico #earthquake #race to #find #survivors under #collaps\n",
            "True Output: 0\n",
            "Response:    #mexico #earthquake #race to #find #survivors under #collaps\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: a selection of sunday front pages focusing on the california wildfires\n",
            "Input:       a selection of sunday front pages focusing on the california wildfires\n",
            "True Output: 0\n",
            "Response:    a selection of sunday front pages focusing on the california wildfires\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: dem rep soto hurricane maria has already become a katrina\n",
            "Input:       dem rep soto hurricane maria has already become a katrina\n",
            "True Output: 0\n",
            "Response:    dem rep soto hurricane maria has already become a katrina\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: california insurance commissioner says preliminary wildfire losses exceed 1 billion are expected to rise\n",
            "Input:       california insurance commissioner says preliminary wildfire losses exceed 1 billion are expected to rise\n",
            "True Output: 0\n",
            "Response:    california insurance commissioner says preliminary wildfire losses exceed 1 billion are expected to rise\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: a 2 risk of a tornado within 25 miles of a point today #inwx #indy #carmelin\n",
            "Input:       a 2 risk of a tornado within 25 miles of a point today #inwx #indy #carmelin\n",
            "True Output: 0\n",
            "Response:    a 2 risk of a tornado within 25 miles of a point today #inwx #indy #carmelin\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: antonio mora mind bending illusions\n",
            "Input:       antonio mora mind bending illusions\n",
            "True Output: 0\n",
            "Response:    antonio mora mind bending illusions\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: #harvey still causing damage and flooding\n",
            "Input:       #harvey still causing damage and flooding\n",
            "True Output: 0\n",
            "Response:    #harvey still causing damage and flooding\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: morgan stanley says pgampe shares may have overreacted to california fire risk pcg\n",
            "Input:       morgan stanley says pgampe shares may have overreacted to california fire risk pcg\n",
            "True Output: 0\n",
            "Response:    morgan stanley says pgampe shares may have overreacted to california fire risk pcg\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: california fire solution intervene with r larceny check on donald trump\n",
            "Input:       california fire solution intervene with r larceny check on donald trump\n",
            "True Output: 0\n",
            "Response:    california fire solution intervene with r larceny check on donald trump\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: crews battle brush fire near mount wilson observatory in california\n",
            "Input:       crews battle brush fire near mount wilson observatory in california\n",
            "True Output: 0\n",
            "Response:    crews battle brush fire near mount wilson observatory in california\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: clearing heads with a long walk after a sleepless election night greenest train tracks in use #mora #30dayswild\n",
            "Input:       clearing heads with a long walk after a sleepless election night greenest train tracks in use #mora #30dayswild\n",
            "True Output: 0\n",
            "Response:    clearing heads with a long walk after a sleepless election night greenest train tracks in use #mora #30dayswild\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: despair as death toll jumps after massive #iran quake\n",
            "Input:       despair as death toll jumps after massive #iran quake\n",
            "True Output: 0\n",
            "Response:    despair as death toll jumps after massive #iran quake\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: two weeks after #harvey miles upon miles of gutted homes so sad\n",
            "Input:       two weeks after #harvey miles upon miles of gutted homes so sad\n",
            "True Output: 0\n",
            "Response:    two weeks after #harvey miles upon miles of gutted homes so sad\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: irma has submerged the town of caibarien cuba under several feet of water\n",
            "Input:       irma has submerged the town of caibarien cuba under several feet of water\n",
            "True Output: 0\n",
            "Response:    irma has submerged the town of caibarien cuba under several feet of water\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: when i first saw this book i assumed jim mora wrote it #1987 #49erssaints\n",
            "Input:       when i first saw this book i assumed jim mora wrote it #1987 #49erssaints\n",
            "True Output: 0\n",
            "Response:    when i first saw this book i assumed jim mora wrote it #1987 #49erssaints\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: #maria now a category 2 hurricane movement nw ill go over the turn to the n on 5pm #savannah\n",
            "Input:       #maria now a category 2 hurricane movement nw ill go over the turn to the n on 5pm #savannah\n",
            "True Output: 0\n",
            "Response:    #maria now a category 2 hurricane movement nw ill go over the turn to the n on 5pm #savannah\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: i miss my roomies irma youve separated us all for too long\n",
            "Input:       i miss my roomies irma youve separated us all for too long\n",
            "True Output: 0\n",
            "Response:    i miss my roomies irma youve separated us all for too long\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: #usslakeerie continue to provide #srilanka humanitarian aid and assistance in wake of #floodsl #aroundthefleet\n",
            "Input:       #usslakeerie continue to provide #srilanka humanitarian aid and assistance in wake of #floodsl #aroundthefleet\n",
            "True Output: 0\n",
            "Response:    #usslakeerie continue to provide #srilanka humanitarian aid and assistance in wake of #floodsl #aroundthefleet\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: just in check what #law enforcement thinks caused the #california wildfires\n",
            "Input:       just in check what #law enforcement thinks caused the #california wildfires\n",
            "True Output: 0\n",
            "Response:    just in check what #law enforcement thinks caused the #california wildfires\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: some #hurricaneharvey rescues now fleeing #wildfires in #california\n",
            "Input:       some #hurricaneharvey rescues now fleeing #wildfires in #california\n",
            "True Output: 0\n",
            "Response:    some #hurricaneharvey rescues now fleeing #wildfires in #california\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: fire cleanup will be the largest in california history\n",
            "Input:       fire cleanup will be the largest in california history\n",
            "True Output: 0\n",
            "Response:    fire cleanup will be the largest in california history\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== 1 Tweet Group ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: property lost to california wildfires exceeds 1b\n",
            "Input:       property lost to california wildfires exceeds 1b\n",
            "True Output: 1\n",
            "Response:    property lost to california wildfires exceeds 1b\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: tryon equestrian center to help hundreds of horses escape hurricane irma #nc\n",
            "Input:       tryon equestrian center to help hundreds of horses escape hurricane irma #nc\n",
            "True Output: 1\n",
            "Response:    tryon equestrian center to help hundreds of horses escape hurricane irma #nc\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: is offering free housing to people displaced by the california wildfires\n",
            "Input:       is offering free housing to people displaced by the california wildfires\n",
            "True Output: 1\n",
            "Response:    is offering free housing to people displaced by the california wildfires\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: hospitals in #puertorico are in critical condition #hurricanemaria #publichealth\n",
            "Input:       hospitals in #puertorico are in critical condition #hurricanemaria #publichealth\n",
            "True Output: 1\n",
            "Response:    hospitals in #puertorico are in critical condition #hurricanemaria #publichealth\n",
            "----------------------------------------------------------------------\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "Input:       help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "True Output: 1\n",
            "Response:    help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Check if CUDA is available and get the device index\n",
        "if torch.cuda.is_available():\n",
        "    device = 0  # Assuming your primary GPU is index 0\n",
        "    print(f\"CUDA is available. Using GPU {device}\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"CUDA not available. Using CPU.\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map={'': device}\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Increased LoRA rank\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "b717c096d2764fe7bd9fcb3dfdde925b",
            "68eb60a75b324ef988745be44e9e64e1",
            "e0e5873a8f0940f7a82c74bf0e7deeb4",
            "b7543757a74c418c9474a325d4b9dac8",
            "ba0293db13f54ad08ba6c2b06b361c5a",
            "7b56c5452c8c4026aa3163ba8546c1f1",
            "5326f92decb24ce2b4832c240b7f7f18",
            "0ce81a4d00f9413389dcbc04e2693aa5",
            "b11e1c58790642169cc46d126ce45e33",
            "5a2b2edb75c84c6aac1cb546c1aeb42e",
            "5f43a36a64704156afda4a1c52b0fae1"
          ]
        },
        "id": "njre9UM-HbWp",
        "outputId": "2f5207eb-c15b-458b-8eb9-06517d1df692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b717c096d2764fe7bd9fcb3dfdde925b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Explicitly disable use_cache in the model config\n",
        "model.config.use_cache = False\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Remove padding tokens (-100 is often the ignore_index)\n",
        "    mask = labels != -100\n",
        "    predictions_masked = predictions[mask]\n",
        "    labels_masked = labels[mask]\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels_masked, predictions_masked),\n",
        "        'precision': precision_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "        'f1': f1_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "    }\n",
        "\n",
        "# Training arguments (gradient checkpointing disabled)\n",
        "output_dir = \"./llama-7b-wildfire-multi-task-lora-no-gc-v2\" # Changed output directory\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=10,  # Increased number of epochs\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs-lora-no-gc-v2\", # Changed logging directory\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        "    max_grad_norm=1.0,\n",
        "    optim=\"adamw_torch\",\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    gradient_checkpointing=False,  # Gradient checkpointing disabled\n",
        "    per_device_eval_batch_size=2,  # Reduced evaluation batch size\n",
        "    eval_steps=100,  # Evaluate every 100 training steps\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,  # Use the scikit-learn based compute_metrics\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model after training\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SXE0OMw6Opve",
        "outputId": "bde7915f-964e-49c3-cae2-eca5de7b9f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [620/620 23:36, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>14.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.813100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.004400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.003400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.003400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='179' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [179/179 00:32]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "{'eval_loss': 0.0027134146075695753, 'eval_accuracy': 0.9765625, 'eval_precision': 0.9688120039682541, 'eval_recall': 0.9765625, 'eval_f1': 0.9726718127490039, 'eval_runtime': 36.7358, 'eval_samples_per_second': 9.745, 'eval_steps_per_second': 4.873, 'epoch': 9.848}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Take a random sample of unique tweet IDs from the validation dataset\n",
        "sample_size = 5  # Let's look at a smaller number of tweets with all instructions\n",
        "unique_tweet_ids = val_df['tweet_id'].unique().tolist()\n",
        "random_tweet_ids = random.sample(unique_tweet_ids, min(sample_size, len(unique_tweet_ids)))\n",
        "sample_val_df = val_df[val_df['tweet_id'].isin(random_tweet_ids)]\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "grouped_results = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "for index, row in sample_val_df.iterrows():\n",
        "    tweet_id = row['tweet_id']\n",
        "    instruction = format_instruction(row)\n",
        "    input_text = row['tweet_text']\n",
        "    true_output = format_output(row)\n",
        "\n",
        "    if instruction:  # Only process rows where an instruction was generated\n",
        "        input_prompt = f\"{instruction}\\n{input_text}\"\n",
        "        input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(input_ids=input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "        predicted_output = tokenizer.decode(output[0], skip_special_tokens=True).split('\\n')[-1].strip()\n",
        "\n",
        "        grouped_results[tweet_id][instruction]['input'] = input_text\n",
        "        grouped_results[tweet_id][instruction]['true_output'] = true_output\n",
        "        grouped_results[tweet_id][instruction]['predicted_output'] = predicted_output\n",
        "\n",
        "model.train()  # Set the model back to training mode\n",
        "\n",
        "for tweet_id, instruction_results in grouped_results.items():\n",
        "    print(f\"============================== Tweet ID: {tweet_id} ==============================\")\n",
        "    for instruction, results in instruction_results.items():\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"Input:       {results['input']}\")\n",
        "        print(f\"Distress?:   {results['true_output']}\")\n",
        "        print(f\"Response:    {results['predicted_output']}\")\n",
        "        print(\"-\" * 70)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlYNOXohOp15",
        "outputId": "8339c7cd-8db3-4b16-d04d-7ca52f1075fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== Tweet ID: 9.13010357786021e+17 ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: hospitals in #puertorico are in critical condition #hurricanemaria #publichealth\n",
            "Input:       hospitals in #puertorico are in critical condition #hurricanemaria #publichealth\n",
            "Distress?:   1\n",
            "Response:    hospitals in #puertorico are in critical condition #hurricanemaria #publichealth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.0537578323295e+17 ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: as a class apostolic project pk3 pk4 collected items for hurricane harvey evacuees in irving shelters\n",
            "Input:       as a class apostolic project pk3 pk4 collected items for hurricane harvey evacuees in irving shelters\n",
            "Distress?:   0\n",
            "Response:    as a class apostolic project pk3 pk4 collected items for hurricane harvey evacuees in irving shelters\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.09394006974459e+17 ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: irma hits georgia pecan and cotton crops hard may boost peanuts\n",
            "Input:       irma hits georgia pecan and cotton crops hard may boost peanuts\n",
            "Distress?:   0\n",
            "Response:    irma hits georgia pecan and cotton crops hard may boost peanuts\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.10151285793488e+17 ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: i found this lil baby on the sidewalk after hurricane irma a i named her irma\n",
            "Input:       i found this lil baby on the sidewalk after hurricane irma a i named her irma\n",
            "Distress?:   0\n",
            "Response:    i found this lil baby on the sidewalk after hurricane irma a i named her irma\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.17836981291413e+17 ==============================\n",
            "Instruction: You are an emergency detection system. Determine if the tweet indicates distress. Input: wildfires sweep across northern california 15 are dead by thomas fuller jonah engel\n",
            "Input:       wildfires sweep across northern california 15 are dead by thomas fuller jonah engel\n",
            "Distress?:   0\n",
            "Response:    wildfires sweep across northern california 15 are dead by thomas fuller jonah engel\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Explicitly disable use_cache in the model config\n",
        "model.config.use_cache = False\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Remove padding tokens (-100 is often the ignore_index)\n",
        "    mask = labels != -100\n",
        "    predictions_masked = predictions[mask]\n",
        "    labels_masked = labels[mask]\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels_masked, predictions_masked),\n",
        "        'precision': precision_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "        'recall': recall_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "        'f1': f1_score(labels_masked, predictions_masked, average='weighted', zero_division=0),\n",
        "    }\n",
        "\n",
        "# Training arguments (gradient checkpointing disabled)\n",
        "output_dir = \"./llama-7b-wildfire-multi-task-lora-no-gc-v2\" # Changed output directory\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=5,  # Increased number of epochs\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs-lora-no-gc-v2\", # Changed logging directory\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        "    max_grad_norm=1.0,\n",
        "    optim=\"adamw_torch\",\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    gradient_checkpointing=False,  # Gradient checkpointing disabled\n",
        "    per_device_eval_batch_size=2,  # Reduced evaluation batch size\n",
        "    eval_steps=100,  # Evaluate every 100 training steps\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,  # Use the scikit-learn based compute_metrics\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model after training\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CuTIXsnGWliD",
        "outputId": "1c3ea3a2-f55f-4d2a-cb6f-b52ff17efc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [310/310 11:41, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.031100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.006200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.004400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.003400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.005300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.004700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.006200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.005700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.004700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='179' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [179/179 00:31]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "{'eval_loss': 0.005448967218399048, 'eval_accuracy': 0.9774135824022346, 'eval_precision': 0.9705013953669789, 'eval_recall': 0.9774135824022346, 'eval_f1': 0.97394522491771, 'eval_runtime': 35.7564, 'eval_samples_per_second': 10.012, 'eval_steps_per_second': 5.006, 'epoch': 4.928}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Take a random sample of unique tweet IDs from the validation dataset\n",
        "sample_size = 5  # Let's look at a smaller number of tweets with all instructions\n",
        "unique_tweet_ids = val_df['tweet_id'].unique().tolist()\n",
        "random_tweet_ids = random.sample(unique_tweet_ids, min(sample_size, len(unique_tweet_ids)))\n",
        "sample_val_df = val_df[val_df['tweet_id'].isin(random_tweet_ids)]\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "grouped_results = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "for index, row in sample_val_df.iterrows():\n",
        "    tweet_id = row['tweet_id']\n",
        "    instruction = format_instruction(row)\n",
        "    input_text = row['tweet_text']\n",
        "    true_output = format_output(row)\n",
        "\n",
        "    if instruction:  # Only process rows where an instruction was generated\n",
        "        input_prompt = f\"{instruction}\\n{input_text}\"\n",
        "        input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(input_ids=input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "        predicted_output = tokenizer.decode(output[0], skip_special_tokens=True).split('\\n')[-1].strip()\n",
        "\n",
        "        grouped_results[tweet_id][instruction]['input'] = input_text\n",
        "        grouped_results[tweet_id][instruction]['true_output'] = true_output\n",
        "        grouped_results[tweet_id][instruction]['predicted_output'] = predicted_output\n",
        "\n",
        "model.train()  # Set the model back to training mode\n",
        "\n",
        "for tweet_id, instruction_results in grouped_results.items():\n",
        "    print(f\"============================== Tweet ID: {tweet_id} ==============================\")\n",
        "    for instruction, results in instruction_results.items():\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"Input:       {results['input']}\")\n",
        "        if \"emergency response system\" in instruction.lower():\n",
        "            print(f\"True Action: {results['true_output']}\")\n",
        "            print(f\"Predicted:   {results['predicted_output']}\")\n",
        "        else:\n",
        "            print(f\"True Output: {results['true_output']}\")\n",
        "            print(f\"Response:    {results['predicted_output']}\")\n",
        "        print(\"-\" * 70)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1dU81wCWlnB",
        "outputId": "43a64792-889f-464f-b161-3ce891f45fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== Tweet ID: 8.70190470466596e+17 ==============================\n",
            "Instruction: You are a disaster response coordinator. Based on the content of the tweet, if it indicates distress, recommend the most urgent emergency action from the following: evacuation, medical aid, fire suppression, rescue, resource delivery, monitor only. Input: the #flood extent in kalutara district #srilanka was captured by #terrasarx on 30 may\n",
            "Input:       the #flood extent in kalutara district #srilanka was captured by #terrasarx on 30 may\n",
            "True Output: monitor only\n",
            "Response:    the #flood extent in kalutara district #srilanka was captured by #terrasarx on 30 may\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.04453241005641e+17 ==============================\n",
            "Instruction: You are a disaster response coordinator. Based on the content of the tweet, if it indicates distress, recommend the most urgent emergency action from the following: evacuation, medical aid, fire suppression, rescue, resource delivery, monitor only. Input: hurricane harveys impact on #houston commercial real estate #cre\n",
            "Input:       hurricane harveys impact on #houston commercial real estate #cre\n",
            "True Output: monitor only\n",
            "Response:    hurricane harveys impact on #houston commercial real estate #cre\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.04705134239903e+17 ==============================\n",
            "Instruction: You are a disaster response coordinator. Based on the content of the tweet, if it indicates distress, recommend the most urgent emergency action from the following: evacuation, medical aid, fire suppression, rescue, resource delivery, monitor only. Input: a 2 risk of a tornado within 25 miles of a point today #inwx #indy #carmelin\n",
            "Input:       a 2 risk of a tornado within 25 miles of a point today #inwx #indy #carmelin\n",
            "True Output: monitor only\n",
            "Response:    a 2 risk of a tornado within 25 miles of a point today #inwx #indy #carmelin\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.070867855992128e+17 ==============================\n",
            "Instruction: You are a disaster response coordinator. Based on the content of the tweet, if it indicates distress, recommend the most urgent emergency action from the following: evacuation, medical aid, fire suppression, rescue, resource delivery, monitor only. Input: out of office until tuesday responses may be delayed also i found my old kite at mamaws house #hurricaneharvey\n",
            "Input:       out of office until tuesday responses may be delayed also i found my old kite at mamaws house #hurricaneharvey\n",
            "True Output: monitor only\n",
            "Response:    out of office until tuesday responses may be delayed also i found my old kite at mamaws house #hurricaneharvey\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.1997925444782e+17 ==============================\n",
            "Instruction: You are a disaster response coordinator. Based on the content of the tweet, if it indicates distress, recommend the most urgent emergency action from the following: evacuation, medical aid, fire suppression, rescue, resource delivery, monitor only. Input: morgan stanley says pgampe shares may have overreacted to california fire risk pcg\n",
            "Input:       morgan stanley says pgampe shares may have overreacted to california fire risk pcg\n",
            "True Output: monitor only\n",
            "Response:    morgan stanley says pgampe shares may have overreacted to california fire risk pcg\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Check if CUDA is available and get the device index\n",
        "if torch.cuda.is_available():\n",
        "    device = 0  # Assuming your primary GPU is index 0\n",
        "    print(f\"CUDA is available. Using GPU {device}\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"CUDA not available. Using CPU.\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map={'': device}\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Increased LoRA rank\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "b60d3563f3484feea283704067a7f20c",
            "b3cf8c5e115c462fa54f86fae7fb78eb",
            "8285f0f3cf4e48e1a1892b9ba4d878ea",
            "1384bdd53d4141d2a9e3a92da325e2c8",
            "3271ebfdb41d455a8f6cc8cebee6650d",
            "abc0d127e4c7445c9eaa8091d51aaab8",
            "963d3ad6af01422cbfd393bcdf5d62d1",
            "a5ffa43ba15242b689cb8a8e12c84137",
            "a7f97f958f2d4054b2bc9721381f02d0",
            "eed5404f9a14483fa26117e75dfbfd73",
            "68833a5e06f44a8f850f2bf8d4ff76bc"
          ]
        },
        "id": "xXfMo93Eb0Vb",
        "outputId": "4fd02328-0379-4d36-8739-86f263d0c3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b60d3563f3484feea283704067a7f20c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load a balanced sample of the datasets for testing\n",
        "def balanced_sample(df, n=500, label_column=None):\n",
        "    if 'Wildfire' in df.columns:\n",
        "        label_column = 'Wildfire'\n",
        "    elif 'distress' in df.columns:\n",
        "        label_column = 'distress'\n",
        "    elif 'state' in df.columns:\n",
        "        label_column = 'state'\n",
        "    elif 'take_action' in df.columns:\n",
        "        label_column = 'take_action'\n",
        "    else:\n",
        "        return df.head(min(n, len(df)))  # If no suitable label column, take the head\n",
        "\n",
        "    if label_column:\n",
        "        grouped = df.groupby(label_column)\n",
        "        sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
        "        sampled_df = sampled_df.reset_index(drop=True)\n",
        "        return sampled_df\n",
        "    else:\n",
        "        return df.head(min(n, len(df)))\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/llama_train_balanced.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/llama_val_balanced.csv')\n",
        "\n",
        "train_df = balanced_sample(train_df, n=500)\n",
        "val_df = balanced_sample(val_df, n=500)\n",
        "\n",
        "action_labels_9 = ['evacuate', 'shelter', 'rescue', 'medical', 'supply', 'information', 'infrastructure', 'security', 'other'] # Define your 9 action labels\n",
        "\n",
        "def format_instruction(row):\n",
        "    if 'Wildfire' in row and row['Wildfire'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return f\"You are a wildfire classifier. Determine if the tweet is about a wildfire. Input: {row['tweet_text']}\"\n",
        "    elif 'distress' in row and row['distress'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return f\"Emergency situation detected? If so, what is the most urgent action needed from: {', '.join(action_labels_9)}. Input: {row['tweet_text']}\"\n",
        "    elif 'state' in row and isinstance(row['state'], str) and row['state'].lower() != 'none':\n",
        "        return f\"Identify the U.S. state mentioned in the tweet. Input: {row['tweet_text']}\"\n",
        "    elif 'take_action' in row and row['take_action'] in action_labels_9:\n",
        "        return f\"What is the most urgent action needed from: {', '.join(action_labels_9)}. Input: {row['tweet_text']}\"\n",
        "    return \"\"\n",
        "\n",
        "def format_output(row):\n",
        "    if 'Wildfire' in row and row['Wildfire'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        return str(row['Wildfire'])\n",
        "    elif 'distress' in row and row['distress'] in [0, 1, '0', '1', 'yes', 'no', True, False]:\n",
        "        # If distress, directly try to return the 'take_action' label\n",
        "        if str(row['distress']) in ['1', 'yes', 'True']:\n",
        "            if 'take_action' in row and row['take_action'] in action_labels_9:\n",
        "                return row['take_action']\n",
        "            else:\n",
        "                return 'monitor only' # Or a more appropriate default if no action label found\n",
        "        else:\n",
        "            return 'monitor only'\n",
        "    elif 'state' in row and isinstance(row['state'], str) and row['state'].lower() != 'none':\n",
        "        return row['state']\n",
        "    elif 'take_action' in row and row['take_action'] in action_labels_9:\n",
        "        return row['take_action']\n",
        "    return \"\"\n",
        "\n",
        "train_df['instruction'] = train_df.apply(format_instruction, axis=1)\n",
        "train_df['output'] = train_df.apply(format_output, axis=1)\n",
        "train_df['input'] = train_df['tweet_text']\n",
        "train_df_filtered = train_df[train_df['instruction'] != \"\"]\n",
        "train_dataset = Dataset.from_pandas(train_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "val_df['instruction'] = val_df.apply(format_instruction, axis=1)\n",
        "val_df['output'] = val_df.apply(format_output, axis=1)\n",
        "val_df['input'] = val_df['tweet_text']\n",
        "val_df_filtered = val_df[val_df['instruction'] != \"\"]\n",
        "val_dataset = Dataset.from_pandas(val_df_filtered[['instruction', 'input', 'output']])\n",
        "\n",
        "max_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"{instruction}\\n{input_text}\" for instruction, input_text in zip(examples['instruction'], examples['input'])]\n",
        "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples['output']]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
        "    return {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"], \"labels\": labels[\"input_ids\"]}\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172,
          "referenced_widgets": [
            "92db660e55c94e2b842d89d0eb59cf7f",
            "80d53f8137434acba6f61bf6ed400d6a",
            "ff86fcdc8f5f48be8a97c2cbf7e36869",
            "cd03e14434f04d3bbad60f278058ad05",
            "4917e72f0c9d4dffa13fbf38fe80183d",
            "372b2cc079494a5893f4c4045c507705",
            "5b2856b8a7d94e4da7c253b32a6aa731",
            "a714bcffb525422ab6c35a45121f653c",
            "6944e19f24fa4443a6b3551f9f65203f",
            "792387d9fb3d4279b19991f9f7ded3b2",
            "47654b19fa684161a57f65d274cd041a",
            "fc1f6bbc05164ac4865d3a3453b50792",
            "c358ce9bfd3540ed88d92760cc465aa5",
            "4ea87abcf3a54cf785240fc408e559d6",
            "8482ce3578a343878ca4f356a92ce055",
            "8551dc2e992e417fbb6446c051350db8",
            "b0f5c88d619742ba89868aabd0f5216c",
            "cdcd6061cd1d49288fc65010a1e4ba77",
            "f6d137e90386427c97dad82d91c4a84e",
            "1a3f2b09625b45a4babed4f8200082ab",
            "41325219baa5470dab2210932e67802f",
            "b61565af409b400baed61e7a127fab8a"
          ]
        },
        "id": "iohDWqzzb0SO",
        "outputId": "416ea7f0-eaf1-45b0-fd3a-b313a6854e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b40a67250922>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n",
            "<ipython-input-2-b40a67250922>:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_df = grouped.apply(lambda x: x.sample(min(len(x), n // grouped.ngroups), random_state=42))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92db660e55c94e2b842d89d0eb59cf7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc1f6bbc05164ac4865d3a3453b50792"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Take a random sample of unique tweet IDs from the validation dataset\n",
        "sample_size = 5  # Let's look at a smaller number of tweets with all instructions\n",
        "unique_tweet_ids = val_df['tweet_id'].unique().tolist()\n",
        "random_tweet_ids = random.sample(unique_tweet_ids, min(sample_size, len(unique_tweet_ids)))\n",
        "sample_val_df = val_df[val_df['tweet_id'].isin(random_tweet_ids)]\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "grouped_results = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "for index, row in sample_val_df.iterrows():\n",
        "    tweet_id = row['tweet_id']\n",
        "    instruction = format_instruction(row)\n",
        "    input_text = row['tweet_text']\n",
        "    true_output = format_output(row)\n",
        "\n",
        "    if instruction:  # Only process rows where an instruction was generated\n",
        "        input_prompt = f\"{instruction}\\n{input_text}\"\n",
        "        input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(input_ids=input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "        predicted_output = tokenizer.decode(output[0], skip_special_tokens=True).split('\\n')[-1].strip()\n",
        "\n",
        "        grouped_results[tweet_id][instruction]['input'] = input_text\n",
        "        grouped_results[tweet_id][instruction]['true_output'] = true_output\n",
        "        grouped_results[tweet_id][instruction]['predicted_output'] = predicted_output\n",
        "\n",
        "model.train()  # Set the model back to training mode\n",
        "\n",
        "for tweet_id, instruction_results in grouped_results.items():\n",
        "    print(f\"============================== Tweet ID: {tweet_id} ==============================\")\n",
        "    for instruction, results in instruction_results.items():\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"Input:       {results['input']}\")\n",
        "        if \"emergency response system\" in instruction.lower():\n",
        "            print(f\"True Action: {results['true_output']}\")\n",
        "            print(f\"Predicted:   {results['predicted_output']}\")\n",
        "        else:\n",
        "            print(f\"True Output: {results['true_output']}\")\n",
        "            print(f\"Response:    {results['predicted_output']}\")\n",
        "        print(\"-\" * 70)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUH-0-2ub0Lp",
        "outputId": "42b56822-98b8-4c52-bd81-91ee570cf6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== Tweet ID: 9.13103799367065e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: hurricane maria coordination call september 28 400 pm et\n",
            "Input:       hurricane maria coordination call september 28 400 pm et\n",
            "True Output: monitor only\n",
            "Response:    hurricane maria coordination call september 28 400 pm et\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.11710649666981e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: humidity will increase sunday tracking when cooler air returns amp hurricane maria right now on channel 11 news #wpxi\n",
            "Input:       humidity will increase sunday tracking when cooler air returns amp hurricane maria right now on channel 11 news #wpxi\n",
            "True Output: monitor only\n",
            "Response:    humidity will increase sunday tracking when cooler air returns amp hurricane maria right now on channel 11 news #wpxi\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.09832055512428e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: jacksonvilles tourism industry welcomes visitors following hurricane irma\n",
            "Input:       jacksonvilles tourism industry welcomes visitors following hurricane irma\n",
            "True Output: monitor only\n",
            "Response:    jacksonvilles tourism industry welcomes visitors following hurricane irma\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.17819230732603e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: wine country fire in napa valley california\n",
            "Input:       wine country fire in napa valley california\n",
            "True Output: monitor only\n",
            "Response:    wine country fire in napa valley california\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.20357770544668e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: pickup truck full of marijuana crashes outside northern california fire station\n",
            "Input:       pickup truck full of marijuana crashes outside northern california fire station\n",
            "True Output: monitor only\n",
            "Response:    pickup truck full of marijuana crashes outside northern california fire station\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Take a random sample of unique tweet IDs from the validation dataset\n",
        "sample_size = 5\n",
        "unique_tweet_ids = val_df['tweet_id'].unique().tolist()\n",
        "random_tweet_ids = random.sample(unique_tweet_ids, min(sample_size, len(unique_tweet_ids)))\n",
        "sample_val_df = val_df[val_df['tweet_id'].isin(random_tweet_ids)]\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "grouped_results = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "print(\"--- DEBUGGING INSTRUCTIONS AND TRUE OUTPUTS FOR EMERGENCY RESPONSE ---\")\n",
        "for index, row in sample_val_df.iterrows():\n",
        "    if 'distress' in row and str(row['distress']) in ['1', 'yes', 'True']:\n",
        "        instruction = format_instruction(row)\n",
        "        true_output = format_output(row)\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"True Action: {true_output}\")\n",
        "        print(f\"Tweet: {row['tweet_text']}\")\n",
        "        print(\"-\" * 50)\n",
        "print(\"--- END DEBUGGING ---\")\n",
        "\n",
        "for index, row in sample_val_df.iterrows():\n",
        "    tweet_id = row['tweet_id']\n",
        "    instruction = format_instruction(row)\n",
        "    input_text = row['tweet_text']\n",
        "    true_output = format_output(row)\n",
        "\n",
        "    if instruction:  # Only process rows where an instruction was generated\n",
        "        input_prompt = f\"{instruction}\\n{input_text}\"\n",
        "        input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(input_ids=input_ids, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "\n",
        "        predicted_output = tokenizer.decode(output[0], skip_special_tokens=True).split('\\n')[-1].strip()\n",
        "\n",
        "        grouped_results[tweet_id][instruction]['input'] = input_text\n",
        "        grouped_results[tweet_id][instruction]['true_output'] = true_output\n",
        "        grouped_results[tweet_id][instruction]['predicted_output'] = predicted_output\n",
        "\n",
        "model.train()  # Set the model back to training mode\n",
        "\n",
        "for tweet_id, instruction_results in grouped_results.items():\n",
        "    print(f\"============================== Tweet ID: {tweet_id} ==============================\")\n",
        "    for instruction, results in instruction_results.items():\n",
        "        print(f\"Instruction: {instruction}\")\n",
        "        print(f\"Input:       {results['input']}\")\n",
        "        if \"emergency situation detected\" in instruction.lower():\n",
        "            print(f\"True Action: {results['true_output']}\")\n",
        "            print(f\"Predicted:   {results['predicted_output']}\")\n",
        "        else:\n",
        "            print(f\"True Output: {results['true_output']}\")\n",
        "            print(f\"Response:    {results['predicted_output']}\")\n",
        "        print(\"-\" * 70)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhS5Wr-8bz9M",
        "outputId": "27ffc4df-8e34-42df-a72d-c9c075d6468e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DEBUGGING INSTRUCTIONS AND TRUE OUTPUTS FOR EMERGENCY RESPONSE ---\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "True Action: monitor only\n",
            "Tweet: help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "--------------------------------------------------\n",
            "--- END DEBUGGING ---\n",
            "============================== Tweet ID: 9.22465813650706e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: how will puerto rico devastated and drowning in debt pay torebuild\n",
            "Input:       how will puerto rico devastated and drowning in debt pay torebuild\n",
            "True Action: monitor only\n",
            "Predicted:   how will puerto rico devastated and drowning in debt pay torebuild\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.10232170454585e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "Input:       help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "True Action: monitor only\n",
            "Predicted:   help #houstonstrong #redcross #irma #giveblood now and ongoing more #lifesaving stories at\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.062578708571008e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: yo harvey imma let you finish but\n",
            "Input:       yo harvey imma let you finish but\n",
            "True Action: monitor only\n",
            "Predicted:   yo harvey imma let you finish but\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.16351942212939e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: it may be dark in puerto rico but electric customers are still getting bills\n",
            "Input:       it may be dark in puerto rico but electric customers are still getting bills\n",
            "True Action: monitor only\n",
            "Predicted:   it may be dark in puerto rico but electric customers are still getting bills\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "============================== Tweet ID: 9.205019778466072e+17 ==============================\n",
            "Instruction: Emergency situation detected? If so, what is the most urgent action needed from: evacuate, shelter, rescue, medical, supply, information, infrastructure, security, other. Input: pgampe should be broken up if responsible for california wildfires lawmake\n",
            "Input:       pgampe should be broken up if responsible for california wildfires lawmake\n",
            "True Action: monitor only\n",
            "Predicted:   pgampe should be broken up if responsible for california wildfires lawmake\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}