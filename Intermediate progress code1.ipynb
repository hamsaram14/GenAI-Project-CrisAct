{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd28c1cd608b44558cc9203464198d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a01df0c54b854e2dbf770f480ae01d4f",
              "IPY_MODEL_9d6c11e19d0a4612b6a8b76b73a9df28",
              "IPY_MODEL_633f429e39474bfbb07748c97fce8766"
            ],
            "layout": "IPY_MODEL_67fe2ed54387456b98eca728f9ac1a08"
          }
        },
        "a01df0c54b854e2dbf770f480ae01d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_822d5e24c77446e08799a0815e6b6116",
            "placeholder": "​",
            "style": "IPY_MODEL_d65aff30c2cc45d3a6c78545865b6161",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "9d6c11e19d0a4612b6a8b76b73a9df28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9767d247305447feaa674ede6d553788",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d0bf68f7f4e46c08c05e0185c7c0a21",
            "value": 48
          }
        },
        "633f429e39474bfbb07748c97fce8766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc25b00372744bdeb5a1d435533b8a71",
            "placeholder": "​",
            "style": "IPY_MODEL_721d6315de994cd689e2a54d6f7c42d1",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.56kB/s]"
          }
        },
        "67fe2ed54387456b98eca728f9ac1a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822d5e24c77446e08799a0815e6b6116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d65aff30c2cc45d3a6c78545865b6161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9767d247305447feaa674ede6d553788": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0bf68f7f4e46c08c05e0185c7c0a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc25b00372744bdeb5a1d435533b8a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721d6315de994cd689e2a54d6f7c42d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8445cd75e0b9455586a05aa2bedf336c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0667b7793c12402982331faa4bcd727f",
              "IPY_MODEL_3c0e6c9d603f46fcb14599823a3412a4",
              "IPY_MODEL_43691f611452486d9f4a032740028b12"
            ],
            "layout": "IPY_MODEL_dbea187355f34ab590a0d1d9140b110b"
          }
        },
        "0667b7793c12402982331faa4bcd727f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f126d99d5c714e618ac28e3f90756f8c",
            "placeholder": "​",
            "style": "IPY_MODEL_a5580d65caf44a5e898534355a64ab00",
            "value": "config.json: 100%"
          }
        },
        "3c0e6c9d603f46fcb14599823a3412a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0bb790646cc459d8d864f1e9af40acb",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58a49542022140009c52df8541b6d05b",
            "value": 483
          }
        },
        "43691f611452486d9f4a032740028b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e956c5bb86c4d1184545ce7251edc64",
            "placeholder": "​",
            "style": "IPY_MODEL_72651a132015464591c5980984cedf99",
            "value": " 483/483 [00:00&lt;00:00, 36.3kB/s]"
          }
        },
        "dbea187355f34ab590a0d1d9140b110b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f126d99d5c714e618ac28e3f90756f8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5580d65caf44a5e898534355a64ab00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0bb790646cc459d8d864f1e9af40acb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a49542022140009c52df8541b6d05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e956c5bb86c4d1184545ce7251edc64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72651a132015464591c5980984cedf99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63f489b1b50f43b7a7b0cd2a0f1de046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0982e594c6484b1eb31d4b4668ba0702",
              "IPY_MODEL_4fe32c2a42734a22b8348361d796c802",
              "IPY_MODEL_887aeb04e4c14737838c6a003564664b"
            ],
            "layout": "IPY_MODEL_ba716695f6184a5d8c706d1997a894f3"
          }
        },
        "0982e594c6484b1eb31d4b4668ba0702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2df40a9845d845a998ec8a0045670612",
            "placeholder": "​",
            "style": "IPY_MODEL_5f6f4c4dd8374429ae7f847f55546b66",
            "value": "vocab.txt: 100%"
          }
        },
        "4fe32c2a42734a22b8348361d796c802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2360c774e74141cd9ed67f3625713b55",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ae0a24840814fa8a79c80b3b3ff8b10",
            "value": 231508
          }
        },
        "887aeb04e4c14737838c6a003564664b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e9961685a04465ab44bf3822b822f28",
            "placeholder": "​",
            "style": "IPY_MODEL_35025f8d85994e30a3e05c131e296807",
            "value": " 232k/232k [00:00&lt;00:00, 11.3MB/s]"
          }
        },
        "ba716695f6184a5d8c706d1997a894f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df40a9845d845a998ec8a0045670612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6f4c4dd8374429ae7f847f55546b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2360c774e74141cd9ed67f3625713b55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ae0a24840814fa8a79c80b3b3ff8b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e9961685a04465ab44bf3822b822f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35025f8d85994e30a3e05c131e296807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9e10be8f5ab47879ee35d0171e40097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f7f1c0df08343cfaf1fb6cdf305b416",
              "IPY_MODEL_8f4d664d5b9e4dac845d6a43e3745d56",
              "IPY_MODEL_b193797e6b1848c1929e3a321a5bae83"
            ],
            "layout": "IPY_MODEL_c9aa7449bd5b455ea9d5f841daa965a3"
          }
        },
        "2f7f1c0df08343cfaf1fb6cdf305b416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4af93b6ab2ad4a7991e222a9d32d09e6",
            "placeholder": "​",
            "style": "IPY_MODEL_1d257d54902a45f58e5390266305d819",
            "value": "tokenizer.json: 100%"
          }
        },
        "8f4d664d5b9e4dac845d6a43e3745d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352bf7a2eabd420d93f9670a9ce5c8de",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71985b239d404299a7910569ed6c3be9",
            "value": 466062
          }
        },
        "b193797e6b1848c1929e3a321a5bae83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7e9b44aff1463e9814b54aecca02ed",
            "placeholder": "​",
            "style": "IPY_MODEL_ba44c41bdd0b45adad779a46965b484d",
            "value": " 466k/466k [00:00&lt;00:00, 18.2MB/s]"
          }
        },
        "c9aa7449bd5b455ea9d5f841daa965a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af93b6ab2ad4a7991e222a9d32d09e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d257d54902a45f58e5390266305d819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352bf7a2eabd420d93f9670a9ce5c8de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71985b239d404299a7910569ed6c3be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e7e9b44aff1463e9814b54aecca02ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba44c41bdd0b45adad779a46965b484d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccec796e6a5e46ee99693549fdb07ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c0cf0f5a28d46159d7718bc85c0f1a4",
              "IPY_MODEL_0837992c4cff45a19d63143619dfd55e",
              "IPY_MODEL_93ddb8f54372468584b178e6059da89f"
            ],
            "layout": "IPY_MODEL_ab5d7a325e8f4045b70177ae7c2204be"
          }
        },
        "7c0cf0f5a28d46159d7718bc85c0f1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea51187badc4ee3beb2850882669d2b",
            "placeholder": "​",
            "style": "IPY_MODEL_782940afb8fa434f86f9284a57b2b399",
            "value": "model.safetensors: 100%"
          }
        },
        "0837992c4cff45a19d63143619dfd55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b151ddf17b47b8a5aa748ad6340e5f",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8866a1b2ee1a4f0e98172b784d540bc2",
            "value": 267954768
          }
        },
        "93ddb8f54372468584b178e6059da89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d92b0a5b09254810ad2b898b449e84f0",
            "placeholder": "​",
            "style": "IPY_MODEL_6c6444ecb9b94afc87d55238e0b5085f",
            "value": " 268M/268M [00:01&lt;00:00, 252MB/s]"
          }
        },
        "ab5d7a325e8f4045b70177ae7c2204be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea51187badc4ee3beb2850882669d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "782940afb8fa434f86f9284a57b2b399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4b151ddf17b47b8a5aa748ad6340e5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8866a1b2ee1a4f0e98172b784d540bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d92b0a5b09254810ad2b898b449e84f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c6444ecb9b94afc87d55238e0b5085f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8183b7336dc0451e805cb2a709c256ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9682f9ff88084c0f89da96f1511a3ea6",
              "IPY_MODEL_1f77055a4392443b87e53f5f4d4b4da5",
              "IPY_MODEL_00242aebc4744240a5b84a18396a65c4"
            ],
            "layout": "IPY_MODEL_a76011533c4849c6a3cfde0219478bcd"
          }
        },
        "9682f9ff88084c0f89da96f1511a3ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075cd21afabd4557a15a07568ea3a214",
            "placeholder": "​",
            "style": "IPY_MODEL_80e3e98ba1524a99a85cfa88b56a01f2",
            "value": "Map: 100%"
          }
        },
        "1f77055a4392443b87e53f5f4d4b4da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7d839a75fbb4e7a8bbacd2474f7c802",
            "max": 14465,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d969b46882894077bc1fb31492477cde",
            "value": 14465
          }
        },
        "00242aebc4744240a5b84a18396a65c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95720dccb36242a98433a21946bb491e",
            "placeholder": "​",
            "style": "IPY_MODEL_2d1e0e31c3e44bfdbe3c33154fcd7a93",
            "value": " 14465/14465 [00:01&lt;00:00, 8317.30 examples/s]"
          }
        },
        "a76011533c4849c6a3cfde0219478bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075cd21afabd4557a15a07568ea3a214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e3e98ba1524a99a85cfa88b56a01f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7d839a75fbb4e7a8bbacd2474f7c802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d969b46882894077bc1fb31492477cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95720dccb36242a98433a21946bb491e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d1e0e31c3e44bfdbe3c33154fcd7a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5c7296dbb3a48c7a011ce0197bf8153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad1876986f284f779475faf5378c83da",
              "IPY_MODEL_3ee29a7734dc4ec5a1ffc9b998a1868d",
              "IPY_MODEL_905cadc6c4e045fba6e821b70ea38254"
            ],
            "layout": "IPY_MODEL_4a7a0116fc3c409884d2849d0227f7b9"
          }
        },
        "ad1876986f284f779475faf5378c83da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_262d571b7e764fafadcb70d5cbe338f5",
            "placeholder": "​",
            "style": "IPY_MODEL_d7710364e5f14752b3949df5b681eab6",
            "value": "Map: 100%"
          }
        },
        "3ee29a7734dc4ec5a1ffc9b998a1868d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96596594286142c7891d3222257a70ba",
            "max": 3617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f1f48dca3c44e70be1ba574d0fc7796",
            "value": 3617
          }
        },
        "905cadc6c4e045fba6e821b70ea38254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829bb76189ea4130a5cdf4371872c061",
            "placeholder": "​",
            "style": "IPY_MODEL_f943bd356b874b70a1ac840a1ec9e5d0",
            "value": " 3617/3617 [00:00&lt;00:00, 8263.08 examples/s]"
          }
        },
        "4a7a0116fc3c409884d2849d0227f7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262d571b7e764fafadcb70d5cbe338f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7710364e5f14752b3949df5b681eab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96596594286142c7891d3222257a70ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f1f48dca3c44e70be1ba574d0fc7796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "829bb76189ea4130a5cdf4371872c061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f943bd356b874b70a1ac840a1ec9e5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bcaa0502103493a9b2fc95288d1df91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ad6ea98694f4470c8af67a4b58fc4a68"
          }
        },
        "ac84cf9ffeca4802989abb7f00ba6896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf53d80cdba14c3da9d8eb338114c2b3",
            "placeholder": "​",
            "style": "IPY_MODEL_9d1d764b462d4f39bd47b62b65549809",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "23eea19bda7543d2971ea3ffe782c812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3ab2c537be894780842c108282845b0e",
            "placeholder": "​",
            "style": "IPY_MODEL_f57e6932ee3f43baa7e178b139f42245",
            "value": ""
          }
        },
        "f445090b0f2041938e871325cf850734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_edd1ad2eb29542aabcdb9ef0604d2f4f",
            "style": "IPY_MODEL_c78b969277234a909d50d9f2ed341c97",
            "value": true
          }
        },
        "1c38f08803734a5b80c2544dd7b3acb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_29972545987b49799b7512c83356b19c",
            "style": "IPY_MODEL_12abdd5734e04fe2b33874f5bd84d874",
            "tooltip": ""
          }
        },
        "8e87965262954c7db71b101e12c8548c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adbf3e6ef84e47c89360af489cccf4e8",
            "placeholder": "​",
            "style": "IPY_MODEL_e28251beee814214bf60482fab6fb411",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ad6ea98694f4470c8af67a4b58fc4a68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "bf53d80cdba14c3da9d8eb338114c2b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d1d764b462d4f39bd47b62b65549809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ab2c537be894780842c108282845b0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57e6932ee3f43baa7e178b139f42245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edd1ad2eb29542aabcdb9ef0604d2f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c78b969277234a909d50d9f2ed341c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29972545987b49799b7512c83356b19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12abdd5734e04fe2b33874f5bd84d874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "adbf3e6ef84e47c89360af489cccf4e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28251beee814214bf60482fab6fb411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12dbd20052e4b9290216beeebce642c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de1e49d8f124526803433960be21f6d",
            "placeholder": "​",
            "style": "IPY_MODEL_975106362b6743b88df99a83bb101c0e",
            "value": "Connecting..."
          }
        },
        "8de1e49d8f124526803433960be21f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975106362b6743b88df99a83bb101c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cfe8922f4904ab5b257fc139f5bb7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_742f2f13f7c74f06b503ae4ef3f82f5a",
              "IPY_MODEL_090af11f8dea4200b3b56e882886f81b",
              "IPY_MODEL_a85874e66f3341a3a04cb03630dbe264"
            ],
            "layout": "IPY_MODEL_b69ebd9e39c74b09be9e16dfc737c9e5"
          }
        },
        "742f2f13f7c74f06b503ae4ef3f82f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d99f03586f4350918a4734eddb6962",
            "placeholder": "​",
            "style": "IPY_MODEL_6aa5cbc1db214521a746bdcf8782e052",
            "value": "Loading checkpoint shards:  25%"
          }
        },
        "090af11f8dea4200b3b56e882886f81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec373ce3b99940b48e4bb32a98f95498",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_320c4b972eb04f19a548001bee04f39f",
            "value": 1
          }
        },
        "a85874e66f3341a3a04cb03630dbe264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd599798e9a647e3b8d93f6426822ac0",
            "placeholder": "​",
            "style": "IPY_MODEL_669bb5882cc942eeb73b4c429d34fff4",
            "value": " 1/4 [00:20&lt;01:01, 20.48s/it]"
          }
        },
        "b69ebd9e39c74b09be9e16dfc737c9e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68d99f03586f4350918a4734eddb6962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa5cbc1db214521a746bdcf8782e052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec373ce3b99940b48e4bb32a98f95498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320c4b972eb04f19a548001bee04f39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd599798e9a647e3b8d93f6426822ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669bb5882cc942eeb73b4c429d34fff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "522d9e0eb6d342dcb43deba08803a641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_090d633349644c12a7806a446c90e7d7",
              "IPY_MODEL_1d55311f05d544b7a4365323ce7fa0b8",
              "IPY_MODEL_b8cd50a881c84af89477c813cf1572d8"
            ],
            "layout": "IPY_MODEL_6fac20dfe9624557a3e6438069328fbd"
          }
        },
        "090d633349644c12a7806a446c90e7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3451ed967b3d4a30bd8a76462b3d869d",
            "placeholder": "​",
            "style": "IPY_MODEL_857897ffeee449ebaeb4c96954f45faf",
            "value": "Loading checkpoint shards:  33%"
          }
        },
        "1d55311f05d544b7a4365323ce7fa0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65ffa0ea89a3428b8679eba7f4110cd4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33e9b3d1a40640bca44e65f84c08d73b",
            "value": 1
          }
        },
        "b8cd50a881c84af89477c813cf1572d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3183372ce37b4271a8a8754294f1f2c9",
            "placeholder": "​",
            "style": "IPY_MODEL_7701aa1cb9dc4d5aac9814c6b044de29",
            "value": " 1/3 [00:25&lt;00:50, 25.42s/it]"
          }
        },
        "6fac20dfe9624557a3e6438069328fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3451ed967b3d4a30bd8a76462b3d869d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857897ffeee449ebaeb4c96954f45faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65ffa0ea89a3428b8679eba7f4110cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33e9b3d1a40640bca44e65f84c08d73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3183372ce37b4271a8a8754294f1f2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7701aa1cb9dc4d5aac9814c6b044de29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd3ae70be547418b983b586f83fbb63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4334c987639f4bfa8f38a1782dcfbfc7",
              "IPY_MODEL_59a4c26b25ae456dbbdbcc4dec5aba0e",
              "IPY_MODEL_a8d13208fe5d4d9f87c7abfefd6073bd"
            ],
            "layout": "IPY_MODEL_b0fc2d5e3c404181a36b76df77cf606a"
          }
        },
        "4334c987639f4bfa8f38a1782dcfbfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a47ec054872469dba90108cae9920b6",
            "placeholder": "​",
            "style": "IPY_MODEL_12b1b9fa750e4744a87e62dfb3cfd78a",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "59a4c26b25ae456dbbdbcc4dec5aba0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72dd339b574941f7b6b8ceeaae84730f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d42f30d50fe4b1a89bbe850dc49d65a",
            "value": 0
          }
        },
        "a8d13208fe5d4d9f87c7abfefd6073bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a135c690a77472c8425bd312214d74a",
            "placeholder": "​",
            "style": "IPY_MODEL_ecc3cd17d5ed494ba3fb2f44567ec764",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "b0fc2d5e3c404181a36b76df77cf606a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a47ec054872469dba90108cae9920b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12b1b9fa750e4744a87e62dfb3cfd78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72dd339b574941f7b6b8ceeaae84730f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d42f30d50fe4b1a89bbe850dc49d65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a135c690a77472c8425bd312214d74a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc3cd17d5ed494ba3fb2f44567ec764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ce2cbcd957c4e4eb14b1badb4d7d282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bcb9f9108ba4be08b2b280b5424a37f",
              "IPY_MODEL_9addea728b2e4bd99878eabb1d48d686",
              "IPY_MODEL_21c5598205634771903f7eeada1dd64f"
            ],
            "layout": "IPY_MODEL_aee5cdd5bd2c4dd884e2a24cc911e70a"
          }
        },
        "3bcb9f9108ba4be08b2b280b5424a37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_491ace4357e54a94aa8eb67c69d2836c",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2fe62dfb8e4385a5f7de3887068446",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9addea728b2e4bd99878eabb1d48d686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ae1bed0b5148a48357b91fa02f7ba0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_950b701975ce493294fa19bf2d4c2280",
            "value": 2
          }
        },
        "21c5598205634771903f7eeada1dd64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb4db1934db42cc9623575c11921428",
            "placeholder": "​",
            "style": "IPY_MODEL_8befbe9db35e42b695c6269183a3f81d",
            "value": " 2/2 [00:43&lt;00:00, 18.88s/it]"
          }
        },
        "aee5cdd5bd2c4dd884e2a24cc911e70a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "491ace4357e54a94aa8eb67c69d2836c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c2fe62dfb8e4385a5f7de3887068446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ae1bed0b5148a48357b91fa02f7ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950b701975ce493294fa19bf2d4c2280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6eb4db1934db42cc9623575c11921428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8befbe9db35e42b695c6269183a3f81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7453f9da166f4340ba72e669aa64b0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_042b007acb224cd98900535d847c9d68",
              "IPY_MODEL_b6e89764194c47e0a959384e5ff3bd98",
              "IPY_MODEL_9fbdebb939a54f4f937c3ae40a66fecf"
            ],
            "layout": "IPY_MODEL_52944bab8d194cb3b254cba781eeb293"
          }
        },
        "042b007acb224cd98900535d847c9d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bbdbcc3d5774ccdadfa52628be68743",
            "placeholder": "​",
            "style": "IPY_MODEL_f9a4d43f56914b9093be6b117502b74f",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "b6e89764194c47e0a959384e5ff3bd98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d6fe5fde3a4c32a588c12af22e2c1c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eb3cfe59e2d405d932eea445d71a944",
            "value": 0
          }
        },
        "9fbdebb939a54f4f937c3ae40a66fecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c168bfe74fb4d65b2e8c6aa7f8a0c1e",
            "placeholder": "​",
            "style": "IPY_MODEL_ff2670c72e1f45758a930322fd9bf7f0",
            "value": " 0/2 [00:01&lt;?, ?it/s]"
          }
        },
        "52944bab8d194cb3b254cba781eeb293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bbdbcc3d5774ccdadfa52628be68743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a4d43f56914b9093be6b117502b74f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07d6fe5fde3a4c32a588c12af22e2c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb3cfe59e2d405d932eea445d71a944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c168bfe74fb4d65b2e8c6aa7f8a0c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2670c72e1f45758a930322fd9bf7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUraEM7WVuIC",
        "outputId": "88abb238-e57e-443d-9a59-de7332eb655a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "DataFrame loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your processed text data\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "    print(\"DataFrame loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {processed_csv_path}\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    df = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if df is not None:\n",
        "    print(\"Column names in your DataFrame:\")\n",
        "    print(df.columns)\n",
        "else:\n",
        "    print(\"DataFrame was not loaded. Please check the loading step.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvYp6_ZhWWnY",
        "outputId": "6bab3c35-f7ec-4139-f11d-b86b8fd9eea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in your DataFrame:\n",
            "Index(['tweet_id', 'image_id', 'text_info', 'text_info_conf', 'image_info',\n",
            "       'image_info_conf', 'text_human', 'text_human_conf', 'image_human',\n",
            "       'image_human_conf', 'tweet_text', 'image_url', 'image_path',\n",
            "       'crisis_type', 'is_california_fire', 'has_image', 'cleaned_text',\n",
            "       'hashtags', 'entities', 'metadata', 'text_embedding'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if df is not None and 'text_human' in df.columns:\n",
        "    print(\"Unique values in 'text_human' column:\")\n",
        "    print(df['text_human'].unique())\n",
        "else:\n",
        "    print(\"DataFrame not loaded or 'text_human' column not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXrmdvJPV9gg",
        "outputId": "8b7f1404-e1c7-4840-b901-3009377947f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'text_human' column:\n",
            "['infrastructure_and_utility_damage' 'other_relevant_information'\n",
            " 'not_humanitarian' 'injured_or_dead_people'\n",
            " 'rescue_volunteering_or_donation_effort' 'affected_individuals'\n",
            " 'vehicle_damage' 'missing_or_found_people']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if df is not None and 'text_info' in df.columns:\n",
        "    print(\"Unique values in 'text_info' column:\")\n",
        "    print(df['text_info'].unique())\n",
        "else:\n",
        "    print(\"DataFrame not loaded or 'text_info' column not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_9m-KtUV9jF",
        "outputId": "7136581d-033c-420f-90dd-2ee987471f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'text_info' column:\n",
            "['informative' 'not_informative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if df is not None:\n",
        "    # Select the text and labels\n",
        "    text_column = 'cleaned_text'\n",
        "    label_column = 'text_info'  # Using 'text_info' as the label column\n",
        "\n",
        "    # Check if the label column exists\n",
        "    if label_column not in df.columns:\n",
        "        print(f\"Error: Label column '{label_column}' not found in DataFrame.\")\n",
        "        df = None\n",
        "    else:\n",
        "        # Convert labels to numerical (assuming 'informative' is string 'informative'/'not informative')\n",
        "        df['labels'] = df[label_column].apply(lambda x: 1 if str(x).lower() == 'informative' else 0)\n",
        "\n",
        "        # Select only the necessary columns\n",
        "        df = df[[text_column, 'labels']].dropna()\n",
        "\n",
        "        # Split data into training and validation sets\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "        print(f\"Training set size: {len(train_df)}\")\n",
        "        print(f\"Validation set size: {len(val_df)}\")\n",
        "else:\n",
        "    print(\"DataFrame not loaded, cannot prepare data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMwVdWNsV9nE",
        "outputId": "be6a891f-c906-45c5-ec0d-5ec8855e2a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 14465\n",
            "Validation set size: 3617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_df is not None and val_df is not None:\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "    model_name = 'distilbert-base-uncased'  # A smaller, faster model for demonstration\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # 2 classes: informative and not informative\n",
        "else:\n",
        "    print(\"Data not prepared, cannot load model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "fd28c1cd608b44558cc9203464198d18",
            "a01df0c54b854e2dbf770f480ae01d4f",
            "9d6c11e19d0a4612b6a8b76b73a9df28",
            "633f429e39474bfbb07748c97fce8766",
            "67fe2ed54387456b98eca728f9ac1a08",
            "822d5e24c77446e08799a0815e6b6116",
            "d65aff30c2cc45d3a6c78545865b6161",
            "9767d247305447feaa674ede6d553788",
            "7d0bf68f7f4e46c08c05e0185c7c0a21",
            "bc25b00372744bdeb5a1d435533b8a71",
            "721d6315de994cd689e2a54d6f7c42d1",
            "8445cd75e0b9455586a05aa2bedf336c",
            "0667b7793c12402982331faa4bcd727f",
            "3c0e6c9d603f46fcb14599823a3412a4",
            "43691f611452486d9f4a032740028b12",
            "dbea187355f34ab590a0d1d9140b110b",
            "f126d99d5c714e618ac28e3f90756f8c",
            "a5580d65caf44a5e898534355a64ab00",
            "c0bb790646cc459d8d864f1e9af40acb",
            "58a49542022140009c52df8541b6d05b",
            "3e956c5bb86c4d1184545ce7251edc64",
            "72651a132015464591c5980984cedf99",
            "63f489b1b50f43b7a7b0cd2a0f1de046",
            "0982e594c6484b1eb31d4b4668ba0702",
            "4fe32c2a42734a22b8348361d796c802",
            "887aeb04e4c14737838c6a003564664b",
            "ba716695f6184a5d8c706d1997a894f3",
            "2df40a9845d845a998ec8a0045670612",
            "5f6f4c4dd8374429ae7f847f55546b66",
            "2360c774e74141cd9ed67f3625713b55",
            "9ae0a24840814fa8a79c80b3b3ff8b10",
            "2e9961685a04465ab44bf3822b822f28",
            "35025f8d85994e30a3e05c131e296807",
            "f9e10be8f5ab47879ee35d0171e40097",
            "2f7f1c0df08343cfaf1fb6cdf305b416",
            "8f4d664d5b9e4dac845d6a43e3745d56",
            "b193797e6b1848c1929e3a321a5bae83",
            "c9aa7449bd5b455ea9d5f841daa965a3",
            "4af93b6ab2ad4a7991e222a9d32d09e6",
            "1d257d54902a45f58e5390266305d819",
            "352bf7a2eabd420d93f9670a9ce5c8de",
            "71985b239d404299a7910569ed6c3be9",
            "4e7e9b44aff1463e9814b54aecca02ed",
            "ba44c41bdd0b45adad779a46965b484d",
            "ccec796e6a5e46ee99693549fdb07ad1",
            "7c0cf0f5a28d46159d7718bc85c0f1a4",
            "0837992c4cff45a19d63143619dfd55e",
            "93ddb8f54372468584b178e6059da89f",
            "ab5d7a325e8f4045b70177ae7c2204be",
            "cea51187badc4ee3beb2850882669d2b",
            "782940afb8fa434f86f9284a57b2b399",
            "e4b151ddf17b47b8a5aa748ad6340e5f",
            "8866a1b2ee1a4f0e98172b784d540bc2",
            "d92b0a5b09254810ad2b898b449e84f0",
            "6c6444ecb9b94afc87d55238e0b5085f"
          ]
        },
        "id": "O40l_dFyXBtn",
        "outputId": "4a709e1b-3047-4c7d-8d27-412e12c7292d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd28c1cd608b44558cc9203464198d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8445cd75e0b9455586a05aa2bedf336c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63f489b1b50f43b7a7b0cd2a0f1de046"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9e10be8f5ab47879ee35d0171e40097"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccec796e6a5e46ee99693549fdb07ad1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsP-aKdrXq9f",
        "outputId": "a9b0b760-32f0-4518-c3fd-2656368e7095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is not None:\n",
        "    from datasets import Dataset\n",
        "\n",
        "    # Convert pandas DataFrames to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['cleaned_text'], truncation=True, padding='max_length', max_length=128) # Example: limit to 128 tokens\n",
        "\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Remove the original text column as the model will use the tokenized inputs\n",
        "    tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"cleaned_text\", \"__index_level_0__\"])\n",
        "    tokenized_val_dataset = tokenized_val_dataset.remove_columns([\"cleaned_text\", \"__index_level_0__\"])\n",
        "\n",
        "    # Rename the label column to 'labels' as expected by the Trainer\n",
        "    tokenized_train_dataset = tokenized_train_dataset.rename_column(\"labels\", \"label\")\n",
        "    tokenized_val_dataset = tokenized_val_dataset.rename_column(\"labels\", \"label\")\n",
        "\n",
        "    print(\"Datasets tokenized.\")\n",
        "else:\n",
        "    print(\"Model not loaded, cannot tokenize datasets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "8183b7336dc0451e805cb2a709c256ed",
            "9682f9ff88084c0f89da96f1511a3ea6",
            "1f77055a4392443b87e53f5f4d4b4da5",
            "00242aebc4744240a5b84a18396a65c4",
            "a76011533c4849c6a3cfde0219478bcd",
            "075cd21afabd4557a15a07568ea3a214",
            "80e3e98ba1524a99a85cfa88b56a01f2",
            "e7d839a75fbb4e7a8bbacd2474f7c802",
            "d969b46882894077bc1fb31492477cde",
            "95720dccb36242a98433a21946bb491e",
            "2d1e0e31c3e44bfdbe3c33154fcd7a93",
            "b5c7296dbb3a48c7a011ce0197bf8153",
            "ad1876986f284f779475faf5378c83da",
            "3ee29a7734dc4ec5a1ffc9b998a1868d",
            "905cadc6c4e045fba6e821b70ea38254",
            "4a7a0116fc3c409884d2849d0227f7b9",
            "262d571b7e764fafadcb70d5cbe338f5",
            "d7710364e5f14752b3949df5b681eab6",
            "96596594286142c7891d3222257a70ba",
            "6f1f48dca3c44e70be1ba574d0fc7796",
            "829bb76189ea4130a5cdf4371872c061",
            "f943bd356b874b70a1ac840a1ec9e5d0"
          ]
        },
        "id": "BgCxsnsFXBwT",
        "outputId": "d9cc378b-74c7-46e7-c65d-f7e7027a902e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14465 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8183b7336dc0451e805cb2a709c256ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3617 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5c7296dbb3a48c7a011ce0197bf8153"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets tokenized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_dataset is not None and val_dataset is not None:\n",
        "    from transformers import TrainingArguments\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "      output_dir='./results_informative',\n",
        "      learning_rate=2e-5,\n",
        "      per_device_train_batch_size=16,  # Reduced batch size\n",
        "      per_device_eval_batch_size=16,\n",
        "      gradient_accumulation_steps=2,\n",
        "      num_train_epochs=3,\n",
        "      weight_decay=0.01,\n",
        "      evaluation_strategy='epoch',\n",
        "      save_strategy='epoch',\n",
        "      logging_dir='./logs_informative',\n",
        "      report_to=\"none\"\n",
        ")\n",
        "\n",
        "else:\n",
        "    print(\"Datasets not ready, cannot define training arguments.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFYrmmYeXBzs",
        "outputId": "3d668138-e34c-43f8-e824-522b13586e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if model is not None and tokenized_train_dataset is not None and tokenized_val_dataset is not None and training_args is not None:\n",
        "    from transformers import Trainer\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = pred.predictions.argmax(-1)\n",
        "        precision = precision_score(labels, preds, average='binary')\n",
        "        recall = recall_score(labels, preds, average='binary')\n",
        "        f1 = f1_score(labels, preds, average='binary')\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        return {\n",
        "            'accuracy': acc,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Training finished. Evaluating...\")\n",
        "    evaluation_results = trainer.evaluate()\n",
        "    print(evaluation_results)\n",
        "else:\n",
        "    print(\"Trainer not set up, cannot start training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "29QARglNXB3K",
        "outputId": "e11be361-848d-4abb-c3be-30dd38bc5705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-9eface384ec6>:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1356' max='1356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1356/1356 8:55:55, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.375412</td>\n",
              "      <td>0.842964</td>\n",
              "      <td>0.892179</td>\n",
              "      <td>0.870048</td>\n",
              "      <td>0.915466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.287300</td>\n",
              "      <td>0.424813</td>\n",
              "      <td>0.841028</td>\n",
              "      <td>0.892140</td>\n",
              "      <td>0.860347</td>\n",
              "      <td>0.926373</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished. Evaluating...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='227' max='227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [227/227 12:46]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.42481330037117004, 'eval_accuracy': 0.8410284766380979, 'eval_f1': 0.8921403113862315, 'eval_precision': 0.8603473227206947, 'eval_recall': 0.9263731982859369, 'eval_runtime': 769.3811, 'eval_samples_per_second': 4.701, 'eval_steps_per_second': 0.295, 'epoch': 2.994475138121547}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load tokenizer and (ideally) the fine-tuned model ---\n",
        "model_name = 'distilbert-base-uncased' # Use this if you can't reload fine-tuned\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Ensure num_labels matches your task\n",
        "\n",
        "# Forcefully add a pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# --- Load your DataFrame ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "    print(\"DataFrame loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {processed_csv_path}\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    df = None\n",
        "\n",
        "# --- Prepare labels (assuming 'text_info' column) ---\n",
        "if df is not None and 'text_info' in df.columns:\n",
        "    df['labels'] = df['text_info'].apply(lambda x: 1 if str(x).lower() == 'informative' else 0)\n",
        "    df_processed = df[['cleaned_text', 'labels']].dropna()\n",
        "else:\n",
        "    print(\"Error: DataFrame not loaded or 'text_info' column not found.\")\n",
        "    df_processed = None\n",
        "\n",
        "# --- Prepare validation data ---\n",
        "if df_processed is not None:\n",
        "    train_df, val_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    val_df = None\n",
        "\n",
        "if val_df is not None:\n",
        "    prompt_template = \"Classify the following tweet as either informative or not informative regarding a disaster: [TEXT]\"\n",
        "\n",
        "    def create_prompted_text(text):\n",
        "        return prompt_template.replace(\"[TEXT]\", text)\n",
        "\n",
        "    prompted_val_texts = [create_prompted_text(text) for text in val_df['cleaned_text'].tolist()]\n",
        "\n",
        "    # Tokenize the prompted texts\n",
        "    tokenizer.pad_token = '[PAD]'  # Ensure pad_token is explicitly set\n",
        "    prompted_val_tokens = tokenizer(prompted_val_texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=128).to(model.device)\n",
        "\n",
        "    # Get predictions from the (ideally fine-tuned) model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prompted_predictions = model(**prompted_val_tokens)\n",
        "        prompted_predicted_labels = torch.argmax(prompted_predictions.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    # Get true labels from the validation set\n",
        "    true_labels = val_df['labels'].tolist()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report_with_prompts = classification_report(true_labels, prompted_predicted_labels, target_names=['not_informative', 'informative'])\n",
        "    print(\"\\nClassification Report on Validation Set (with Prompt):\")\n",
        "    print(report_with_prompts)\n",
        "\n",
        "else:\n",
        "    print(\"Validation DataFrame not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y361V-UEXB6I",
        "outputId": "0a976836-e334-44ec-917d-085e9da4cfe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded successfully.\n",
            "\n",
            "Classification Report on Validation Set (with Prompt):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "not_informative       0.00      0.00      0.00      1050\n",
            "    informative       0.71      1.00      0.83      2567\n",
            "\n",
            "       accuracy                           0.71      3617\n",
            "      macro avg       0.35      0.50      0.42      3617\n",
            "   weighted avg       0.50      0.71      0.59      3617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load tokenizer and base model ---\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Ensure pad_token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# --- Load your DataFrame and prepare validation data (as before) ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "except FileNotFoundError:\n",
        "    df = None\n",
        "\n",
        "if df is not None and 'text_info' in df.columns:\n",
        "    df['labels'] = df['text_info'].apply(lambda x: 1 if str(x).lower() == 'informative' else 0)\n",
        "    df_processed = df[['cleaned_text', 'labels']].dropna()\n",
        "    train_df, val_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    val_df = None\n",
        "\n",
        "if val_df is not None:\n",
        "    # Tokenize the raw validation texts (without prompt)\n",
        "    val_texts = val_df['cleaned_text'].tolist()\n",
        "    val_tokens = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=128).to(model.device)\n",
        "\n",
        "    # Get predictions from the base model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        base_predictions = model(**val_tokens)\n",
        "        base_predicted_labels = torch.argmax(base_predictions.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    true_labels = val_df['labels'].tolist()\n",
        "\n",
        "    # Generate the classification report for the base model (no prompt)\n",
        "    report_base = classification_report(true_labels, base_predicted_labels, target_names=['not_informative', 'informative'])\n",
        "    print(\"\\nClassification Report on Validation Set (Base Model, No Prompt):\")\n",
        "    print(report_base)\n",
        "\n",
        "else:\n",
        "    print(\"Validation DataFrame not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5pt3U0wXB81",
        "outputId": "77b460cb-be57-4a07-989f-7b11908958c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report on Validation Set (Base Model, No Prompt):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "not_informative       0.19      0.28      0.22      1050\n",
            "    informative       0.63      0.49      0.55      2567\n",
            "\n",
            "       accuracy                           0.43      3617\n",
            "      macro avg       0.41      0.39      0.39      3617\n",
            "   weighted avg       0.50      0.43      0.45      3617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load tokenizer and (ideally) the fine-tuned model ---\n",
        "# Replace 'distilbert-base-uncased' with the path to your saved fine-tuned model if available\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Ensure pad_token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# --- Load your DataFrame and prepare validation data (as before) ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "except FileNotFoundError:\n",
        "    df = None\n",
        "\n",
        "if df is not None and 'text_info' in df.columns:\n",
        "    df['labels'] = df['text_info'].apply(lambda x: 1 if str(x).lower() == 'informative' else 0)\n",
        "    df_processed = df[['cleaned_text', 'labels']].dropna()\n",
        "    train_df, val_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
        "else:\n",
        "    val_df = None\n",
        "\n",
        "if val_df is not None:\n",
        "    # Tokenize the raw validation texts (without prompt)\n",
        "    val_texts = val_df['cleaned_text'].tolist()\n",
        "    val_tokens = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=128).to(model.device)\n",
        "\n",
        "    # Get predictions from the base model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        base_predictions = model(**val_tokens)\n",
        "        base_predicted_labels = torch.argmax(base_predictions.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    true_labels = val_df['labels'].tolist()\n",
        "\n",
        "    # Generate the classification report for the base model (no prompt)\n",
        "    report_base = classification_report(true_labels, base_predicted_labels, target_names=['not_informative', 'informative'])\n",
        "    print(\"\\nClassification Report on Validation Set (Base Model, No Prompt):\")\n",
        "    print(report_base)\n",
        "\n",
        "else:\n",
        "    print(\"Validation DataFrame not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVcLliHcXCBN",
        "outputId": "21f749cf-f30f-49e7-839a-172caca53c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report on Validation Set (Base Model, No Prompt):\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "not_informative       0.26      0.71      0.38      1050\n",
            "    informative       0.57      0.16      0.25      2567\n",
            "\n",
            "       accuracy                           0.32      3617\n",
            "      macro avg       0.41      0.43      0.31      3617\n",
            "   weighted avg       0.48      0.32      0.29      3617\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG**"
      ],
      "metadata": {
        "id": "Sg5nE88jBClh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Load your DataFrame ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "    print(\"DataFrame loaded successfully for RAG preparation.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {processed_csv_path}\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    df = None\n",
        "\n",
        "if df is not None and 'cleaned_text' in df.columns and 'text_info' in df.columns:\n",
        "    # Filter for informative tweets (using ground truth)\n",
        "    informative_tweets_df = df[df['text_info'].str.lower() == 'informative'][['cleaned_text']]\n",
        "    print(f\"{len(informative_tweets_df)} informative tweets identified for RAG.\")\n",
        "else:\n",
        "    informative_tweets_df = None\n",
        "    print(\"Error: DataFrame or required columns not found for RAG preparation.\")\n",
        "\n",
        "# Now, informative_tweets_df contains the text of your informative tweets.\n",
        "# This is your basic knowledge base for retrieval."
      ],
      "metadata": {
        "id": "oNOoZcqSkziR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3959bb-3863-4df4-bbde-5a477a8c620b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded successfully for RAG preparation.\n",
            "12855 informative tweets identified for RAG.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The below retrieved tweets don't seem directly related to the query about \"power outages.\" This highlights the limitations of a simple keyword-based search. The keywords \"hurricane,\" \"irma,\" \"weather,\" and \"advisory\" were likely present in the informative tweets, but they don't specifically address power outages."
      ],
      "metadata": {
        "id": "7xi6xv44B9yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_tweets(user_query, knowledge_base_df, top_n=3):\n",
        "    \"\"\"\n",
        "    Retrieves the top_n most relevant tweets from the knowledge base\n",
        "    based on keyword matching with the user query.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question or request.\n",
        "        knowledge_base_df (pd.DataFrame): DataFrame containing informative tweets\n",
        "                                           with a 'cleaned_text' column.\n",
        "        top_n (int): The maximum number of relevant tweets to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of the top_n most relevant tweet texts.\n",
        "    \"\"\"\n",
        "    if knowledge_base_df is None or knowledge_base_df.empty:\n",
        "        return []\n",
        "\n",
        "    query_keywords = user_query.lower().split()\n",
        "    relevant_tweets = []\n",
        "\n",
        "    for index, row in knowledge_base_df.iterrows():\n",
        "        tweet_text = row['cleaned_text'].lower()\n",
        "        for keyword in query_keywords:\n",
        "            if keyword in tweet_text:\n",
        "                relevant_tweets.append(row['cleaned_text'])\n",
        "                break  # Move to the next tweet once a keyword is found\n",
        "\n",
        "    return relevant_tweets[:top_n]\n",
        "\n",
        "# Example usage:\n",
        "user_question = \"What is the current situation with power outages?\"\n",
        "relevant_info = retrieve_relevant_tweets(user_question, informative_tweets_df)\n",
        "\n",
        "if relevant_info:\n",
        "    print(f\"Retrieved relevant information for query: '{user_question}'\")\n",
        "    for i, tweet in enumerate(relevant_info):\n",
        "        print(f\"[{i+1}] {tweet}\")\n",
        "else:\n",
        "    print(f\"No relevant information found for query: '{user_question}'\")"
      ],
      "metadata": {
        "id": "MkPHi7eeXCI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c754e293-3cf7-4118-f906-45e82934875a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved relevant information for query: 'What is the current situation with power outages?'\n",
            "[1] rt island barbuda literally water hurricane irma\n",
            "[2] pm hurricane irma update weather\n",
            "[3] rt pm advisory hurricane irma firstalertwx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "still not very relevant results"
      ],
      "metadata": {
        "id": "SyGwbFd-F8Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_tweets_improved(user_query, knowledge_base_df, top_n=3):\n",
        "    \"\"\"\n",
        "    Retrieves the top_n most relevant tweets based on the number of\n",
        "    matching keywords.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's question or request.\n",
        "        knowledge_base_df (pd.DataFrame): DataFrame containing informative tweets\n",
        "                                           with a 'cleaned_text' column.\n",
        "        top_n (int): The maximum number of relevant tweets to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of the top_n most relevant tweet texts, ordered by\n",
        "              the number of matching keywords (descending).\n",
        "    \"\"\"\n",
        "    if knowledge_base_df is None or knowledge_base_df.empty:\n",
        "        return []\n",
        "\n",
        "    query_keywords = user_query.lower().split()\n",
        "    tweet_scores = {}\n",
        "\n",
        "    for index, row in knowledge_base_df.iterrows():\n",
        "        tweet_text = row['cleaned_text'].lower()\n",
        "        score = 0\n",
        "        for keyword in query_keywords:\n",
        "            if keyword in tweet_text:\n",
        "                score += 1\n",
        "        if score > 0:\n",
        "            tweet_scores[row['cleaned_text']] = score\n",
        "\n",
        "    # Sort tweets by score (highest score first) and get the top_n\n",
        "    sorted_tweets = sorted(tweet_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [tweet for tweet, score in sorted_tweets[:top_n]]\n",
        "\n",
        "# Example usage with the improved function:\n",
        "user_question = \"What is the current situation with power outages?\"\n",
        "relevant_info = retrieve_relevant_tweets_improved(user_question, informative_tweets_df)\n",
        "\n",
        "if relevant_info:\n",
        "    print(f\"Retrieved relevant information for query: '{user_question}' (Improved)\")\n",
        "    for i, tweet in enumerate(relevant_info):\n",
        "        print(f\"[{i+1}] {tweet}\")\n",
        "else:\n",
        "    print(f\"No relevant information found for query: '{user_question}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYlw2FUBCTcb",
        "outputId": "56877103-a2d7-478f-994a-e367fe4325f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved relevant information for query: 'What is the current situation with power outages?' (Improved)\n",
            "[1] theguardianukcrisis grows puerto rico town without water power phone service\n",
            "[2] powerhouse chef thepalmbeaches partner fundraising dinner sept aid florida key post\n",
            "[3] powerful hurricane irma could next weather disaster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simulating the generation -ignore\n",
        "The language model identifies the word \"power\" in the first tweet.\n",
        "It understands that a lack of power is a common problem during crises.\n",
        "It likely disregards the irrelevant context of the other two tweets (\"powerhouse chef,\" \"powerful hurricane\" in a future context).\n",
        "It uses its general knowledge to infer that power outages are a relevant concern during disasters."
      ],
      "metadata": {
        "id": "sEhzaGqOF_bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# --- Step 1: Prepare Your Informative Data for Retrieval (Ground Truth) ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "    print(\"DataFrame loaded successfully for RAG preparation.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {processed_csv_path}\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    df = None\n",
        "\n",
        "informative_tweets_df = None\n",
        "if df is not None and 'cleaned_text' in df.columns and 'text_info' in df.columns:\n",
        "    informative_tweets_df = df[df['text_info'].str.lower() == 'informative'][['cleaned_text']].reset_index(drop=True)\n",
        "    print(f\"{len(informative_tweets_df)} informative tweets identified for RAG.\")\n",
        "else:\n",
        "    print(\"Error: DataFrame or required columns not found for RAG preparation.\")\n",
        "\n",
        "# --- Step 2: Implement the Retrieval Mechanism (Improved Keyword Matching) ---\n",
        "def retrieve_relevant_tweets_improved(user_query, knowledge_base_df, top_n=3):\n",
        "    if knowledge_base_df is None or knowledge_base_df.empty:\n",
        "        return []\n",
        "\n",
        "    query_keywords = user_query.lower().split()\n",
        "    tweet_scores = {}\n",
        "\n",
        "    for index, row in knowledge_base_df.iterrows():\n",
        "        tweet_text = row['cleaned_text'].lower()\n",
        "        score = 0\n",
        "        for keyword in query_keywords:\n",
        "            if keyword in tweet_text:\n",
        "                score += 1\n",
        "        if score > 0:\n",
        "            tweet_scores[row['cleaned_text']] = score\n",
        "\n",
        "    sorted_tweets = sorted(tweet_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [tweet for tweet, score in sorted_tweets[:top_n]]\n",
        "\n",
        "# --- Step 3: Simulate the Generation Step (Illustrative - No actual LLM API call) ---\n",
        "def generate_answer(user_question, retrieved_tweets):\n",
        "    if not retrieved_tweets:\n",
        "        return \"No relevant information found to answer your question.\"\n",
        "\n",
        "    context = \"\\n\".join([f\"Tweet {i+1}: {tweet}\" for i, tweet in enumerate(retrieved_tweets)])\n",
        "    prompt = f\"\"\"Answer the user's question based on the following retrieved information:\n",
        "{context}\n",
        "User Question: {user_question}\"\"\"\n",
        "\n",
        "    print(\"\\n--- Simulated Prompt ---\")\n",
        "    print(prompt)\n",
        "    print(\"\\n--- Simulated Answer (Illustrative) ---\")\n",
        "    # In a real application, you would send this prompt to a language model API\n",
        "    # and get the actual generated answer.\n",
        "    return \"Based on the retrieved information, and general knowledge, here's a potential answer...\"\n",
        "\n",
        "# --- Example RAG Workflow ---\n",
        "if informative_tweets_df is not None:\n",
        "    user_question = \"What is the current situation with power outages?\"\n",
        "    relevant_info = retrieve_relevant_tweets_improved(user_question, informative_tweets_df)\n",
        "\n",
        "    print(f\"\\nRetrieved relevant information for query: '{user_question}' (Improved)\")\n",
        "    for i, tweet in enumerate(relevant_info):\n",
        "        print(f\"[{i+1}] {tweet}\")\n",
        "\n",
        "    answer = generate_answer(user_question, relevant_info)\n",
        "    print(\"\\n--- RAG Simulated Answer ---\")\n",
        "    print(answer)\n",
        "\n",
        "else:\n",
        "    print(\"Knowledge base not available, cannot run RAG example.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKtQ_HVvF6M5",
        "outputId": "0dd604e2-878d-4e82-fd24-7063aff4441e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-110c423a2dfc>\", line 2, in <cell line: 0>\n",
            "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 25, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame loaded successfully for RAG preparation.\n",
            "12855 informative tweets identified for RAG.\n",
            "\n",
            "Retrieved relevant information for query: 'What is the current situation with power outages?' (Improved)\n",
            "[1] theguardianukcrisis grows puerto rico town without water power phone service\n",
            "[2] powerhouse chef thepalmbeaches partner fundraising dinner sept aid florida key post\n",
            "[3] powerful hurricane irma could next weather disaster\n",
            "\n",
            "--- Simulated Prompt ---\n",
            "Answer the user's question based on the following retrieved information:\n",
            "Tweet 1: theguardianukcrisis grows puerto rico town without water power phone service\n",
            "Tweet 2: powerhouse chef thepalmbeaches partner fundraising dinner sept aid florida key post\n",
            "Tweet 3: powerful hurricane irma could next weather disaster\n",
            "User Question: What is the current situation with power outages?\n",
            "\n",
            "--- Simulated Answer (Illustrative) ---\n",
            "\n",
            "--- RAG Simulated Answer ---\n",
            "Based on the retrieved information, and general knowledge, here's a potential answer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "real generation with llm-ignore"
      ],
      "metadata": {
        "id": "GeC90vHEIHVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4RZol-aILM8",
        "outputId": "46896643-d006-4bc4-a5c6-524e45fb4aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q0TYPvpOIOlH",
        "outputId": "b716ff0f-3308-4612-83c0-574aa9731c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "00656444e08349b49ef00572f7e972f2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eviMcLMCQLti",
        "outputId": "d3e69688-7273-4921-a002-b17d5dffbdd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall -y torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh6YyPvkQPBh",
        "outputId": "300f2901-89ec-4da0-ea30-a06e9fb85b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchvision==0.16.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qei8WQpMR_vx",
        "outputId": "2e7c9bb5-cf51-4dd6-bb48-eb6c34d9d184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torchvision==0.16.0+cu118 in /usr/local/lib/python3.11/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (2.32.3)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (2.1.0+rocm5.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (2025.3.2)\n",
            "Requirement already satisfied: pytorch-triton-rocm==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchvision==0.16.0+cu118) (2.1.0)\n",
            "Requirement already satisfied: cmake>=3.20 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton-rocm==2.1.0->torch==2.1.0->torchvision==0.16.0+cu118) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from pytorch-triton-rocm==2.1.0->torch==2.1.0->torchvision==0.16.0+cu118) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0->torchvision==0.16.0+cu118) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0->torchvision==0.16.0+cu118) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "9bcaa0502103493a9b2fc95288d1df91",
            "ac84cf9ffeca4802989abb7f00ba6896",
            "23eea19bda7543d2971ea3ffe782c812",
            "f445090b0f2041938e871325cf850734",
            "1c38f08803734a5b80c2544dd7b3acb3",
            "8e87965262954c7db71b101e12c8548c",
            "ad6ea98694f4470c8af67a4b58fc4a68",
            "bf53d80cdba14c3da9d8eb338114c2b3",
            "9d1d764b462d4f39bd47b62b65549809",
            "3ab2c537be894780842c108282845b0e",
            "f57e6932ee3f43baa7e178b139f42245",
            "edd1ad2eb29542aabcdb9ef0604d2f4f",
            "c78b969277234a909d50d9f2ed341c97",
            "29972545987b49799b7512c83356b19c",
            "12abdd5734e04fe2b33874f5bd84d874",
            "adbf3e6ef84e47c89360af489cccf4e8",
            "e28251beee814214bf60482fab6fb411",
            "d12dbd20052e4b9290216beeebce642c",
            "8de1e49d8f124526803433960be21f6d",
            "975106362b6743b88df99a83bb101c0e"
          ]
        },
        "id": "NFywHtNLTXB3",
        "outputId": "23910e57-8a46-4da1-8bab-cbb8851a03a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bcaa0502103493a9b2fc95288d1df91"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ssAsUkjV6ij",
        "outputId": "522b549d-c98b-4cca-ae09-774f5cd0f3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "The token `access token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `access token`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ensure no large variables are taking up RAM ---\n",
        "if 'df' in locals():\n",
        "    del df\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# --- Load Llama 3 Model and Tokenizer ---\n",
        "model_name_llama = \"meta-llama/Meta-Llama-3-8b-instruct\"\n",
        "your_token = \"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\"  # Replace with your actual token\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_name_llama, token=your_token)\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(model_name_llama, token=your_token, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32).to(device)\n",
        "\n",
        "print(f\"Model and tokenizer for {model_name_llama} loaded on {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3cfe8922f4904ab5b257fc139f5bb7ee",
            "742f2f13f7c74f06b503ae4ef3f82f5a",
            "090af11f8dea4200b3b56e882886f81b",
            "a85874e66f3341a3a04cb03630dbe264",
            "b69ebd9e39c74b09be9e16dfc737c9e5",
            "68d99f03586f4350918a4734eddb6962",
            "6aa5cbc1db214521a746bdcf8782e052",
            "ec373ce3b99940b48e4bb32a98f95498",
            "320c4b972eb04f19a548001bee04f39f",
            "bd599798e9a647e3b8d93f6426822ac0",
            "669bb5882cc942eeb73b4c429d34fff4"
          ]
        },
        "id": "AUFgOpWqIOm9",
        "outputId": "c0c0ad98-0896-4c18-b7ad-ea940de83382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-bd939da5fd04>\", line 5, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cfe8922f4904ab5b257fc139f5bb7ee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mistral-ignore"
      ],
      "metadata": {
        "id": "C4uvjzUpjFNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load Mistral 7B Instruct v0.2 Model and Tokenizer ---\n",
        "model_name_mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_mistral = AutoModelForCausalLM.from_pretrained(model_name_mistral, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32).to(device)\n",
        "\n",
        "print(f\"Model and tokenizer for {model_name_mistral} loaded on {device}.\")\n",
        "\n",
        "# --- Load your DataFrame and prepare informative tweets (if not already done) ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "    informative_tweets_df = df[df['text_info'].str.lower() == 'informative'][['cleaned_text']].reset_index(drop=True)\n",
        "    print(\"Informative tweets DataFrame is ready.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {processed_csv_path}\")\n",
        "    informative_tweets_df = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    informative_tweets_df = None\n",
        "\n",
        "# --- Improved Retrieval Function (if not already defined) ---\n",
        "def retrieve_relevant_tweets_improved(user_query, knowledge_base_df, top_n=3):\n",
        "    if knowledge_base_df is None or knowledge_base_df.empty:\n",
        "        return []\n",
        "\n",
        "    query_keywords = user_query.lower().split()\n",
        "    tweet_scores = {}\n",
        "\n",
        "    for index, row in knowledge_base_df.iterrows():\n",
        "        tweet_text = row['cleaned_text'].lower()\n",
        "        score = 0\n",
        "        for keyword in query_keywords:\n",
        "            if keyword in tweet_text:\n",
        "                score += 1\n",
        "        if score > 0:\n",
        "            tweet_scores[row['cleaned_text']] = score\n",
        "\n",
        "    sorted_tweets = sorted(tweet_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [tweet for tweet, score in sorted_tweets[:top_n]]\n",
        "\n",
        "# --- Generate Answer with Mistral ---\n",
        "def generate_answer_with_mistral(user_question, retrieved_tweets, tokenizer, model):\n",
        "    if not retrieved_tweets:\n",
        "        return \"No relevant information found to answer your question.\"\n",
        "\n",
        "    context = \"\\n\".join([f\"Tweet {i+1}: {tweet}\" for i, tweet in enumerate(retrieved_tweets)])\n",
        "    prompt = f\"\"\"<s>[INST] Answer the user's question based on the following retrieved information:\n",
        "{context}\n",
        "User Question: {user_question} [/INST]\"\"\"\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=500,  # Adjust as needed\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.2,  # Adjust for creativity\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    assistant_start = generated_answer.find(\"[/INST]\")\n",
        "    if assistant_start != -1:\n",
        "        return generated_answer[assistant_start + len(\"[/INST]\") :].strip()\n",
        "    else:\n",
        "        return generated_answer.strip()\n",
        "\n",
        "# --- Example RAG Workflow ---\n",
        "if informative_tweets_df is not None:\n",
        "    user_question = \"What is the current situation with power outages?\"\n",
        "    relevant_info = retrieve_relevant_tweets_improved(user_question, informative_tweets_df)\n",
        "\n",
        "    print(f\"\\nRetrieved relevant information for query: '{user_question}' (Improved)\")\n",
        "    for i, tweet in enumerate(relevant_info):\n",
        "        print(f\"[{i+1}] {tweet}\")\n",
        "\n",
        "    answer = generate_answer_with_mistral(user_question, relevant_info, tokenizer_mistral, model_mistral)\n",
        "    print(\"\\n--- RAG Answer from Mistral 7B Instruct v0.2 ---\")\n",
        "    print(answer)\n",
        "\n",
        "else:\n",
        "    print(\"Knowledge base not available, cannot run RAG example.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "522d9e0eb6d342dcb43deba08803a641",
            "090d633349644c12a7806a446c90e7d7",
            "1d55311f05d544b7a4365323ce7fa0b8",
            "b8cd50a881c84af89477c813cf1572d8",
            "6fac20dfe9624557a3e6438069328fbd",
            "3451ed967b3d4a30bd8a76462b3d869d",
            "857897ffeee449ebaeb4c96954f45faf",
            "65ffa0ea89a3428b8679eba7f4110cd4",
            "33e9b3d1a40640bca44e65f84c08d73b",
            "3183372ce37b4271a8a8754294f1f2c9",
            "7701aa1cb9dc4d5aac9814c6b044de29"
          ]
        },
        "id": "cJnn-4xrIOtv",
        "outputId": "d7f4a0e7-64d2-4c2a-d439-27542855257a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-4c7222e53efe>\", line 1, in <cell line: 0>\n",
            "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 25, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "522d9e0eb6d342dcb43deba08803a641"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load your DataFrame and prepare informative tweets ---\n",
        "processed_csv_path = '/content/drive/MyDrive/processed_text_data_flair_extended.csv'\n",
        "try:\n",
        "    df = pd.read_csv(processed_csv_path)\n",
        "    informative_tweets_df = df[df['text_info'].str.lower() == 'informative'][['cleaned_text']].reset_index(drop=True)\n",
        "    print(f\"Original informative tweets DataFrame loaded with {len(informative_tweets_df)} entries.\")\n",
        "    # --- Create a small sample ---\n",
        "    informative_tweets_df_sample = informative_tweets_df.head(100)\n",
        "    print(f\"Working with a sample of {len(informative_tweets_df_sample)} informative tweets.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {processed_csv_path}\")\n",
        "    informative_tweets_df_sample = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    informative_tweets_df_sample = None\n",
        "\n",
        "# --- Load Mistral 7B Instruct v0.2 Model and Tokenizer ---\n",
        "model_name_mistral = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer_mistral = AutoTokenizer.from_pretrained(model_name_mistral)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_mistral = AutoModelForCausalLM.from_pretrained(model_name_mistral, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32).to(device)\n",
        "\n",
        "print(f\"Model and tokenizer for {model_name_mistral} loaded on {device}.\")\n",
        "\n",
        "# --- Improved Retrieval Function (using the sample) ---\n",
        "def retrieve_relevant_tweets_improved(user_query, knowledge_base_df, top_n=3):\n",
        "    if knowledge_base_df is None or knowledge_base_df.empty:\n",
        "        return []\n",
        "\n",
        "    query_keywords = user_query.lower().split()\n",
        "    tweet_scores = {}\n",
        "\n",
        "    for index, row in knowledge_base_df.iterrows():\n",
        "        tweet_text = row['cleaned_text'].lower()\n",
        "        score = 0\n",
        "        for keyword in query_keywords:\n",
        "            if keyword in tweet_text:\n",
        "                score += 1\n",
        "        if score > 0:\n",
        "            tweet_scores[row['cleaned_text']] = score\n",
        "\n",
        "    sorted_tweets = sorted(tweet_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [tweet for tweet, score in sorted_tweets[:top_n]]\n",
        "\n",
        "# --- Generate Answer with Mistral ---\n",
        "def generate_answer_with_mistral(user_question, retrieved_tweets, tokenizer, model):\n",
        "    if not retrieved_tweets:\n",
        "        return \"No relevant information found to answer your question.\"\n",
        "\n",
        "    context = \"\\n\".join([f\"Tweet {i+1}: {tweet}\" for i, tweet in enumerate(retrieved_tweets)])\n",
        "    prompt = f\"\"\"<s>[INST] Answer the user's question based on the following retrieved information:\n",
        "{context}\n",
        "User Question: {user_question} [/INST]\"\"\"\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=500,  # Adjust as needed\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.2,  # Adjust for creativity\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    assistant_start = generated_answer.find(\"[/INST]\")\n",
        "    if assistant_start != -1:\n",
        "        return generated_answer[assistant_start + len(\"[/INST]\") :].strip()\n",
        "    else:\n",
        "        return generated_answer.strip()\n",
        "\n",
        "# --- Example RAG Workflow (using the sample DataFrame) ---\n",
        "if informative_tweets_df_sample is not None:\n",
        "    user_question = \"What is the current situation with power outages?\"\n",
        "    relevant_info = retrieve_relevant_tweets_improved(user_question, informative_tweets_df_sample, top_n=3)\n",
        "\n",
        "    print(f\"\\nRetrieved relevant information for query: '{user_question}' (Improved - Sample)\")\n",
        "    for i, tweet in enumerate(relevant_info):\n",
        "        print(f\"[{i+1}] {tweet}\")\n",
        "\n",
        "    answer = generate_answer_with_mistral(user_question, relevant_info, tokenizer_mistral, model_mistral)\n",
        "    print(\"\\n--- RAG Answer from Mistral 7B Instruct v0.2 (Sample) ---\")\n",
        "    print(answer)\n",
        "\n",
        "else:\n",
        "    print(\"Informative tweets DataFrame sample is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bd3ae70be547418b983b586f83fbb63e",
            "4334c987639f4bfa8f38a1782dcfbfc7",
            "59a4c26b25ae456dbbdbcc4dec5aba0e",
            "a8d13208fe5d4d9f87c7abfefd6073bd",
            "b0fc2d5e3c404181a36b76df77cf606a",
            "1a47ec054872469dba90108cae9920b6",
            "12b1b9fa750e4744a87e62dfb3cfd78a",
            "72dd339b574941f7b6b8ceeaae84730f",
            "2d42f30d50fe4b1a89bbe850dc49d65a",
            "7a135c690a77472c8425bd312214d74a",
            "ecc3cd17d5ed494ba3fb2f44567ec764"
          ]
        },
        "id": "MuoxTF0vsNaz",
        "outputId": "5e3a8ea4-ea51-4fa5-ed59-5f78174001c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-fc1f6466666e>\", line 1, in <cell line: 0>\n",
            "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\", line 26, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\", line 25, in <module>\n",
            "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
            "    from torch import Tensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1382, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 7, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original informative tweets DataFrame loaded with 12855 entries.\n",
            "Working with a sample of 100 informative tweets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd3ae70be547418b983b586f83fbb63e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ground truth csv"
      ],
      "metadata": {
        "id": "klTm8QyDPoF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM3DFv59QRXx",
        "outputId": "4ac3c684-5cc4-4b88-9856-27e655dc8267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset.csv'"
      ],
      "metadata": {
        "id": "4MtjssV-QrLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Replace with the actual path to your friend's CSV file ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset.csv'\n",
        "\n",
        "try:\n",
        "    # Load the CSV file into a pandas DataFrame named 'ground_truth_df'\n",
        "    ground_truth_df = pd.read_csv(csv_path)\n",
        "    print(\"CSV file loaded successfully.\")\n",
        "    print(ground_truth_df.head()) # Display the first few rows\n",
        "    print(ground_truth_df.columns) # Display the column names\n",
        "\n",
        "    # --- Create the 'Wildfire' label based on the 'state' column ---\n",
        "    ground_truth_df['Wildfire'] = 'No'\n",
        "    ground_truth_df.loc[ground_truth_df['state'] == 'California', 'Wildfire'] = 'Yes'\n",
        "\n",
        "    print(\"\\n'Wildfire' label created.\")\n",
        "    print(ground_truth_df[['state', 'Wildfire']].head()) # Display first few rows with 'Wildfire' label\n",
        "    print(\"\\nDistribution of 'Wildfire' label:\")\n",
        "    print(ground_truth_df['Wildfire'].value_counts()) # Show counts of 'Yes' and 'No'\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N3vvKeIPnT6",
        "outputId": "51b80216-a414-4f06-b86e-cf24648e7684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file loaded successfully.\n",
            "       tweet_id                  image_id  \\\n",
            "0  9.177910e+17  917791044158185473_0.jpg   \n",
            "1  9.177911e+17  917791130590183424_0.jpg   \n",
            "2  9.177913e+17  917791291823591425_0.jpg   \n",
            "3  9.177913e+17  917791291823591425_1.jpg   \n",
            "4  9.177921e+17  917792092100988929_0.jpg   \n",
            "\n",
            "                                      raw_tweet_text  \\\n",
            "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
            "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
            "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "3  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "4  RT @TIME: California's raging wildfires as you...   \n",
            "\n",
            "                                          tweet_text tweet_hashtags  \\\n",
            "0  wildfires raging through northern california a...            NaN   \n",
            "1         photos deadly wildfires rage in california            NaN   \n",
            "2  pls share were capturing wildfire response rec...            NaN   \n",
            "3  pls share were capturing wildfire response rec...            NaN   \n",
            "4  californias raging wildfires as youve never se...            NaN   \n",
            "\n",
            "                                       image_caption  distress take_action  \\\n",
            "0  a fire is seen burning through the trees in th...         0         NaN   \n",
            "1  two people are standing in a burned area with ...         0         NaN   \n",
            "2         a fire burns in a burned area near a house         0         NaN   \n",
            "3  several people standing in front of a screen w...         0         NaN   \n",
            "4  a night sky with a mountain and a milky in the...         0         NaN   \n",
            "\n",
            "        state sub_location  \n",
            "0  California     northern  \n",
            "1  California          NaN  \n",
            "2         NaN          NaN  \n",
            "3         NaN          NaN  \n",
            "4  California          NaN  \n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location'],\n",
            "      dtype='object')\n",
            "\n",
            "'Wildfire' label created.\n",
            "        state Wildfire\n",
            "0  California      Yes\n",
            "1  California      Yes\n",
            "2         NaN       No\n",
            "3         NaN       No\n",
            "4  California      Yes\n",
            "\n",
            "Distribution of 'Wildfire' label:\n",
            "Wildfire\n",
            "No     16730\n",
            "Yes     1352\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a new CSV file\n",
        "output_csv_path = '/content/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"\\nDataFrame with 'Wildfire' label saved to: {output_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYBP7I1PnWN",
        "outputId": "0066752a-09e6-4177-b3fa-5f4776d0626c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame with 'Wildfire' label saved to: /content/ground_truth_dataset_with_wildfire.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with the state as the primary location. Since your dataset is focused on California wildfires, we know the state is relevant.\n",
        "\n",
        "If sub_location is available (not NaN), append it to the state with a separator (e.g., \", \"). This will provide more specific location details when they exist.\n",
        "\n",
        "If sub_location is missing (NaN), just use \"California\" as the location."
      ],
      "metadata": {
        "id": "GlSM7S1IUeGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new 'Location' column\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',  # Handle cases where state might be missing (though unlikely here)\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n'Location' column created.\")\n",
        "print(ground_truth_df[['state', 'sub_location', 'Location']].head(10)) # Display first 10 rows\n",
        "print(\"\\nValue counts for 'Location':\")\n",
        "print(ground_truth_df['Location'].value_counts().head(20)) # Show top 20 locations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Toi_ktC9PnaE",
        "outputId": "7d844b5e-7448-47be-cefb-a4cfd9552e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Location' column created.\n",
            "        state sub_location               Location\n",
            "0  California     northern   California, northern\n",
            "1  California          NaN             California\n",
            "2         NaN          NaN  No location mentioned\n",
            "3         NaN          NaN  No location mentioned\n",
            "4  California          NaN             California\n",
            "5  California          NaN             California\n",
            "6  California    wildfires  California, wildfires\n",
            "7  California          NaN             California\n",
            "8  California          NaN             California\n",
            "9  California          NaN             California\n",
            "\n",
            "Value counts for 'Location':\n",
            "Location\n",
            "No location mentioned                            16730\n",
            "California                                         966\n",
            "California, northern                                88\n",
            "California, southern                                15\n",
            "California, santa rosa                               9\n",
            "California, napa                                     8\n",
            "California, wildfires                                8\n",
            "California, sonoma northern                          5\n",
            "California, reuters                                  5\n",
            "California, mexico                                   5\n",
            "California, oak grove                                4\n",
            "California, sonoma                                   4\n",
            "California, disneyland                               4\n",
            "California, northern spain portugal                  4\n",
            "California, santa cruz cowlitz clark counties        4\n",
            "California, obamanation s forests                    4\n",
            "California, oak grove oakgrove                       4\n",
            "California, the redwood forest                       3\n",
            "California, puerto rico                              3\n",
            "California, yosemite                                 3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDP6J3eEVaPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "    # Add more keywords and responders as you analyze your 'take_action' data\n",
        "}"
      ],
      "metadata": {
        "id": "D5h5ODI2Pnci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()  # Convert to string and lowercase for matching\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "\n",
        "print(\"\\n'Responders (Suggested)' column created.\")\n",
        "print(ground_truth_df[['distress', 'take_action', 'Responders (Suggested)']].head(20))\n",
        "print(\"\\nValue counts for 'Responders (Suggested)':\")\n",
        "print(ground_truth_df['Responders (Suggested)'].value_counts().head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4H07hutVcMG",
        "outputId": "1481ce39-2297-4186-df86-5e9a8838886c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Responders (Suggested)' column created.\n",
            "    distress                          take_action  \\\n",
            "0          0                                  NaN   \n",
            "1          0                                  NaN   \n",
            "2          0                                  NaN   \n",
            "3          0                                  NaN   \n",
            "4          0                                  NaN   \n",
            "5          0                                  NaN   \n",
            "6          1  send evacuation and shelter support   \n",
            "7          0                                  NaN   \n",
            "8          0                                  NaN   \n",
            "9          0                                  NaN   \n",
            "10         0                                  NaN   \n",
            "11         0                                  NaN   \n",
            "12         0                                  NaN   \n",
            "13         0                                  NaN   \n",
            "14         1          start missing person search   \n",
            "15         0                                  NaN   \n",
            "16         1          start missing person search   \n",
            "17         0                                  NaN   \n",
            "18         0                                  NaN   \n",
            "19         0                                  NaN   \n",
            "\n",
            "                      Responders (Suggested)  \n",
            "0                             Not applicable  \n",
            "1                             Not applicable  \n",
            "2                             Not applicable  \n",
            "3                             Not applicable  \n",
            "4                             Not applicable  \n",
            "5                             Not applicable  \n",
            "6            Red Cross, Emergency Management  \n",
            "7                             Not applicable  \n",
            "8                             Not applicable  \n",
            "9                             Not applicable  \n",
            "10                            Not applicable  \n",
            "11                            Not applicable  \n",
            "12                            Not applicable  \n",
            "13                            Not applicable  \n",
            "14  Search and Rescue Teams, Law Enforcement  \n",
            "15                            Not applicable  \n",
            "16  Search and Rescue Teams, Law Enforcement  \n",
            "17                            Not applicable  \n",
            "18                            Not applicable  \n",
            "19                            Not applicable  \n",
            "\n",
            "Value counts for 'Responders (Suggested)':\n",
            "Responders (Suggested)\n",
            "Not applicable                              16100\n",
            "Local Authorities, Emergency Services        1548\n",
            "Search and Rescue Teams, Law Enforcement      149\n",
            "Red Cross, Emergency Management               128\n",
            "Search and Rescue Teams, Fire Department       56\n",
            "Responders unclear                             53\n",
            "Emergency Medical Services                     31\n",
            "General Emergency Services                     17\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inspection_sample = ground_truth_df.sample(frac=0.15, random_state=42) # Adjust fraction as needed\n",
        "print(f\"Generated a sample of {len(inspection_sample)} rows for manual inspection.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZTlUnHTVcOL",
        "outputId": "06aed728-6356-410d-f898-bbbb6e0de37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated a sample of 2712 rows for manual inspection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inspection_subset = inspection_sample[['raw_tweet_text', 'state', 'sub_location', 'Wildfire', 'distress', 'Location', 'take_action', 'Responders (Suggested)']]\n",
        "print(inspection_subset.head(20)) # Display the first 20 rows of the sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwuufst3VcSM",
        "outputId": "c38d91e3-0288-40e6-f82a-480177d729a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          raw_tweet_text       state  \\\n",
            "11482  . #Maria is now a weak and ragged looking Cat-...         NaN   \n",
            "13112  Puerto Rico governor: I answered Trump... http...         NaN   \n",
            "2501   RT @MPrendergastTX: Buffalo Bayou in Houston. ...         NaN   \n",
            "322    Company Helps Coordinate Air Attack On Califor...  California   \n",
            "8422   4th hr's back &amp; louder than Irma #whatifIr...         NaN   \n",
            "15453  Turkish Red Crescent cooperates with Iraqi Red...         NaN   \n",
            "11923  #PuertoRico has suffered immense devastation f...         NaN   \n",
            "7695   BuzzFeed : This Florida county used an interpr...     Florida   \n",
            "15472  President Dr. Kerem Kinik in Darbendixan distr...         NaN   \n",
            "7945   .@SecretarySonny Perdue, @marcorubio and @TomR...     Florida   \n",
            "290    Fire chief: We got outrun by the fires https:/...         NaN   \n",
            "12303  Hurricane Maria Not Getting Same Amount of Cov...         NaN   \n",
            "11326  This morning's update on #hurricanemaria - rem...         NaN   \n",
            "15929  Rescue efforts underway after second deadly Me...         NaN   \n",
            "9736   Adecco is in our office NOW! Stop by and find ...         NaN   \n",
            "1162   Baptism by fire for California's potÂ farmers ...  California   \n",
            "4706   Salvation Army &amp; Ingram Micro employees un...         NaN   \n",
            "4304   Out-of-towners helping with Harvey relief prob...       Texas   \n",
            "2003   RT @NavyTimes: Drones, disaster teams helping ...         NaN   \n",
            "17582  Retweeted Kate Mora (@jedikat71): Can't have e...         NaN   \n",
            "\n",
            "              sub_location Wildfire  distress               Location  \\\n",
            "11482                  NaN       No         0  No location mentioned   \n",
            "13112          puerto rico       No         0  No location mentioned   \n",
            "2501                   NaN       No         0  No location mentioned   \n",
            "322                    NaN      Yes         1             California   \n",
            "8422                   NaN       No         0  No location mentioned   \n",
            "15453                 iraq       No         0  No location mentioned   \n",
            "11923           puertorico       No         1  No location mentioned   \n",
            "7695   this florida county       No         0  No location mentioned   \n",
            "15472                  NaN       No         0  No location mentioned   \n",
            "7945                   NaN       No         0  No location mentioned   \n",
            "290                 usnews       No         0  No location mentioned   \n",
            "12303                  NaN       No         0  No location mentioned   \n",
            "11326       hurricanemaria       No         0  No location mentioned   \n",
            "15929               mexico       No         0  No location mentioned   \n",
            "9736                   NaN       No         1  No location mentioned   \n",
            "1162                   NaN      Yes         0             California   \n",
            "4706                   NaN       No         0  No location mentioned   \n",
            "4304                 texas       No         1  No location mentioned   \n",
            "2003                   NaN       No         1  No location mentioned   \n",
            "17582                  NaN       No         0  No location mentioned   \n",
            "\n",
            "             take_action                 Responders (Suggested)  \n",
            "11482                NaN                         Not applicable  \n",
            "13112                NaN                         Not applicable  \n",
            "2501                 NaN                         Not applicable  \n",
            "322    monitor situation  Local Authorities, Emergency Services  \n",
            "8422                 NaN                         Not applicable  \n",
            "15453                NaN                         Not applicable  \n",
            "11923  monitor situation  Local Authorities, Emergency Services  \n",
            "7695                 NaN                         Not applicable  \n",
            "15472                NaN                         Not applicable  \n",
            "7945                 NaN                         Not applicable  \n",
            "290                  NaN                         Not applicable  \n",
            "12303                NaN                         Not applicable  \n",
            "11326                NaN                         Not applicable  \n",
            "15929                NaN                         Not applicable  \n",
            "9736   monitor situation  Local Authorities, Emergency Services  \n",
            "1162                 NaN                         Not applicable  \n",
            "4706                 NaN                         Not applicable  \n",
            "4304   monitor situation  Local Authorities, Emergency Services  \n",
            "2003   monitor situation  Local Authorities, Emergency Services  \n",
            "17582                NaN                         Not applicable  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*dataset* split"
      ],
      "metadata": {
        "id": "6VQW0tin70Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'  # Or the path where you saved it\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "print(\"Ground truth dataset loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YAdYyG4VcVY",
        "outputId": "89291024-c74c-4c46-94f0-e045477ada02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth dataset loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load the CSV file from the root directory ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "\n",
        "try:\n",
        "    ground_truth_df = pd.read_csv(csv_path)\n",
        "    print(\"Ground truth dataset loaded.\")\n",
        "\n",
        "    # --- Determine the counts of each combination of 'Wildfire' and 'distress' ---\n",
        "    label_counts = ground_truth_df.groupby(['Wildfire', 'distress']).size().reset_index(name='counts')\n",
        "    print(\"Counts of each label combination:\")\n",
        "    print(label_counts)\n",
        "\n",
        "    # --- Aim for roughly equal samples (up to 25) from each combination for the 100-sample split ---\n",
        "    sample_size_per_group = 25\n",
        "    sample_split = pd.DataFrame()\n",
        "    sampled_indices = []\n",
        "\n",
        "    for index, row in label_counts.iterrows():\n",
        "        wildfire_label = row['Wildfire']\n",
        "        distress_label = row['distress']\n",
        "        count = row['counts']\n",
        "\n",
        "        n_samples = min(count, sample_size_per_group)  # Take up to 25, or fewer if the group is smaller\n",
        "        group_sample = ground_truth_df[\n",
        "            (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "        ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "        sample_split = pd.concat([sample_split, group_sample])\n",
        "        sampled_indices.extend(group_sample.index)\n",
        "\n",
        "    print(f\"\\nSize of the Sample Split: {len(sample_split)}\")\n",
        "    print(\"\\nDistribution of labels in the Sample Split:\")\n",
        "    print(sample_split.groupby(['Wildfire', 'distress']).size())\n",
        "\n",
        "    # --- Create the remaining DataFrame by removing the sampled rows ---\n",
        "    remaining_df = ground_truth_df.drop(sampled_indices)\n",
        "    print(f\"\\nSize of the Remaining DataFrame: {len(remaining_df)}\")\n",
        "\n",
        "    # Now 'sample_split' contains your 100-sample (or close to it, balanced) held-out set\n",
        "    # and 'remaining_df' contains the data for the 80/10/10 split.\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at /content/ground-truth_dataset_with_wildfire.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9WMhcfC7zH6",
        "outputId": "7bd56b08-097d-493d-e368-11d5c91df758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth dataset loaded.\n",
            "Counts of each label combination:\n",
            "  Wildfire  distress  counts\n",
            "0       No         0   14896\n",
            "1       No         1    1834\n",
            "2      Yes         0    1204\n",
            "3      Yes         1     148\n",
            "\n",
            "Size of the Sample Split: 100\n",
            "\n",
            "Distribution of labels in the Sample Split:\n",
            "Wildfire  distress\n",
            "No        0           25\n",
            "          1           25\n",
            "Yes       0           25\n",
            "          1           25\n",
            "dtype: int64\n",
            "\n",
            "Size of the Remaining DataFrame: 17982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Split remaining_df into training (80%) and a temporary set (20%) ---\n",
        "train_df, temp_df = train_test_split(\n",
        "    remaining_df,\n",
        "    test_size=0.2,\n",
        "    stratify=remaining_df[['Wildfire', 'distress']],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- Split the temporary set (20%) into validation (10%) and testing (10%) ---\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,  # 50% of the temp_df is 10% of the original\n",
        "    stratify=temp_df[['Wildfire', 'distress']],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Size of Training Set: {len(train_df)}\")\n",
        "print(f\"Size of Validation Set: {len(val_df)}\")\n",
        "print(f\"Size of Testing Set: {len(test_df)}\")\n",
        "\n",
        "print(\"\\nDistribution of labels in Training Set:\")\n",
        "print(train_df.groupby(['Wildfire', 'distress']).size() / len(train_df))\n",
        "\n",
        "print(\"\\nDistribution of labels in Validation Set:\")\n",
        "print(val_df.groupby(['Wildfire', 'distress']).size() / len(val_df))\n",
        "\n",
        "print(\"\\nDistribution of labels in Testing Set:\")\n",
        "print(test_df.groupby(['Wildfire', 'distress']).size() / len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO4kDYmM7zLl",
        "outputId": "1a13b784-f899-4eaa-d4f8-e7508a4960aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Training Set: 14385\n",
            "Size of Validation Set: 1798\n",
            "Size of Testing Set: 1799\n",
            "\n",
            "Distribution of labels in Training Set:\n",
            "Wildfire  distress\n",
            "No        0           0.826973\n",
            "          1           0.100591\n",
            "Yes       0           0.065554\n",
            "          1           0.006882\n",
            "dtype: float64\n",
            "\n",
            "Distribution of labels in Validation Set:\n",
            "Wildfire  distress\n",
            "No        0           0.827030\n",
            "          1           0.100667\n",
            "Yes       0           0.065628\n",
            "          1           0.006674\n",
            "dtype: float64\n",
            "\n",
            "Distribution of labels in Testing Set:\n",
            "Wildfire  distress\n",
            "No        0           0.827126\n",
            "          1           0.100611\n",
            "Yes       0           0.065592\n",
            "          1           0.006670\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "t5-small"
      ],
      "metadata": {
        "id": "-bokFFMNCtop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# --- Load base T5 small model and tokenizer ---\n",
        "model_name_base = \"t5-small\"\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(model_name_base)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name_base).to(device)\n",
        "\n",
        "print(f\"Base model and tokenizer for {model_name_base} loaded on {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyLD1VHD7zPS",
        "outputId": "42b47819-8d24-4444-c8c5-d0dce4ccdaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model and tokenizer for t5-small loaded on cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare prompts for the sample split ---\n",
        "prompts = []\n",
        "sample_tweets = sample_split['tweet_text'].tolist()\n",
        "\n",
        "for tweet in sample_tweets:\n",
        "    prompts.append(f\"Is this tweet about a California wildfire? Tweet: {tweet}\")\n",
        "    prompts.append(f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What location is mentioned in this tweet? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What action and responders are needed based on this tweet? Tweet: {tweet}\")\n",
        "\n",
        "# --- Tokenize the prompts ---\n",
        "inputs = tokenizer_base.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model_base.device)\n",
        "\n",
        "# --- Generate predictions ---\n",
        "with torch.no_grad():\n",
        "    outputs = model_base.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# --- Decode the predictions ---\n",
        "predictions = tokenizer_base.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# --- Display the prompts and predictions for the first few examples ---\n",
        "num_examples = 5\n",
        "for i in range(num_examples):\n",
        "    tweet_index = i // 4\n",
        "    question_index = i % 4\n",
        "    question = [\"Wildfire?\", \"Distress?\", \"Location?\", \"Action/Responders?\"][question_index]\n",
        "    print(f\"Tweet: {sample_tweets[tweet_index][:50]}...\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbSPr21nAvE3",
        "outputId": "dce0a705-1ecc-45ea-d56f-299ac546cc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Wildfire?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Distress?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery? Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Location?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Question: Action/Responders?\n",
            "Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: hurricane maria moves north lee still far from lan...\n",
            "Question: Wildfire?\n",
            "Prediction: Tweet: hurricane maria moves north lee still far from land.\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare prompts and generate predictions for a small sample of tweets ---\n",
        "num_tweets_to_examine = 3  # You can change this number\n",
        "sample_tweets = sample_split['tweet_text'].tolist()[:num_tweets_to_examine] # Take the first N tweets\n",
        "\n",
        "all_predictions = []\n",
        "all_prompts = []\n",
        "original_tweets = []\n",
        "\n",
        "for tweet in sample_tweets:\n",
        "    original_tweets.append(tweet)\n",
        "    prompts = [\n",
        "        f\"Is this tweet about a California wildfire? Tweet: {tweet}\",\n",
        "        f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\",\n",
        "        f\"What location is mentioned in this tweet? Tweet: {tweet}\",\n",
        "        f\"What action and responders are needed based on this tweet? Tweet: {tweet}\"\n",
        "    ]\n",
        "    all_prompts.extend(prompts)\n",
        "\n",
        "    inputs = tokenizer_base.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model_base.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_base.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "    predictions = tokenizer_base.batch_decode(outputs, skip_special_tokens=True)\n",
        "    all_predictions.extend(predictions)\n",
        "\n",
        "# --- Display the prompts and predictions ---\n",
        "for i in range(len(original_tweets)):\n",
        "    tweet = original_tweets[i]\n",
        "    print(f\"Tweet: {tweet[:50]}...\")\n",
        "    for j in range(4):\n",
        "        question = [\"Wildfire?\", \"Distress?\", \"Location?\", \"Action/Responders?\"][j]\n",
        "        prediction = all_predictions[i * 4 + j]\n",
        "        print(f\"  Question: {question}\")\n",
        "        print(f\"  Prediction: {prediction}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Slk-tPAvI3",
        "outputId": "d59c18a8-3ba7-45b8-a0e4-9d47a83b533c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "  Question: Wildfire?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "  Question: Distress?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery? Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "  Question: Location?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "  Question: Action/Responders?\n",
            "  Prediction: Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.\n",
            "------------------------------\n",
            "Tweet: hurricane maria moves north lee still far from lan...\n",
            "  Question: Wildfire?\n",
            "  Prediction: Tweet: hurricane maria moves north lee still far from land.\n",
            "  Question: Distress?\n",
            "  Prediction: Tweet:\n",
            "  Question: Location?\n",
            "  Prediction: Hurricane maria moves north lee still far from land\n",
            "  Question: Action/Responders?\n",
            "  Prediction: Tweet: hurricane maria moves north lee still far from land.\n",
            "------------------------------\n",
            "Tweet: this hurricane is targeting all the rich folks wit...\n",
            "  Question: Wildfire?\n",
            "  Prediction: Tweet: this hurricane is targeting all the rich folks witchcraft things #irmahurricane #irmahurricane #irmahurricane #irmahurricane\n",
            "  Question: Distress?\n",
            "  Prediction: Tweet: This hurricane is targeting all the rich folks witchcraft things #irmahurricane #irma\n",
            "  Question: Location?\n",
            "  Prediction: Tweet: this hurricane is targeting all the rich folks witchcraft things #irmahurricane #irmahurricane #irmahurricane #irmahurricane\n",
            "  Question: Action/Responders?\n",
            "  Prediction: ? Tweet: this hurricane is targeting all the rich folks witchcraft things #irmahurricane #irmahurricane #irmahurricane #irmahurric\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new 'Location' column\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',  # Handle cases where state might be missing (though unlikely here)\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n'Location' column created.\")\n",
        "print(ground_truth_df[['state', 'sub_location', 'Location']].head(10))\n",
        "print(\"\\nValue counts for 'Location':\")\n",
        "print(ground_truth_df['Location'].value_counts().head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdD3bncxEyVB",
        "outputId": "b30d171b-c250-44a4-a718-0fb8fb5acb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Location' column created.\n",
            "        state sub_location               Location\n",
            "0  California     northern   California, northern\n",
            "1  California          NaN             California\n",
            "2         NaN          NaN  No location mentioned\n",
            "3         NaN          NaN  No location mentioned\n",
            "4  California          NaN             California\n",
            "5  California          NaN             California\n",
            "6  California    wildfires  California, wildfires\n",
            "7  California          NaN             California\n",
            "8  California          NaN             California\n",
            "9  California          NaN             California\n",
            "\n",
            "Value counts for 'Location':\n",
            "Location\n",
            "No location mentioned                            16730\n",
            "California                                         966\n",
            "California, northern                                88\n",
            "California, southern                                15\n",
            "California, santa rosa                               9\n",
            "California, napa                                     8\n",
            "California, wildfires                                8\n",
            "California, sonoma northern                          5\n",
            "California, reuters                                  5\n",
            "California, mexico                                   5\n",
            "California, oak grove                                4\n",
            "California, sonoma                                   4\n",
            "California, disneyland                               4\n",
            "California, northern spain portugal                  4\n",
            "California, santa cruz cowlitz clark counties        4\n",
            "California, obamanation s forests                    4\n",
            "California, oak grove oakgrove                       4\n",
            "California, the redwood forest                       3\n",
            "California, puerto rico                              3\n",
            "California, yosemite                                 3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in ground_truth_df after creating 'Location':\")\n",
        "print(ground_truth_df.columns)\n",
        "print(\"\\nColumns in sample_split:\")\n",
        "print(sample_split.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFBVkgDHFIbB",
        "outputId": "5503b97f-c767-4197-d296-7b75fc2bb656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in ground_truth_df after creating 'Location':\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire', 'Location'],\n",
            "      dtype='object')\n",
            "\n",
            "Columns in sample_split:\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Aim for roughly equal samples (up to 25) from each combination for the 100-sample split ---\n",
        "sample_size_per_group = 25\n",
        "sample_split = pd.DataFrame()\n",
        "sampled_indices = []\n",
        "\n",
        "for index, row in label_counts.iterrows():\n",
        "    wildfire_label = row['Wildfire']\n",
        "    distress_label = row['distress']\n",
        "    count = row['counts']\n",
        "\n",
        "    n_samples = min(count, sample_size_per_group)  # Take up to 25, or fewer if the group is smaller\n",
        "    group_sample = ground_truth_df[\n",
        "        (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "    ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "    sample_split = pd.concat([sample_split, group_sample])\n",
        "    sampled_indices.extend(group_sample.index)\n",
        "\n",
        "print(f\"\\nSize of the Sample Split: {len(sample_split)}\")\n",
        "print(\"\\nDistribution of labels in the Sample Split:\")\n",
        "print(sample_split.groupby(['Wildfire', 'distress']).size())\n",
        "\n",
        "# --- Create the remaining DataFrame by removing the sampled rows ---\n",
        "remaining_df = ground_truth_df.drop(sampled_indices)\n",
        "print(f\"\\nSize of the Remaining DataFrame: {len(remaining_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4xgH5sFR9k",
        "outputId": "a6b5604c-086b-4f28-b84e-b194c56d97a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Size of the Sample Split: 100\n",
            "\n",
            "Distribution of labels in the Sample Split:\n",
            "Wildfire  distress\n",
            "No        0           25\n",
            "          1           25\n",
            "Yes       0           25\n",
            "          1           25\n",
            "dtype: int64\n",
            "\n",
            "Size of the Remaining DataFrame: 17982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "    # Add more keywords and responders as you analyze your 'take_action' data\n",
        "}\n",
        "\n",
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()  # Convert to string and lowercase for matching\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "\n",
        "print(\"\\n'Responders (Suggested)' column created.\")\n",
        "print(ground_truth_df[['distress', 'take_action', 'Responders (Suggested)']].head(20))\n",
        "print(\"\\nValue counts for 'Responders (Suggested)':\")\n",
        "print(ground_truth_df['Responders (Suggested)'].value_counts().head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7XTLLATFlLx",
        "outputId": "d7772361-4b00-4bc5-db9d-42a704f86a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'Responders (Suggested)' column created.\n",
            "    distress                          take_action  \\\n",
            "0          0                                  NaN   \n",
            "1          0                                  NaN   \n",
            "2          0                                  NaN   \n",
            "3          0                                  NaN   \n",
            "4          0                                  NaN   \n",
            "5          0                                  NaN   \n",
            "6          1  send evacuation and shelter support   \n",
            "7          0                                  NaN   \n",
            "8          0                                  NaN   \n",
            "9          0                                  NaN   \n",
            "10         0                                  NaN   \n",
            "11         0                                  NaN   \n",
            "12         0                                  NaN   \n",
            "13         0                                  NaN   \n",
            "14         1          start missing person search   \n",
            "15         0                                  NaN   \n",
            "16         1          start missing person search   \n",
            "17         0                                  NaN   \n",
            "18         0                                  NaN   \n",
            "19         0                                  NaN   \n",
            "\n",
            "                      Responders (Suggested)  \n",
            "0                             Not applicable  \n",
            "1                             Not applicable  \n",
            "2                             Not applicable  \n",
            "3                             Not applicable  \n",
            "4                             Not applicable  \n",
            "5                             Not applicable  \n",
            "6            Red Cross, Emergency Management  \n",
            "7                             Not applicable  \n",
            "8                             Not applicable  \n",
            "9                             Not applicable  \n",
            "10                            Not applicable  \n",
            "11                            Not applicable  \n",
            "12                            Not applicable  \n",
            "13                            Not applicable  \n",
            "14  Search and Rescue Teams, Law Enforcement  \n",
            "15                            Not applicable  \n",
            "16  Search and Rescue Teams, Law Enforcement  \n",
            "17                            Not applicable  \n",
            "18                            Not applicable  \n",
            "19                            Not applicable  \n",
            "\n",
            "Value counts for 'Responders (Suggested)':\n",
            "Responders (Suggested)\n",
            "Not applicable                              16100\n",
            "Local Authorities, Emergency Services        1548\n",
            "Search and Rescue Teams, Law Enforcement      149\n",
            "Red Cross, Emergency Management               128\n",
            "Search and Rescue Teams, Fire Department       56\n",
            "Responders unclear                             53\n",
            "Emergency Medical Services                     31\n",
            "General Emergency Services                     17\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Step 1: Load Data ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "# --- Step 2: Create 'Wildfire' Column ---\n",
        "ground_truth_df['Wildfire'] = 'No'\n",
        "ground_truth_df.loc[ground_truth_df['state'] == 'California', 'Wildfire'] = 'Yes'\n",
        "print(\"'Wildfire' column created.\")\n",
        "\n",
        "# --- Step 3: Create 'Location' Column ---\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',\n",
        "    axis=1\n",
        ")\n",
        "print(\"'Location' column created.\")\n",
        "\n",
        "# --- Step 4: Create 'Responders (Suggested)' Column ---\n",
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "}\n",
        "\n",
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "print(\"'Responders (Suggested)' column created.\")\n",
        "\n",
        "# --- Step 5: Create sample_split ---\n",
        "label_counts = ground_truth_df.groupby(['Wildfire', 'distress']).size().reset_index(name='counts')\n",
        "\n",
        "sample_size_per_group = 25\n",
        "sample_split = pd.DataFrame()\n",
        "sampled_indices = []\n",
        "\n",
        "for index, row in label_counts.iterrows():\n",
        "    wildfire_label = row['Wildfire']\n",
        "    distress_label = row['distress']\n",
        "    count = row['counts']\n",
        "\n",
        "    n_samples = min(count, sample_size_per_group)\n",
        "    group_sample = ground_truth_df[\n",
        "        (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "    ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "    sample_split = pd.concat([sample_split, group_sample])\n",
        "    sampled_indices.extend(group_sample.index)\n",
        "\n",
        "print(\"sample_split created.\")\n",
        "\n",
        "# --- Step 6: Check Columns of sample_split ---\n",
        "print(\"Columns in sample_split after creation:\")\n",
        "print(sample_split.columns)\n",
        "\n",
        "# --- Step 7: Prepare Ground Truth for Evaluation ---\n",
        "ground_truth_wildfire = sample_split['Wildfire'].tolist()\n",
        "ground_truth_distress = sample_split['distress'].tolist()\n",
        "ground_truth_location = sample_split['Location'].tolist()\n",
        "ground_truth_action = sample_split['take_action'].tolist()\n",
        "ground_truth_responders = sample_split['Responders (Suggested)'].tolist()\n",
        "print(\"Ground truth prepared for evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pwYTZohAvM_",
        "outputId": "18e20eac-0547-489a-c233-c6dc362af8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded.\n",
            "'Wildfire' column created.\n",
            "'Location' column created.\n",
            "'Responders (Suggested)' column created.\n",
            "sample_split created.\n",
            "Columns in sample_split after creation:\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire', 'Location', 'Responders (Suggested)'],\n",
            "      dtype='object')\n",
            "Ground truth prepared for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare prompts for the entire sample split ---\n",
        "prompts = []\n",
        "sample_tweets = sample_split['tweet_text'].tolist()\n",
        "\n",
        "for tweet in sample_tweets:\n",
        "    prompts.append(f\"Is this tweet about a California wildfire? Tweet: {tweet}\")\n",
        "    prompts.append(f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What location is mentioned in this tweet? Tweet: {tweet}\")\n",
        "    prompts.append(f\"What action and responders are needed based on this tweet? Tweet: {tweet}\")\n",
        "\n",
        "# --- Tokenize and generate predictions for the entire sample split ---\n",
        "inputs = tokenizer_base.batch_encode_plus(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model_base.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model_base.generate(**inputs, max_length=50, num_return_sequences=1)\n",
        "\n",
        "predictions = tokenizer_base.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# --- Reshape predictions to align with the four questions per tweet ---\n",
        "reshaped_predictions = [predictions[i:i + 4] for i in range(0, len(predictions), 4)]\n",
        "\n",
        "print(\"Predictions generated for the entire Sample Split.\")\n",
        "print(f\"Number of tweets in Sample Split: {len(sample_tweets)}\")\n",
        "print(f\"Number of sets of predictions: {len(reshaped_predictions)}\")\n",
        "print(\"First example:\")\n",
        "print(f\"Tweet: {sample_tweets[0][:50]}...\")\n",
        "print(f\"Predictions: {reshaped_predictions[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Pt1dR2GVVT",
        "outputId": "f40b0741-5abb-42d5-dc2b-55f7e8ff7914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions generated for the entire Sample Split.\n",
            "Number of tweets in Sample Split: 100\n",
            "Number of sets of predictions: 100\n",
            "First example:\n",
            "Tweet: chamillionaire starts the robins heart foundation ...\n",
            "Predictions: ['Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.', 'Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery? Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.', 'Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.', 'Tweet: chamillionaire starts the robins heart foundation to assist with harvey recovery.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# --- Map T5 predictions to Yes/No for Wildfire ---\n",
        "predicted_wildfire = []\n",
        "for prediction in reshaped_predictions:\n",
        "    if any(keyword in prediction[0].lower() for keyword in [\"yes\", \"it is\", \"wildfire\", \"fire\", \"burn\"]):\n",
        "        predicted_wildfire.append(\"Yes\")\n",
        "    else:\n",
        "        predicted_wildfire.append(\"No\")\n",
        "\n",
        "# --- Map T5 predictions to Yes/No for Distress ---\n",
        "predicted_distress = []\n",
        "for prediction in reshaped_predictions:\n",
        "    if any(keyword in prediction[1].lower() for keyword in [\"help\", \"urgent\", \"emergency\", \"need\", \"assistance\", \"critical\", \"danger\"]):\n",
        "        predicted_distress.append(\"Yes\")\n",
        "    else:\n",
        "        predicted_distress.append(\"No\")\n",
        "\n",
        "# --- Evaluate Wildfire detection ---\n",
        "wildfire_accuracy = accuracy_score(ground_truth_wildfire, predicted_wildfire)\n",
        "wildfire_f1 = f1_score(\n",
        "    [1 if label == \"Yes\" else 0 for label in ground_truth_wildfire],\n",
        "    [1 if label == \"Yes\" else 0 for label in predicted_wildfire]\n",
        ")\n",
        "\n",
        "print(f\"Wildfire Detection Accuracy: {wildfire_accuracy:.4f}\")\n",
        "print(f\"Wildfire Detection F1 Score: {wildfire_f1:.4f}\")\n",
        "\n",
        "# --- Evaluate Distress detection ---\n",
        "# Note: ground_truth_distress is 0 or 1, so we map predicted_distress accordingly\n",
        "distress_accuracy = accuracy_score(\n",
        "    ground_truth_distress, [1 if label == \"Yes\" else 0 for label in predicted_distress]\n",
        ")\n",
        "distress_f1 = f1_score(ground_truth_distress, [1 if label == \"Yes\" else 0 for label in predicted_distress])\n",
        "\n",
        "print(f\"Distress Detection Accuracy: {distress_accuracy:.4f}\")\n",
        "print(f\"Distress Detection F1 Score: {distress_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3RVwt1PGVZd",
        "outputId": "8a8b583e-2054-4480-ee67-dd1b36ba3b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wildfire Detection Accuracy: 0.9000\n",
            "Wildfire Detection F1 Score: 0.9057\n",
            "Distress Detection Accuracy: 0.6400\n",
            "Distress Detection F1 Score: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import inspect\n",
        "\n",
        "print(\"Contents of nltk.metrics:\")\n",
        "print(inspect.getmembers(nltk.metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMWXjAdbPnhI",
        "outputId": "c70567ef-f728-4fdb-e780-1955737c82c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of nltk.metrics:\n",
            "[('__builtins__', {'__name__': 'builtins', '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7be62cc2f590>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'aiter': <built-in function aiter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'anext': <built-in function anext>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'BaseExceptionGroup': <class 'BaseExceptionGroup'>, 'Exception': <class 'Exception'>, 'GeneratorExit': <class 'GeneratorExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'SystemExit': <class 'SystemExit'>, 'ArithmeticError': <class 'ArithmeticError'>, 'AssertionError': <class 'AssertionError'>, 'AttributeError': <class 'AttributeError'>, 'BufferError': <class 'BufferError'>, 'EOFError': <class 'EOFError'>, 'ImportError': <class 'ImportError'>, 'LookupError': <class 'LookupError'>, 'MemoryError': <class 'MemoryError'>, 'NameError': <class 'NameError'>, 'OSError': <class 'OSError'>, 'ReferenceError': <class 'ReferenceError'>, 'RuntimeError': <class 'RuntimeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'SyntaxError': <class 'SyntaxError'>, 'SystemError': <class 'SystemError'>, 'TypeError': <class 'TypeError'>, 'ValueError': <class 'ValueError'>, 'Warning': <class 'Warning'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'BytesWarning': <class 'BytesWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'EncodingWarning': <class 'EncodingWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'UserWarning': <class 'UserWarning'>, 'BlockingIOError': <class 'BlockingIOError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionError': <class 'ConnectionError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'InterruptedError': <class 'InterruptedError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'IndentationError': <class 'IndentationError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'RecursionError': <class 'RecursionError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'UnicodeError': <class 'UnicodeError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'TabError': <class 'TabError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'ExceptionGroup': <class 'ExceptionGroup'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 2000 BeOpen.com.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
            "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
            "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x7be6332b72e0>, 'runfile': <function runfile at 0x7be633160f40>, '__IPYTHON__': True, 'display': <function display at 0x7be6341dde40>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x7be62ccaa880>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x7be5da242ca0>, '__pybind11_internals_v4_clang_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7be4f58f6790>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7be62cc50910>>}), ('__cached__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/__pycache__/metrics.cpython-311.pyc'), ('__doc__', None), ('__file__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py'), ('__loader__', <_frozen_importlib_external.SourceFileLoader object at 0x7be3e724ed10>), ('__name__', 'nltk.translate.metrics'), ('__package__', 'nltk.translate'), ('__spec__', ModuleSpec(name='nltk.translate.metrics', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7be3e724ed10>, origin='/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py')), ('alignment_error_rate', <function alignment_error_rate at 0x7be3e72b22a0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import inspect\n",
        "\n",
        "print(\"Contents of nltk.translate.metrics:\")\n",
        "print(inspect.getmembers(nltk.translate.metrics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqr15j8QHIHk",
        "outputId": "4dffd585-84ee-4531-d7d8-d351b0b62245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of nltk.translate.metrics:\n",
            "[('__builtins__', {'__name__': 'builtins', '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7be62cc2f590>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'aiter': <built-in function aiter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'anext': <built-in function anext>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'BaseExceptionGroup': <class 'BaseExceptionGroup'>, 'Exception': <class 'Exception'>, 'GeneratorExit': <class 'GeneratorExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'SystemExit': <class 'SystemExit'>, 'ArithmeticError': <class 'ArithmeticError'>, 'AssertionError': <class 'AssertionError'>, 'AttributeError': <class 'AttributeError'>, 'BufferError': <class 'BufferError'>, 'EOFError': <class 'EOFError'>, 'ImportError': <class 'ImportError'>, 'LookupError': <class 'LookupError'>, 'MemoryError': <class 'MemoryError'>, 'NameError': <class 'NameError'>, 'OSError': <class 'OSError'>, 'ReferenceError': <class 'ReferenceError'>, 'RuntimeError': <class 'RuntimeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'SyntaxError': <class 'SyntaxError'>, 'SystemError': <class 'SystemError'>, 'TypeError': <class 'TypeError'>, 'ValueError': <class 'ValueError'>, 'Warning': <class 'Warning'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'BytesWarning': <class 'BytesWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'EncodingWarning': <class 'EncodingWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'UserWarning': <class 'UserWarning'>, 'BlockingIOError': <class 'BlockingIOError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionError': <class 'ConnectionError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'InterruptedError': <class 'InterruptedError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'IndentationError': <class 'IndentationError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'RecursionError': <class 'RecursionError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'UnicodeError': <class 'UnicodeError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'TabError': <class 'TabError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'ExceptionGroup': <class 'ExceptionGroup'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 2000 BeOpen.com.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
            "All Rights Reserved.\n",
            "\n",
            "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
            "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
            "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., 'execfile': <function execfile at 0x7be6332b72e0>, 'runfile': <function runfile at 0x7be633160f40>, '__IPYTHON__': True, 'display': <function display at 0x7be6341dde40>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1014__': <capsule object NULL at 0x7be62ccaa880>, '__pybind11_internals_v4_gcc_libstdcpp_cxxabi1011__': <capsule object NULL at 0x7be5da242ca0>, '__pybind11_internals_v4_clang_libstdcpp_cxxabi1002__': <capsule object NULL at 0x7be4f58f6790>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7be62cc50910>>}), ('__cached__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/__pycache__/metrics.cpython-311.pyc'), ('__doc__', None), ('__file__', '/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py'), ('__loader__', <_frozen_importlib_external.SourceFileLoader object at 0x7be3e724ed10>), ('__name__', 'nltk.translate.metrics'), ('__package__', 'nltk.translate'), ('__spec__', ModuleSpec(name='nltk.translate.metrics', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7be3e724ed10>, origin='/usr/local/lib/python3.11/dist-packages/nltk/translate/metrics.py')), ('alignment_error_rate', <function alignment_error_rate at 0x7be3e72b22a0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj42PI-HHIMW",
        "outputId": "2477d392-78be-4f59-fc27-adc98b1c38d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "# --- Evaluate Location Prediction ---\n",
        "location_bleu_scores = []\n",
        "location_rouge_scores = []\n",
        "\n",
        "for i in range(len(sample_tweets)):\n",
        "    reference = ground_truth_location[i].lower()\n",
        "    prediction = reshaped_predictions[i][2].lower()\n",
        "    if reference and prediction:  # Avoid errors with empty strings\n",
        "        reference_list = [reference.split()]\n",
        "        prediction_list = prediction.split()\n",
        "        bleu_score = sentence_bleu(reference_list, prediction_list)\n",
        "        scores = rouge.get_scores(prediction, reference)\n",
        "        location_bleu_scores.append(bleu_score)\n",
        "        if scores:\n",
        "            location_rouge_scores.append(scores[0])\n",
        "        else:\n",
        "            location_rouge_scores.append({'rouge-1': {'f': 0}, 'rouge-l': {'f': 0}}) # Handle cases with no scores\n",
        "\n",
        "avg_location_bleu = sum(location_bleu_scores) / len(location_bleu_scores) if location_bleu_scores else 0\n",
        "avg_location_rouge_1 = sum(score['rouge-1']['f'] for score in location_rouge_scores) / len(location_rouge_scores) if location_rouge_scores else 0\n",
        "avg_location_rouge_l = sum(score['rouge-l']['f'] for score in location_rouge_scores) / len(location_rouge_scores) if location_rouge_scores else 0\n",
        "\n",
        "print(f\"\\nAverage BLEU Score (Location): {avg_location_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score (Location): {avg_location_rouge_1:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score (Location): {avg_location_rouge_l:.4f}\")\n",
        "\n",
        "# --- Evaluate Action/Responders Prediction ---\n",
        "action_bleu_scores = []\n",
        "action_rouge_scores = []\n",
        "\n",
        "for i in range(len(sample_tweets)):\n",
        "    reference = (str(ground_truth_action[i]) + \" \" + str(ground_truth_responders[i])).lower()\n",
        "    prediction = reshaped_predictions[i][3].lower()\n",
        "    if reference and prediction:  # Avoid errors with empty strings\n",
        "        reference_list = [reference.split()]\n",
        "        prediction_list = prediction.split()\n",
        "        bleu_score = sentence_bleu(reference_list, prediction_list)\n",
        "        scores = rouge.get_scores(prediction, reference)\n",
        "        action_bleu_scores.append(bleu_score)\n",
        "        if scores:\n",
        "            action_rouge_scores.append(scores[0])\n",
        "        else:\n",
        "            action_rouge_scores.append({'rouge-1': {'f': 0}, 'rouge-l': {'f': 0}}) # Handle cases with no scores\n",
        "\n",
        "avg_action_bleu = sum(action_bleu_scores) / len(action_bleu_scores) if action_bleu_scores else 0\n",
        "avg_action_rouge_1 = sum(score['rouge-1']['f'] for score in action_rouge_scores) / len(action_rouge_scores) if action_rouge_scores else 0\n",
        "avg_action_rouge_l = sum(score['rouge-l']['f'] for score in action_rouge_scores) / len(action_rouge_scores) if action_rouge_scores else 0\n",
        "\n",
        "print(f\"\\nAverage BLEU Score (Action/Responders): {avg_action_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score (Action/Responders): {avg_action_rouge_1:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score (Action/Responders): {avg_action_rouge_l:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAqEPSPtIBh9",
        "outputId": "864788b6-7a5c-4ae1-eafc-c633aa448590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score (Location): 0.0000\n",
            "Average ROUGE-1 F1 Score (Location): 0.0732\n",
            "Average ROUGE-L F1 Score (Location): 0.0732\n",
            "\n",
            "Average BLEU Score (Action/Responders): 0.0000\n",
            "Average ROUGE-1 F1 Score (Action/Responders): 0.0130\n",
            "Average ROUGE-L F1 Score (Action/Responders): 0.0130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Phase 4: Instruction Fine-Tuning."
      ],
      "metadata": {
        "id": "MhmmGPMAImVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Step 1: Load Data ---\n",
        "csv_path = '/content/drive/MyDrive/ground_truth_dataset_with_wildfire.csv'\n",
        "ground_truth_df = pd.read_csv(csv_path)\n",
        "print(\"First few rows of ground_truth_df after loading:\")\n",
        "print(ground_truth_df.head())\n",
        "print(\"Data loaded.\")\n",
        "\n",
        "# --- Step 2: Create 'Wildfire' Column ---\n",
        "ground_truth_df['Wildfire'] = 'No'\n",
        "ground_truth_df.loc[ground_truth_df['state'] == 'California', 'Wildfire'] = 'Yes'\n",
        "print(\"'Wildfire' column created.\")\n",
        "\n",
        "# --- Step 3: Create 'Location' Column ---\n",
        "ground_truth_df['Location'] = ground_truth_df.apply(\n",
        "    lambda row: f\"{row['state']}, {row['sub_location']}\"\n",
        "    if pd.notna(row['sub_location']) and row['state'] == 'California'\n",
        "    else row['state'] if row['state'] == 'California'\n",
        "    else 'No location mentioned',\n",
        "    axis=1\n",
        ")\n",
        "print(\"'Location' column created.\")\n",
        "\n",
        "# --- Step 4: Create 'Responders (Suggested)' Column ---\n",
        "responder_mapping = {\n",
        "    'evacuate': 'Fire Department, Emergency Management',\n",
        "    'shelter': 'Red Cross, Emergency Management',\n",
        "    'rescue': 'Search and Rescue Teams, Fire Department',\n",
        "    'search': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'missing person': 'Search and Rescue Teams, Law Enforcement',\n",
        "    'medical': 'Emergency Medical Services',\n",
        "    'aid': 'Various Aid Organizations',\n",
        "    'help': 'General Emergency Services',\n",
        "    'fire': 'Fire Department',\n",
        "    'burn': 'Fire Department',\n",
        "    'monitor': 'Local Authorities, Emergency Services'\n",
        "}\n",
        "\n",
        "def suggest_responders(row):\n",
        "    if row['distress'] == 1:\n",
        "        action = str(row['take_action']).lower()\n",
        "        responders = set()\n",
        "        for keyword, suggested_responder in responder_mapping.items():\n",
        "            if keyword in action:\n",
        "                responders.add(suggested_responder)\n",
        "        if responders:\n",
        "            return \", \".join(responders)\n",
        "        else:\n",
        "            return \"Responders unclear\"\n",
        "    else:\n",
        "        return \"Not applicable\"\n",
        "\n",
        "ground_truth_df['Responders (Suggested)'] = ground_truth_df.apply(suggest_responders, axis=1)\n",
        "print(\"'Responders (Suggested)' column created.\")\n",
        "\n",
        "# --- Step 5: Create sample_split ---\n",
        "label_counts = ground_truth_df.groupby(['Wildfire', 'distress']).size().reset_index(name='counts')\n",
        "\n",
        "sample_size_per_group = 25\n",
        "sample_split = pd.DataFrame()\n",
        "sampled_indices = []\n",
        "\n",
        "for index, row in label_counts.iterrows():\n",
        "    wildfire_label = row['Wildfire']\n",
        "    distress_label = row['distress']\n",
        "    count = row['counts']\n",
        "\n",
        "    n_samples = min(count, sample_size_per_group)\n",
        "    group_sample = ground_truth_df[\n",
        "        (ground_truth_df['Wildfire'] == wildfire_label) & (ground_truth_df['distress'] == distress_label)\n",
        "    ].sample(n=n_samples, random_state=42)\n",
        "\n",
        "    sample_split = pd.concat([sample_split, group_sample])\n",
        "    sampled_indices.extend(group_sample.index)\n",
        "\n",
        "print(\"sample_split created.\")\n",
        "\n",
        "# --- Step 6: Check Columns of sample_split ---\n",
        "print(\"Columns in sample_split after creation:\")\n",
        "print(sample_split.columns)\n",
        "\n",
        "# --- Step 7: Prepare Ground Truth for Evaluation ---\n",
        "ground_truth_wildfire = sample_split['Wildfire'].tolist()\n",
        "ground_truth_distress = sample_split['distress'].tolist()\n",
        "ground_truth_location = sample_split['Location'].tolist()\n",
        "ground_truth_action = sample_split['take_action'].tolist()\n",
        "ground_truth_responders = sample_split['Responders (Suggested)'].tolist()\n",
        "print(\"Ground truth prepared for evaluation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R15MM3idJQ0X",
        "outputId": "738d5c8f-2f66-404c-e72d-b70c89cc7f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of ground_truth_df after loading:\n",
            "       tweet_id                  image_id  \\\n",
            "0  9.177910e+17  917791044158185473_0.jpg   \n",
            "1  9.177911e+17  917791130590183424_0.jpg   \n",
            "2  9.177913e+17  917791291823591425_0.jpg   \n",
            "3  9.177913e+17  917791291823591425_1.jpg   \n",
            "4  9.177921e+17  917792092100988929_0.jpg   \n",
            "\n",
            "                                      raw_tweet_text  \\\n",
            "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
            "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
            "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "3  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
            "4  RT @TIME: California's raging wildfires as you...   \n",
            "\n",
            "                                          tweet_text tweet_hashtags  \\\n",
            "0  wildfires raging through northern california a...            NaN   \n",
            "1         photos deadly wildfires rage in california            NaN   \n",
            "2  pls share were capturing wildfire response rec...            NaN   \n",
            "3  pls share were capturing wildfire response rec...            NaN   \n",
            "4  californias raging wildfires as youve never se...            NaN   \n",
            "\n",
            "                                       image_caption  distress take_action  \\\n",
            "0  a fire is seen burning through the trees in th...         0         NaN   \n",
            "1  two people are standing in a burned area with ...         0         NaN   \n",
            "2         a fire burns in a burned area near a house         0         NaN   \n",
            "3  several people standing in front of a screen w...         0         NaN   \n",
            "4  a night sky with a mountain and a milky in the...         0         NaN   \n",
            "\n",
            "        state sub_location Wildfire  \n",
            "0  California     northern      Yes  \n",
            "1  California          NaN      Yes  \n",
            "2         NaN          NaN       No  \n",
            "3         NaN          NaN       No  \n",
            "4  California          NaN      Yes  \n",
            "Data loaded.\n",
            "'Wildfire' column created.\n",
            "'Location' column created.\n",
            "'Responders (Suggested)' column created.\n",
            "sample_split created.\n",
            "Columns in sample_split after creation:\n",
            "Index(['tweet_id', 'image_id', 'raw_tweet_text', 'tweet_text',\n",
            "       'tweet_hashtags', 'image_caption', 'distress', 'take_action', 'state',\n",
            "       'sub_location', 'Wildfire', 'Location', 'Responders (Suggested)'],\n",
            "      dtype='object')\n",
            "Ground truth prepared for evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Split the data into training, validation, and test sets ---\n",
        "train_df, temp_df = train_test_split(ground_truth_df, test_size=0.2, random_state=42, stratify=ground_truth_df[['Wildfire', 'distress']])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[['Wildfire', 'distress']])\n",
        "\n",
        "print(f\"Size of training set: {len(train_df)}\")\n",
        "print(f\"Size of validation set: {len(val_df)}\")\n",
        "print(f\"Size of test set: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNjVWopKJ14r",
        "outputId": "cb800717-835d-4d85-8c56-fd51f9d3525c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: 14465\n",
            "Size of validation set: 1808\n",
            "Size of test set: 1809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: Prepare the training data from train_df ---\n",
        "train_data = []\n",
        "for index, row in train_df.iterrows():\n",
        "    tweet = row['tweet_text']\n",
        "    wildfire_answer = row['Wildfire']\n",
        "    distress_answer = 1 if row['distress'] == 1 else 0  # Keep as 0/1 for consistency\n",
        "    location_answer = row['Location']\n",
        "    action_responders_answer = f\"{row['take_action']} {row['Responders (Suggested)']}\"\n",
        "\n",
        "    train_data.append({\n",
        "        'prompt': f\"Is this tweet about a California wildfire? Tweet: {tweet}\",\n",
        "        'target': wildfire_answer\n",
        "    })\n",
        "    train_data.append({\n",
        "        'prompt': f\"Does this tweet indicate distress or emergency? Tweet: {tweet}\",\n",
        "        'target': str(distress_answer)  # Convert to string for text generation\n",
        "    })\n",
        "    train_data.append({\n",
        "        'prompt': f\"What location is mentioned in this tweet? Tweet: {tweet}\",\n",
        "        'target': location_answer\n",
        "    })\n",
        "    train_data.append({\n",
        "        'prompt': f\"What action and responders are needed based on this tweet? Tweet: {tweet}\",\n",
        "        'target': action_responders_answer\n",
        "    })\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(\"First training example:\")\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9NTNzrmJ-8F",
        "outputId": "079ca05f-fd3c-4ecf-ddb6-bba4a2b3161b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 57860\n",
            "First training example:\n",
            "{'prompt': 'Is this tweet about a California wildfire? Tweet: irma victims need our help they cant recover on their own #irmarecovery #irmavictims 9donate medical suppliesb', 'target': 'No'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama"
      ],
      "metadata": {
        "id": "uBCe60rMMBfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch peft accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cLsy7o7IBqL",
        "outputId": "807800aa-876d-4b88-bfcb-3801eadd1192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "-79OU83gNalH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "try:\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",  # Automatically put the model on available GPU(s)\n",
        "    )\n",
        "    print(\"LLaMA-2-7b loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading LLaMA-2-7b: {e}\")\n",
        "    print(\"Please ensure you have accepted the terms on Hugging Face and have a valid access token if required.\")\n",
        "    print(\"We might need to consider a different model or approach.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0ce2cbcd957c4e4eb14b1badb4d7d282",
            "3bcb9f9108ba4be08b2b280b5424a37f",
            "9addea728b2e4bd99878eabb1d48d686",
            "21c5598205634771903f7eeada1dd64f",
            "aee5cdd5bd2c4dd884e2a24cc911e70a",
            "491ace4357e54a94aa8eb67c69d2836c",
            "3c2fe62dfb8e4385a5f7de3887068446",
            "b4ae1bed0b5148a48357b91fa02f7ba0",
            "950b701975ce493294fa19bf2d4c2280",
            "6eb4db1934db42cc9623575c11921428",
            "8befbe9db35e42b695c6269183a3f81d"
          ]
        },
        "id": "UY46l6cYHIRK",
        "outputId": "c7f7db8d-782b-46b1-85e6-0b7906ac6043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ce2cbcd957c4e4eb14b1badb4d7d282"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLaMA-2-7b loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "print(f\"Padding token set to: {tokenizer_llama.pad_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXW3hjiXSjpC",
        "outputId": "63d049d1-ae64-4577-c2b3-0ee0ad58ce0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding token set to: </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_llama_data(data, tokenizer, max_length=512):\n",
        "    tokenized_inputs = []\n",
        "    for item in data:\n",
        "        prompt = item['prompt']\n",
        "        target = item['target']\n",
        "\n",
        "        prompt_encodings = tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encodings = tokenizer(\n",
        "            target,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        tokenized_inputs.append({\n",
        "            'input_ids': prompt_encodings['input_ids'][0],\n",
        "            'attention_mask': prompt_encodings['attention_mask'][0],\n",
        "            'labels': target_encodings['input_ids'][0],\n",
        "        })\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Prepare the training data\n",
        "processed_train_data_llama = prepare_llama_data(train_data, tokenizer_llama)\n",
        "\n",
        "print(f\"Number of processed training examples: {len(processed_train_data_llama)}\")\n",
        "print(\"First processed training example:\")\n",
        "print(processed_train_data_llama[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_C-Ml2XNFCT",
        "outputId": "d8bf2f06-5814-4070-adb6-d968c7947101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of processed training examples: 57860\n",
            "First processed training example:\n",
            "{'input_ids': tensor([    1,  1317,   445,  7780,   300,  1048,   263,  8046,  8775,  8696,\n",
            "        29973,   323, 16668, 29901,  3805,   655,  6879,  9893,   817,  1749,\n",
            "         1371,   896,  5107,  9792,   373,  1009,  1914,   396,  3568,   598,\n",
            "        11911, 29891,   396,  3568,   485,   919,  9893, 29871, 29929,  9176,\n",
            "          403, 16083, 28075, 29890,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "            2,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   1, 1939,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
            "           2,    2,    2,    2,    2,    2,    2,    2])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the size of the LLaMA 2 7B model, it's highly likely that we'll run into memory issues if we try to fine-tune the entire model on a standard Colab GPU. To address this, we'll use LoRA (Low-Rank Adaptation).\n",
        "\n",
        "What is LoRA?\n",
        "\n",
        "LoRA is a Parameter-Efficient Fine-Tuning (PEFT) technique that freezes the pre-trained model weights and adds a small number of new trainable layers (called \"adapters\"). These adapters are low-rank matrices, which means they have far fewer parameters than the original model. During fine-tuning, only these adapter weights are updated, significantly reducing the memory footprint and training time."
      ],
      "metadata": {
        "id": "RjuFHgN6TYyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,  # Rank of the LoRA matrices\n",
        "    lora_alpha=32,  # Scaling factor for LoRA weights\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"gate_proj\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Get the LoRA model\n",
        "model_lora = get_peft_model(model_llama, lora_config)\n",
        "model_lora.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61VqH7CENFGk",
        "outputId": "49e545f2-ced3-4b24-baac-f810dd6f7d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        llm_int8_enable_fp32_cpu_offload=False,\n",
        "    )\n",
        "\n",
        "    # Load the base model directly onto the GPU with quantization\n",
        "    model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map={\"\": device},  # Load directly to GPU\n",
        "    )\n",
        "    print(\"LLaMA-2-7b loaded with 4-bit quantization onto:\", device)\n",
        "\n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"gate_proj\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Get the LoRA model\n",
        "    model_lora = get_peft_model(model_llama, lora_config)\n",
        "    model_lora.print_trainable_parameters()\n",
        "\n",
        "    from transformers import TrainingArguments, Trainer\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./llama-2-7b-lora-fine-tune\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=3,\n",
        "        fp16=True if device == \"cuda\" else False,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "\n",
        "    # Create the Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model_lora,\n",
        "        train_dataset=processed_train_data_llama,\n",
        "        eval_dataset=None,\n",
        "        args=training_args,\n",
        "        data_collator=lambda data: {k: torch.stack([f[k] for f in data]) for k in data[0]},\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check the error message for more details.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "7453f9da166f4340ba72e669aa64b0c6",
            "042b007acb224cd98900535d847c9d68",
            "b6e89764194c47e0a959384e5ff3bd98",
            "9fbdebb939a54f4f937c3ae40a66fecf",
            "52944bab8d194cb3b254cba781eeb293",
            "5bbdbcc3d5774ccdadfa52628be68743",
            "f9a4d43f56914b9093be6b117502b74f",
            "07d6fe5fde3a4c32a588c12af22e2c1c",
            "9eb3cfe59e2d405d932eea445d71a944",
            "2c168bfe74fb4d65b2e8c6aa7f8a0c1e",
            "ff2670c72e1f45758a930322fd9bf7f0"
          ]
        },
        "id": "NdgZyeWRX60L",
        "outputId": "1f03064a-8d0e-432d-8dd2-9fcd0fcf5edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7453f9da166f4340ba72e669aa64b0c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 416338 has 14.72 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 189.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Please check the error message for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent! The output confirms that LoRA has been successfully applied to the LLaMA 2 model.\n",
        "\n",
        "As you can see:\n",
        "\n",
        "Trainable parameters: 19,988,480\n",
        "Total parameters: 6,758,404,096\n",
        "Trainable percentage: 0.2958%\n",
        "This is a dramatic reduction in the number of parameters that will be updated during training. Only about 0.3% of the model's total parameters will be trained, which will significantly reduce memory usage and speed up the fine-tuning process, making it feasible to run on a Colab GPU.\n",
        "\n",
        "Now that we have our LoRA-adapted LLaMA 2 model and our processed training data, the next step is to set up the training using the Hugging Face Trainer API."
      ],
      "metadata": {
        "id": "n_WmsW5xTmsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyU0ZerfUwIV",
        "outputId": "0b1cb575-6c36-46e1-8ca3-163930dbc7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.45.5\n",
            "Uninstalling bitsandbytes-0.45.5:\n",
            "  Successfully uninstalled bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbP-tU-TVZfj",
        "outputId": "f5e1978f-25f9-4e65-c244-3e0f48853dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Using cached bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        llm_int8_enable_fp32_cpu_offload=False,\n",
        "    )\n",
        "\n",
        "    model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    print(\"LLaMA-2-7b loaded with 4-bit quantization.\")\n",
        "\n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "            \"gate_proj\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Get the LoRA model\n",
        "    model_lora = get_peft_model(model_llama, lora_config)\n",
        "    model_lora.print_trainable_parameters()\n",
        "\n",
        "    from transformers import TrainingArguments, Trainer\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./llama-2-7b-lora-fine-tune\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=3,\n",
        "        fp16=True if device == \"cuda\" else False,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        report_to=\"tensorboard\"\n",
        "    )\n",
        "\n",
        "    # Create the Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model_lora,\n",
        "        train_dataset=processed_train_data_llama,\n",
        "        eval_dataset=None,\n",
        "        args=training_args,\n",
        "        data_collator=lambda data: {k: torch.stack([f[k] for f in data]) for k in data[0]},\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check the error message for more details.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpOw51LWNFQB",
        "outputId": "74aab170-567b-4213-f189-8867a687282a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "An error occurred: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
            "Please check the error message for more details.\n"
          ]
        }
      ]
    }
  ]
}